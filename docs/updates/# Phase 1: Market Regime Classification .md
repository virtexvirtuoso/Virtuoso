# Phase 1: Market Regime Classification - Machine learning Implementation Plan

## I. Overview

This document outlines the implementation plan for integrating a Market Regime Classification feature into the quantitative trading system. The goal is to identify distinct market states (regimes) based on a variety of component scores generated by the `ConfluenceAnalyzer`. These regimes can then be used to modulate trading strategy parameters, risk management rules, and provide richer contextual information for analysis and reporting.

This phase focuses on:
1.  Developing a module for training a clustering model (KMeans or GMM) to identify market regimes from historical component scores.
2.  Integrating the trained model into the `ConfluenceAnalyzer` to classify the current market regime in real-time.
3.  Updating downstream modules (`SignalGenerator`, `MarketMonitor`, formatters, reporters) to utilize and react to the identified market regime.
4.  Establishing a robust testing and validation strategy.

## II. Model Training Module (`src/ml/regime_trainer.py`)

### A. Purpose

This standalone module will handle the offline training, evaluation, and saving of the market regime clustering model and associated preprocessor (scaler). It will take historical component scores as input and output a trained model file and a scaler file. It should also include checks for a minimum amount of historical data to ensure meaningful training.

### B. Dependencies

*   `scikit-learn`
*   `pandas`
*   `numpy`
*   `joblib`
*   `matplotlib` (for visualization, optional but highly recommended for `find_optimal_clusters`)
*   `logging`
*   `typing`
*   `os`

### C. Class Structure and Methods: `RegimeTrainer`

```python
# src/ml/regime_trainer.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, GaussianMixture
from sklearn.metrics import silhouette_score #, calinski_harabasz_score, davies_bouldin_score (consider adding)
import joblib
import logging
from typing import List, Dict, Tuple, Union, Optional, Any # Added Any
import os
import json # For saving characterization results
# import matplotlib.pyplot as plt # Uncomment if adding plotting

logger = logging.getLogger(__name__)

class RegimeTrainer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get('ml_market_regime', {})
        self.training_data_path = self.config.get('historical_data_path', 'data/historical_component_scores.csv')
        self.model_save_path = self.config.get('model_path', 'src/ml_models/regime_model.joblib')
        self.scaler_save_path = self.config.get('scaler_path', 'src/ml_models/regime_scaler.joblib')
        self.regime_characteristics_path = self.config.get('regime_characteristics_path', 'data/regime_characteristics.json') # For saving detailed stats
        self.feature_columns = self.config.get('feature_list', [])
        self.model_type = self.config.get('model_type', 'kmeans') # 'kmeans' or 'gmm'
        self.max_k = self.config.get('max_clusters_to_test', 10)
        self.plot_optimal_k_diagnostics = self.config.get('plot_optimal_k_diagnostics', False) # New config for plotting
        self.regime_labels = self.config.get('regime_labels', {})
        self.regime_default_missing_feature_value = self.config.get('default_value_for_missing_feature', 50.0) # Added based on recommendation

        os.makedirs(os.path.dirname(self.model_save_path), exist_ok=True)
        os.makedirs(os.path.dirname(self.scaler_save_path), exist_ok=True)
        os.makedirs(os.path.dirname(self.regime_characteristics_path), exist_ok=True)


    def load_historical_component_scores(self) -> Optional[pd.DataFrame]:
        logger.info(f"Loading historical component scores from {self.training_data_path}...")
        try:
            df = pd.read_csv(self.training_data_path)
            if not self.feature_columns:
                logger.error("Feature columns list is empty in config. Cannot select features.")
                return None
            if not all(col in df.columns for col in self.feature_columns):
                missing_cols = [col for col in self.feature_columns if col not in df.columns]
                logger.error(f"Missing feature columns {missing_cols} in {self.training_data_path}")
                return None
            logger.info(f"Loaded {len(df)} records with columns: {df.columns.tolist()}")
            return df[self.feature_columns].copy() # Ensure it's a copy for modifications
        except FileNotFoundError:
            logger.error(f"Historical data file not found: {self.training_data_path}")
            return None
        except Exception as e:
            logger.error(f"Error loading historical data: {e}")
            return None

    def preprocess_features(self, df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], Optional[StandardScaler]]:
        logger.info("Preprocessing features (scaling and NaN imputation)...")
        if df.empty or not self.feature_columns:
            logger.error("DataFrame is empty or feature columns are not defined for preprocessing.")
            return None, None
        try:
            X = df[self.feature_columns].copy() # Work on a copy
            # Impute NaNs with the mean of each column
            # For financial time series, alternative imputation methods (e.g., median, time-series interpolation)
            # could be explored in future iterations if mean imputation proves to be a significant distortion.
            for col in X.columns:
                if X[col].isnull().any():
                    mean_val = X[col].mean()
                    X[col].fillna(mean_val, inplace=True)
                    logger.debug(f"Imputed NaNs in column '{col}' with mean value {mean_val:.2f}")

            scaler = StandardScaler()
            scaled_X_array = scaler.fit_transform(X)
            scaled_X_df = pd.DataFrame(scaled_X_array, columns=self.feature_columns, index=X.index)
            logger.info("Features scaled successfully.")
            return scaled_X_df, scaler
        except Exception as e:
            logger.error(f"Error during feature preprocessing: {e}")
            return None, None

    def find_optimal_clusters(self, scaled_data: pd.DataFrame) -> int:
        logger.info(f"Finding optimal number of clusters (max_k={self.max_k})...")
        if scaled_data is None or scaled_data.empty:
            logger.error("Cannot find optimal clusters with no data.")
            return self.config.get('default_n_clusters', 3)

        silhouette_scores = {}
        wcss = {} # For Elbow method (KMeans)
        # Consider adding Calinski-Harabasz and Davies-Bouldin scores for a more robust choice,
        # potentially controlled by a configuration flag.
        calinski_harabasz_scores = {}
        davies_bouldin_scores = {}

        for k_val in range(2, self.max_k + 1): # K must be >= 2 for silhouette
            if k_val > len(scaled_data) -1 : # Scikit learn requirement
                logger.warning(f"K={k_val} is too large for sample size {len(scaled_data)}. Skipping.")
                break
            if self.model_type == 'kmeans':
                model = KMeans(n_clusters=k_val, random_state=42, n_init='auto')
            elif self.model_type == 'gmm':
                model = GaussianMixture(n_components=k_val, random_state=42, covariance_type='full') # 'full' is often a good default
            else:
                logger.error(f"Unsupported model type: {self.model_type}")
                return self.config.get('default_n_clusters', 3)

            try:
                cluster_labels = model.fit_predict(scaled_data)
            except Exception as e:
                logger.error(f"Error fitting model for K={k_val}: {e}")
                continue

            if len(np.unique(cluster_labels)) > 1:
                try:
                    score = silhouette_score(scaled_data, cluster_labels)
                    silhouette_scores[k_val] = score
                    logger.debug(f"K={k_val}, Silhouette Score: {score:.4f}")
                    # if self.config.get('calculate_extended_optimal_k_metrics', False):
                    #     calinski_score = calinski_harabasz_score(scaled_data, cluster_labels)
                    #     davies_score = davies_bouldin_score(scaled_data, cluster_labels)
                    #     calinski_harabasz_scores[k_val] = calinski_score
                    #     davies_bouldin_scores[k_val] = davies_score
                    #     logger.debug(f"K={k_val}, Calinski-Harabasz Score: {calinski_score:.2f}")
                    #     logger.debug(f"K={k_val}, Davies-Bouldin Score: {davies_score:.4f}")
                except ValueError as ve: # Catch issues with too few samples per cluster
                     logger.warning(f"Could not calculate silhouette score for K={k_val}: {ve}")
            else:
                logger.warning(f"K={k_val}, Not enough distinct clusters formed ({len(np.unique(cluster_labels))}) for silhouette score.")

            if self.model_type == 'kmeans' and hasattr(model, 'inertia_'):
                 wcss[k_val] = model.inertia_
                 logger.debug(f"K={k_val}, WCSS: {model.inertia_:.2f}")

        # Determine optimal K
        if silhouette_scores:
            optimal_k = max(silhouette_scores, key=silhouette_scores.get)
            logger.info(f"Optimal K based on Silhouette Score: {optimal_k} (Score: {silhouette_scores[optimal_k]:.4f})")
        else:
            logger.warning("Could not determine optimal K via Silhouette. Using default.")
            optimal_k = self.config.get('default_n_clusters', 3)

        # if self.plot_optimal_k_diagnostics and plt: # Check if matplotlib.pyplot was imported as plt
        #     # Plot Silhouette Scores
        #     if silhouette_scores:
        #         plt.figure(figsize=(10, 4))
        #         plt.subplot(1, 2, 1)
        #         plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), marker='o')
        #         plt.title('Silhouette Scores for Optimal K')
        #         plt.xlabel('Number of clusters (K)')
        #         plt.ylabel('Silhouette Score')

        #     # Plot WCSS (Elbow Method) for KMeans
        #     if self.model_type == 'kmeans' and wcss:
        #         plt.subplot(1, 2, 2 if silhouette_scores else 1)
        #         plt.plot(list(wcss.keys()), list(wcss.values()), marker='o')
        #         plt.title('Elbow Method for Optimal K (WCSS)')
        #         plt.xlabel('Number of clusters (K)')
        #         plt.ylabel('WCSS')
        #     plt.tight_layout()
        #     # plt.show() # Or save to file: plt.savefig('optimal_k_plots.png')
        #     # Ensure plots are saved to a configurable directory, e.g., 'data/diagnostics/regime_plots/'
        #     plot_save_path = self.config.get('optimal_k_plot_path', 'data/diagnostics/regime_plots/optimal_k_diagnostics.png')
        #     if self.plot_optimal_k_diagnostics and silhouette_scores: # Check if there's anything to plot
        #        os.makedirs(os.path.dirname(plot_save_path), exist_ok=True)
        #        plt.savefig(plot_save_path)
        #        logger.info(f"Diagnostic plots for optimal K saved to {plot_save_path} (if plotting enabled and library available).")
        #     else:
        #        logger.info("Diagnostic plots for optimal K not generated or saved.")

        return optimal_k

    def train_clustering_model(self, scaled_data: pd.DataFrame, n_clusters: int) -> Tuple[Optional[Union[KMeans, GaussianMixture]], Optional[np.ndarray]]:
        logger.info(f"Training {self.model_type} model with {n_clusters} clusters...")
        if scaled_data is None or scaled_data.empty:
             logger.error("Cannot train model with no data.")
             return None, None
        if n_clusters <= 1 or n_clusters > len(scaled_data) -1:
            logger.error(f"Invalid n_clusters ({n_clusters}) for sample size {len(scaled_data)}. Must be > 1 and < n_samples.")
            return None, None
        try:
            if self.model_type == 'kmeans':
                model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
            elif self.model_type == 'gmm':
                model = GaussianMixture(n_components=n_clusters, random_state=42, covariance_type='full')
            else:
                logger.error(f"Unsupported model_type: {self.model_type}")
                return None, None

            model.fit(scaled_data) # Fit first
            cluster_labels = model.predict(scaled_data) # Then predict on the same data
            logger.info(f"{self.model_type} model trained successfully with {len(np.unique(cluster_labels))} unique labels.")
            return model, cluster_labels
        except Exception as e:
            logger.error(f"Error training clustering model: {e}")
            return None, None

    def save_model_and_scaler(self, model, scaler) -> bool:
        if model is None or scaler is None:
            logger.error("Model or scaler is None, cannot save.")
            return False
        try:
            joblib.dump(model, self.model_save_path)
            logger.info(f"Trained model saved to {self.model_save_path}")
            joblib.dump(scaler, self.scaler_save_path)
            logger.info(f"Scaler saved to {self.scaler_save_path}")
            return True
        except Exception as e:
            logger.error(f"Error saving model/scaler: {e}")
            return False

    def _characterize_regimes(self, original_data: pd.DataFrame, scaled_data: pd.DataFrame, labels: np.ndarray, scaler: StandardScaler, model: Union[KMeans, GaussianMixture]) -> None:
        """Analyzes and logs characteristics of each identified regime."""
        logger.info("Characterizing regimes...")
        if original_data is None or scaled_data is None or labels is None or scaler is None or model is None:
            logger.error("Missing data for regime characterization.")
            return

        unique_labels = np.unique(labels)
        regime_summary = {}

        # Get centroids (original scale)
        if self.model_type == 'kmeans' and hasattr(model, 'cluster_centers_'):
            centroids_scaled = model.cluster_centers_
            centroids_original_scale = scaler.inverse_transform(centroids_scaled)
        elif self.model_type == 'gmm' and hasattr(model, 'means_'):
            centroids_scaled = model.means_ # GMM means are equivalent to KMeans centroids
            centroids_original_scale = scaler.inverse_transform(centroids_scaled)
        else:
            centroids_original_scale = None
            logger.warning("Could not extract centroids/means for model type or model instance.")

        logger.info("Cluster Characteristics (Original Scale):")
        for label in unique_labels:
            label_mask = (labels == label)
            cluster_data_original = original_data[label_mask]
            
            summary_stats = {}
            for col in self.feature_columns:
                summary_stats[col] = {
                    'mean': cluster_data_original[col].mean(),
                    'median': cluster_data_original[col].median(),
                    'std_dev': cluster_data_original[col].std(),
                    'min': cluster_data_original[col].min(),
                    'max': cluster_data_original[col].max(),
                    '25th_percentile': cluster_data_original[col].quantile(0.25),
                    '75th_percentile': cluster_data_original[col].quantile(0.75),
                }

            cluster_info = {
                "cluster_id": int(label),
                "num_points": int(np.sum(label_mask)),
                "percentage_points": float(np.sum(label_mask) / len(labels) * 100),
                "feature_stats": summary_stats,
                "suggested_label_interpretation": "", # User to fill after reviewing stats
                "notes": "" # Additional user notes
            }

            if centroids_original_scale is not None and label < len(centroids_original_scale):
                centroid_features = dict(zip(self.feature_columns, centroids_original_scale[label]))
                cluster_info["centroid_original_scale"] = centroid_features
                logger.info(f"  Cluster {label} (Centroid): {centroid_features}")
            
            logger.info(f"  Cluster {label} (Overall Stats): Num Points={cluster_info['num_points']}, Percentage={cluster_info['percentage_points']:.2f}%")
            for feat, stats in summary_stats.items():
                 logger.info(f"    {feat}: Mean={stats['mean']:.2f}, Median={stats['median']:.2f}, StdDev={stats['std_dev']:.2f}")

            regime_summary[str(label)] = cluster_info
        
        try:
            with open(self.regime_characteristics_path, 'w') as f:
                json.dump(regime_summary, f, indent=4)
            logger.info(f"Regime characteristics saved to {self.regime_characteristics_path}")
        except Exception as e:
            logger.error(f"Error saving regime characteristics: {e}")

    def run_training_pipeline(self):
        logger.info("Starting Market Regime Model training pipeline...")
        # Keep original_df for characterization before scaling alters it
        original_df = self.load_historical_component_scores()
        if original_df is None:
            logger.error("Aborting training: Failed to load data.")
            return

        # Preprocessing must be done on a copy if original_df is to be preserved
        scaled_df, scaler = self.preprocess_features(original_df.copy())
        if scaled_df is None or scaler is None:
            logger.error("Aborting training: Failed to preprocess features.")
            return

        optimal_k = self.find_optimal_clusters(scaled_df)
        
        n_clusters_to_train = self.config.get('n_clusters', optimal_k) # Allow override
        logger.info(f"Will train model with {n_clusters_to_train} clusters.")

        model, labels = self.train_clustering_model(scaled_df, n_clusters_to_train)
        if model is None or labels is None: # Check labels too
            logger.error("Aborting training: Model training failed or no labels generated.")
            return

        if not self.save_model_and_scaler(model, scaler):
            logger.error("Model/scaler saving failed. Please check paths and permissions.")
            # Potentially abort if saving is critical, or just log and continue with characterization
            # return 

        # Characterize regimes using the original unscaled data and the new labels
        self._characterize_regimes(original_df, scaled_df, labels, scaler, model)
            
        logger.info("Market Regime Model training pipeline completed.")
        logger.info(f"Next steps: Analyze cluster characteristics from logs and '{self.regime_characteristics_path}' to define 'regime_labels' in config.yaml for the {n_clusters_to_train} clusters.")
```

### D. Example Usage Script (within `src/ml/regime_trainer.py`)

```python
    # Example usage (typically run as a script from the command line)
    if __name__ == '__main__':
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        # Dummy config for demonstration
        sample_config_data = {
            'ml_market_regime': {
                'historical_data_path': 'data/dummy_historical_component_scores.csv', # Create a dummy CSV for testing
                'model_path': 'src/ml_models/regime_model_test.joblib',
                'scaler_path': 'src/ml_models/regime_scaler_test.joblib',
                'regime_characteristics_path': 'data/regime_characteristics_test.json',
                'feature_list': ['technical_score', 'volume_score', 'orderflow_score', 'orderbook_score', 'sentiment_score'],
                'model_type': 'kmeans', # or 'gmm'
                'max_clusters_to_test': 7,
                'default_n_clusters': 3, # Fallback if optimal_k fails
                # 'n_clusters': 4 # Optionally fix the number of clusters
                'plot_optimal_k_diagnostics': False # Set to True to try plotting
            }
        }
        # Create dummy CSV data
        data_dir = 'data'
        ml_models_dir = 'src/ml_models'
        if not os.path.exists(data_dir): os.makedirs(data_dir)
        if not os.path.exists(ml_models_dir): os.makedirs(ml_models_dir) # Ensure model dir exists

        num_samples = 1000
        features = sample_config_data['ml_market_regime']['feature_list']
        dummy_data = {feature: np.random.rand(num_samples) * 100 for feature in features}
        # Introduce some NaNs for testing imputation
        for feature in features:
            nan_indices = np.random.choice(num_samples, size=num_samples // 20, replace=False) # 5% NaNs
            dummy_data[feature][nan_indices] = np.nan
            
        dummy_df = pd.DataFrame(dummy_data)
        dummy_df.to_csv(sample_config_data['ml_market_regime']['historical_data_path'], index=False)
        logger.info(f"Created dummy data with NaNs at {sample_config_data['ml_market_regime']['historical_data_path']}")

        trainer = RegimeTrainer(sample_config_data)
        trainer.run_training_pipeline()
```

## III. Configuration Updates (`config.yaml`)

### A. Purpose

The main `config.yaml` file will be updated to include parameters for the market regime classification feature, both for training and runtime operation, including configurable strategy and risk adjustments.

### B. Structure

```yaml
# ... other configurations ...

ml_market_regime:
  enabled: true  # Master switch for the feature in ConfluenceAnalyzer
  model_path: "src/ml_models/regime_model.joblib"
  scaler_path: "src/ml_models/regime_scaler.joblib"
  
  # List of component score keys from ConfluenceAnalyzer output used for training.
  # The order MUST match the order of columns used during training.
  # CRITICAL: The order and content of this list MUST EXACTLY match the features used during model training.
  # Consider having RegimeTrainer save a metadata file (e.g., src/ml_models/regime_model_metadata.json)
  # with the feature list (name and order) used. ConfluenceAnalyzer could then either:
  #   a) Validate its configured list against this metadata and error if mismatched.
  #   b) Use the feature list from the metadata file as the authoritative source.
  feature_list:
    - "technical_score"
    - "volume_score"
    - "orderflow_score"
    - "orderbook_score"
    - "price_structure_score"
    - "sentiment_score"
    # - "volatility_score" # Example: Add more as they become available
    # - "liquidity_score" # Example

  # Labels assigned after analyzing cluster characteristics from the training script output
  # Keys are STRINGS of cluster IDs from the model (e.g., "0", "1"), values are human-readable labels.
  regime_labels: 
    "0": "Trending Bullish"
    "1": "Ranging LowVol"
    "2": "Trending Bearish"
    "3": "High Volatility Undetermined"
    # "4": "Accumulation Phase" # Add more based on trained clusters

  # --- Trainer Specific Configuration ---
  model_type: 'kmeans' # 'kmeans' or 'gmm', should match what was trained for runtime if not retraining
  historical_data_path: 'data/historical_component_scores.csv' # For the trainer script
  regime_characteristics_path: 'data/regime_characteristics.json' # Output from trainer
  max_clusters_to_test: 8   # For trainer's optimal_k search
  default_n_clusters: 4     # Fallback for trainer, or for fixed training
  # n_clusters: 4             # Optional: If set, trainer uses this, overriding optimal_k search
  plot_optimal_k_diagnostics: false # For trainer to generate silhouette/elbow plots

  # --- Strategy Adjustments (used by SignalGenerator) ---
  strategy_adjustments:
    enabled: true
    # Default factors if a specific regime isn't listed below, if adjustments are disabled for a regime,
    # or if the identified market_regime_label is not found in the 'regimes' map (e.g., an undefined or error regime).
    default_buy_threshold_factor: 1.0 
    default_sell_threshold_factor: 1.0
    regimes: # Keys must match 'regime_labels' values
      "Trending Bullish":
        buy_threshold_factor: 0.95 # Be more sensitive to buy signals
        sell_threshold_factor: 1.0 
      "Trending Bearish":
        buy_threshold_factor: 1.0
        sell_threshold_factor: 0.95 # Be more sensitive to sell signals
      "Ranging LowVol":
        buy_threshold_factor: 1.05 # Be less sensitive
        sell_threshold_factor: 0.95 # Be less sensitive (wider band for ranging)
      "High Volatility Undetermined":
        buy_threshold_factor: 1.1  # Be much less sensitive, higher confirmation needed
        sell_threshold_factor: 0.9  # Be much less sensitive
      # Add other regimes as defined in regime_labels

  # --- Risk Adjustments (used by MarketMonitor) ---
  risk_adjustments:
    enabled: true
    # Default factors, also used if a specific regime isn't listed below or is an error/undefined state.
    default_stop_loss_factor: 1.0       # Multiplier for SL percentage (e.g., 1.2 = 20% wider)
    default_take_profit_factor: 1.0   # Multiplier for TP based on R:R or fixed target
    default_position_size_factor: 1.0 # Multiplier for position size (e.g., 0.8 = 20% smaller)
    regimes: # Keys must match 'regime_labels' values
      "Trending Bullish":
        stop_loss_factor: 0.9    # Tighter stop for trends
        take_profit_factor: 1.1  # Allow more room for profit
      "Trending Bearish":
        stop_loss_factor: 0.9
        take_profit_factor: 1.1
      "Ranging LowVol":
        stop_loss_factor: 1.0
        take_profit_factor: 0.9  # Quicker profits in ranges
      "High Volatility Undetermined":
        stop_loss_factor: 1.25   # Wider stops
        take_profit_factor: 1.0
        position_size_factor: 0.75 # Reduce size in high uncertainty
      # Add other regimes

# ... other configurations ...
```

## IV. `ConfluenceAnalyzer` Integration (`src/core/analysis/confluence.py`)

### A. Purpose

`ConfluenceAnalyzer` will be modified to load the trained regime model and scaler, use them to classify the current market regime based on the latest component scores, and include this regime information in its analysis output.

### B. Initialization (`__init__`) Changes

*   Load `ml_market_regime` config.
*   If `enabled` is true:
    *   Load the clustering model (`.joblib` file) and the scaler (`.joblib` file) using paths from the config.
    *   Store `feature_list` and `regime_labels` from config.
    *   Implement robust error handling for file loading (e.g., `FileNotFoundError`, `joblib` errors), logging issues and gracefully disabling the feature if loading fails.

### C. `analyze` Method Modifications

```python
# src/core/analysis/confluence.py (Conceptual modifications)
import logging
import joblib
import numpy as np
import pandas as pd 
from typing import Dict, Any, List, Optional 

logger = logging.getLogger(__name__)

class ConfluenceAnalyzer:
    def __init__(self, config: Dict[str, Any], exchange_interface: Optional[Any] = None):
        # ... (existing init) ...
        self.config = config
        self.weights = self.config.get('confluence', {}).get('weights', {}).get('components', {})
        self.logger = logger

        # Market Regime Classification Initialization
        self.regime_config = self.config.get('ml_market_regime', {})
        self.regime_classification_enabled = self.regime_config.get('enabled', False)
        self.regime_model = None
        self.regime_scaler = None
        self.regime_feature_list: List[str] = self.regime_config.get('feature_list', [])
        self.regime_labels: Dict[str, str] = self.regime_config.get('regime_labels', {})
        self.regime_default_missing_feature_value: float = self.regime_config.get('default_value_for_missing_feature', 50.0) # Added

        if self.regime_classification_enabled:
            if not self.regime_feature_list:
                self.logger.error("Market Regime: 'feature_list' is empty in config. Disabling classification.")
                self.regime_classification_enabled = False
            else:
                try:
                    model_path = self.regime_config.get('model_path')
                    scaler_path = self.regime_config.get('scaler_path')
                    if model_path and scaler_path:
                        self.regime_model = joblib.load(model_path)
                        self.regime_scaler = joblib.load(scaler_path)
                        self.logger.info(f"Market Regime Classification model ({model_path}) and scaler ({scaler_path}) loaded successfully.")
                    else:
                        self.logger.error("Market Regime: 'model_path' or 'scaler_path' not configured. Disabling classification.")
                        self.regime_classification_enabled = False
                except FileNotFoundError as fnf_error:
                    self.logger.error(f"Market Regime: Model or scaler file not found: {fnf_error}. Disabling classification.")
                    self.regime_classification_enabled = False
                except Exception as e: # Catches joblib errors, etc.
                    self.logger.error(f"Market Regime: Failed to load model/scaler: {e}. Disabling classification.")
                    self.regime_classification_enabled = False
        else:
            self.logger.info("Market Regime Classification is disabled in the configuration.")

    async def analyze(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        symbol = market_data.get('symbol', 'N/A')
        self.logger.debug(f"Starting confluence analysis for {symbol}...")

        # --- Existing analysis logic to derive component_scores ---
        # This part MUST derive actual scores from various sub-analyzers.
        # Example: component_scores = self._calculate_all_component_scores(market_data)
        # For this plan, we assume `component_scores` (a Dict[str, float]) is correctly populated here.
        # If `ConfluenceAnalyzer` relies on other classes to provide these, ensure they are called.
        # CRITICAL DEPENDENCY: The quality of regime classification heavily depends on the completeness and
        # accuracy of the `component_scores` dictionary. Incomplete data will lead to degraded performance
        # or reliance on default values for missing features.
        # Placeholder - replace with actual score calculation:
        component_scores: Dict[str, float] = {} 
        # Example:
        # if hasattr(self, 'technical_analyzer'): component_scores['technical_score'] = await self.technical_analyzer.get_score(...)
        # ... and so on for all features in self.regime_feature_list
        
        # Fallback for demonstration IF scores are not generated by upstream (should be avoided in production)
        # This is illustrative; in production, ensure component_scores are fully populated by analyzers.
        if not component_scores and self.regime_feature_list: # Only if empty and we have a feature list
             self.logger.warning(f"Component scores dictionary is empty for {symbol}. Using random defaults for demonstration ONLY.")
             component_scores = {feature: np.random.uniform(0, 100) for feature in self.regime_feature_list}
        elif not component_scores and not self.regime_feature_list:
             self.logger.warning(f"Component scores and feature_list are empty for {symbol}.")
             # Fallback to some very basic scores if nothing is available
             component_scores = {"technical_score": 50.0, "volume_score": 50.0}


        # Calculate confluence_score and reliability (placeholders, adapt to your logic)
        # ... (your existing confluence score and reliability calculation) ...
        confluence_score = 50.0 # Placeholder
        reliability = 0.5 # Placeholder
        results_details = {} # Placeholder

        # --- ML Market Regime Classification ---
        market_regime_id: int = -1 
        market_regime_label: str = "Disabled" # Default if not enabled

        if self.regime_classification_enabled:
            if self.regime_model and self.regime_scaler and self.regime_feature_list:
                try:
                    # Construct feature vector in the exact order used for training
                    feature_vector_values = []
                    missing_features_in_scores = []
                    for key in self.regime_feature_list:
                        value = component_scores.get(key)
                        if value is None:
                            # Log clearly when a feature is missing and will be defaulted
                            self.logger.warning(f"Market Regime: Feature '{key}' not found in component_scores for {symbol}. Defaulting to {self.regime_default_missing_feature_value}. Available scores: {list(component_scores.keys())}")
                            feature_vector_values.append(self.regime_default_missing_feature_value) # Default value (now configurable)
                            missing_features_in_scores.append(key)
                        else:
                            feature_vector_values.append(value)
                    
                    if missing_features_in_scores:
                        # This is a more persistent warning if any feature was defaulted.
                        market_regime_label = f"FeatureMissing_Defaulted({','.join(missing_features_in_scores)})"
                    else:
                        market_regime_label = "OK" # Tentative, will be overridden by classification

                    feature_array = np.array(feature_vector_values).reshape(1, -1)
                    scaled_features = self.regime_scaler.transform(feature_array)
                    
                    prediction = self.regime_model.predict(scaled_features)
                    market_regime_id = int(prediction[0])
                    
                    market_regime_label = self.regime_labels.get(str(market_regime_id), f"UndefinedRegime_{market_regime_id}")
                    self.logger.info(f"Market Regime for {symbol}: {market_regime_label} (ID: {market_regime_id})")

                except Exception as e:
                    self.logger.error(f"Market Regime: Error during classification for {symbol}: {e}", exc_info=True)
                    market_regime_label = "ClassificationError"
                    market_regime_id = -2 # Specific ID for classification error
            else:
                # This case means it's enabled in config, but model/scaler/feature_list not properly available
                market_regime_label = "ModelNotLoaded_Or_FeatureListIssue"
                market_regime_id = -3 # Specific ID for this state
                self.logger.warning(f"Market Regime: Classification enabled for {symbol} but model/scaler/feature_list not fully available. Current state: Model {'Loaded' if self.regime_model else 'Not Loaded'}, Scaler {'Loaded' if self.regime_scaler else 'Not Loaded'}, Features {'OK' if self.regime_feature_list else 'Missing'}.")
        
        # --- End ML Market Regime Classification ---

        analysis_response = {
            'symbol': symbol,
            'timestamp': pd.Timestamp.utcnow().isoformat(),
            'confluence_score': confluence_score,
            'reliability': reliability,
            'components': component_scores, # The actual component scores used
            'results': results_details, 
            'market_regime_id': market_regime_id,
            'market_regime_label': market_regime_label,
        }
        
        # For formatter compatibility
        if 'market_analysis_summary' not in analysis_response['results']:
            analysis_response['results']['market_analysis_summary'] = {}
        if 'components' not in analysis_response['results']['market_analysis_summary']:
            analysis_response['results']['market_analysis_summary']['components'] = {}
        analysis_response['results']['market_analysis_summary']['components']['Market Regime'] = market_regime_label

        self.logger.debug(f"Confluence analysis for {symbol} completed. Score: {confluence_score:.2f}, Regime: {market_regime_label}")
        return analysis_response

```

## V. System-Wide Updates

### A. Formatting (`src/core/formatting.py`)

*   **`AnalysisFormatter`**: Update `format_analysis_dashboard` (or equivalent) to include `market_regime_label` from the `analysis_response`.
*   **`LogFormatter`**: Optionally, incorporate `market_regime_label` into relevant log messages for richer context (e.g., final score logs).

### B. Signal Generation (`src/signal_generation/signal_generator.py`)

*   The `analysis_result` (passed as `indicators` or `signal_data`) will contain `market_regime_label` and `market_regime_id`.
*   **Modify `generate_signal` / `process_signal`:**
    *   Access `market_regime_label`.
    *   Load `strategy_adjustments` from the main config.
    *   If adjustments are enabled:
        *   Fetch adjustment factors (e.g., `buy_threshold_factor`) for the current `market_regime_label` from the config.
        *   If the specific regime is not in the config or adjustments for it are disabled, use `default_buy_threshold_factor`.
        *   Apply these factors to base `buy_threshold` / `sell_threshold`.
        *   Log the applied adjustments.
    *   **Handle Unknown/Error Regimes:** If `market_regime_label` indicates an error (e.g., "ClassificationError", "ModelNotLoaded", "FeatureMissing_Defaulted"), the system should:
        *   Revert to default (non-adjusted) thresholds (i.e., use factors of 1.0).
        *   Log this behavior clearly.
        *   Optionally, based on severity (e.g., "ClassificationError" might inhibit signal generation, while "FeatureMissing_Defaulted" might just use defaults but still allow signals), consider inhibiting signal generation or flagging the signal with high uncertainty. This distinction and behavior could be configurable (e.g., a list of `critical_regime_error_labels` that halt signal generation vs. `warning_regime_labels` that revert to defaults).
        *   Include `market_regime_label`, `applied_buy_threshold`, and `applied_sell_threshold` in the `signal_result` dictionary for logging and alerting.

### C. Market Monitoring (`src/monitoring/monitor.py`)

*   **`_process_analysis_result`**:
    *   The `result` dict from `ConfluenceAnalyzer.analyze` will include regime info. Log it.
    *   Ensure this regime info is passed to methods like `_generate_signal` or whatever prepares data for `SignalGenerator`.
*   **`_calculate_trade_parameters`** (or equivalent method for setting SL/TP/size):
    *   Accept `market_regime_label` (and potentially `market_regime_id`) as an argument.
    *   Load `risk_adjustments` from the main config.
    *   If adjustments are enabled:
        *   Fetch adjustment factors (e.g., `stop_loss_factor`, `position_size_factor`) for the current `market_regime_label`.
        *   If the specific regime is not in the config or adjustments for it are disabled, use default factors.
        *   Apply these factors to base risk parameters (e.g., `stop_percentage *= stop_loss_factor`).
        *   Log applied adjustments.
    *   **Handle Unknown/Error Regimes:** Similar to `SignalGenerator`, revert to default risk parameters if the regime is an error state or unknown. Log this. Consider if any error states should prevent trade parameter calculation or flag the trade as high-risk. The same configurable distinction between critical and warning regime labels could apply here (e.g., critical errors prevent trade param calculation, warnings revert to defaults).
    *   Include `regime_at_calc` in the output `trade_params` dict.

### D. Reporting (`src/monitoring/market_reporter.py`)

*   Access `market_regime_label`. This is best done by having `MarketMonitor` pass the regime information along with other data when triggering report generation.
*   Incorporate a summary of the current market regime into reports, either as an overview or per symbol.

## VI. Testing Strategy

### A. Unit Tests

*   **`RegimeTrainer`**:
    *   Test `load_historical_component_scores` (mock CSV, file not found, missing columns, empty feature list).
    *   Test `preprocess_features` (NaN imputation, scaling correctness with known inputs/outputs).
    *   Test `find_optimal_clusters` (mock model fitting, handling of too few samples, correct K selection based on mock scores, default K fallback).
    *   Test `train_clustering_model` (mock fit, handling invalid `n_clusters`).
    *   Test `save_model_and_scaler` (mock `joblib.dump`, error handling).
    *   Test `_characterize_regimes` (correct calculation of stats, JSON output format).
*   **`ConfluenceAnalyzer`**:
    *   Test `__init__`: model/scaler loading (success, `FileNotFoundError`, corrupted file causing `joblib` error, empty feature list).
    *   Test `analyze` method:
        *   Mock `self.regime_model.predict` and `self.regime_scaler.transform`.
        *   Input various `component_scores` (all features present, some missing to test defaulting, empty scores dict).
        *   Verify correct `market_regime_id` and `market_regime_label` in output.
        *   Test with `self.regime_classification_enabled = False`.
        *   Test error handling (e.g., "ClassificationError", "ModelNotLoaded_Or_FeatureListIssue", "FeatureMissing_Defaulted").

### B. Integration Tests

*   Test the flow: `ConfluenceAnalyzer` -> `SignalGenerator` (-> potentially `MarketMonitor` for parameter adjustment).
    *   Use a **stable, version-controlled set of mock input data** (`market_data` for `ConfluenceAnalyzer`).
    *   Use a **dummy pre-trained model and scaler** (also version-controlled) that produce predictable regime outputs for the mock input data.
    *   Verify `ConfluenceAnalyzer` output includes the expected regime.
    *   Verify `SignalGenerator` receives the regime and (if applicable) logs correctly adjusted thresholds based on a test config.
    *   Verify `MarketMonitor` (if in scope for this test slice) correctly adjusts trade parameters based on the regime and test config.
    *   (Optional) Verify `AnalysisFormatter` includes regime in its output string.

### C. Historical Analysis / Regime Tagging Validation

*   **Crucial for tuning `regime_labels` in `config.yaml` and validating model usefulness.**
*   After training the model with `regime_trainer.py`:
    1.  Load historical component scores (the same data used for training, or a hold-out set).
    2.  Use the trained model and scaler to predict regimes for all historical data points.
    3.  **Analyze Cluster Characteristics:**
        *   Use the output from `_characterize_regimes` (the JSON file and logs).
        *   For each predicted regime ID, examine the distribution (mean, median, std, percentiles) of each input feature in its original scale. This helps understand "what" each regime represents (e.g., "Regime 0 has high technical\_score, low sentiment\_score").
    4.  **Visual Validation:**
        *   Plot the regime ID (or assigned label) over time alongside the price chart of a relevant asset (e.g., BTC/USD).
        *   Does a regime labeled "Trending Bullish" visually correspond to periods of uptrends? Does "Ranging LowVol" correspond to sideways, low-volatility price action?
    5.  **Quantitative Validation:**
        *   Calculate financial metrics within each identified regime period (e.g., average daily return, annualized volatility, Sharpe ratio if applicable). This helps quantify the nature of each regime.
    6.  **Regime Stability and Transition Analysis:**
        *   Analyze the frequency of regime changes. Too frequent changes might indicate the model is capturing noise.
        *   Examine typical sequences of regime transitions (e.g., does "Low Vol Range" often precede a "Breakout" regime, if such a regime is defined?). This can provide insights into market dynamics.
        *   Analyze regime duration: How long does the market typically stay in each identified regime? Are some regimes too transient to be actionable or indicative of model instability?
        *   Consider developing a regime transition matrix to visualize probabilities of moving from one regime to another.
    7.  **Iterate:** Based on this analysis, refine the `regime_labels` in `config.yaml`. You might also revisit feature selection or model parameters (`model_type`, `n_clusters`) in `RegimeTrainer` if the regimes are not distinct or meaningful.
    8.  **Document Interpretations:** Create and maintain a document (e.g., `REGIME_DEFINITIONS.md` or similar in `docs/`) that clearly explains the interpretation of each numbered/labeled regime, linking it back to the characteristics observed in the JSON output and visual/quantitative validation. This document becomes a key reference for understanding regime-driven logic.
    9.  **Out-of-Sample Validation:** Where sufficient historical data permits, perform the validation on a hold-out (out-of-sample) dataset that was not used during the model training phase. This helps assess the model's ability to generalize to new, unseen data.

## VII. Implementation Notes & Best Practices

*   **Configuration Driven:** Maximize configurability, especially for thresholds, factors, and feature lists.
*   **Robust Logging:** Implement detailed logging at all stages, especially for model loading, predictions, chosen parameters, and error conditions.
*   **Error Handling:** Gracefully handle errors, particularly in `ConfluenceAnalyzer` during model loading or prediction. The system should continue to function (perhaps with regime classification disabled or in a default state) rather than crashing.
*   **Modularity:** Keep the `RegimeTrainer` independent for offline training. Ensure clear interfaces between modules.
*   **Data Lineage:** Ensure the `feature_list` in `config.yaml` precisely matches the order and names of features used during training and expected by `ConfluenceAnalyzer`.
*   **Scalability:** While initial models might be simple, design with the possibility of more complex features or models in the future.
*   **Iterative Refinement:** Market regime definition is not a one-time task. Plan for periodic retraining and re-evaluation of regime characteristics as market dynamics evolve. The tooling (especially `_characterize_regimes` and historical analysis scripts) should support this.
*   **Model Retraining Schedule:** Establish preliminary guidelines or a schedule for when models should be considered for retraining (e.g., quarterly, after major market structure changes are observed, or if ongoing monitoring indicates a degradation in prediction quality or regime relevance).
*   **Monitoring Prediction Quality (Runtime):** For ongoing monitoring, consider logging additional diagnostics from `ConfluenceAnalyzer` when a regime is predicted:
    *   For KMeans: Optionally log the distance of the scaled feature vector to the assigned cluster's centroid. A large distance might indicate the current market state is an outlier or doesn't fit well into any established regime definition.
    *   For GMM: The model can provide probabilities of the data point belonging to each component. Logging the probability for the assigned component can serve as a confidence measure.
    These diagnostics can help in identifying model drift or situations where the market is behaving in a way not captured by the trained regimes.
*   **Source Control:** Commit model files (`.joblib`), scaler files, and potentially the `regime_characteristics.json` output to version control if they are considered part of a stable "release" of the regime model. For frequently retrained models, consider a dedicated model/data versioning solution.

This comprehensive plan should guide the successful implementation of the Market Regime Classification feature.