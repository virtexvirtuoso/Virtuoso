"""Market data monitoring system.

This module provides monitoring functionality for market data:
- Performance monitoring
- Data quality monitoring
- System health monitoring
- Alert generation

Signal Generation Flow:
- The MarketMonitor analyzes market data and calculates confluence scores
- When a score exceeds the buy threshold (60) or falls below the sell threshold (40),
  the MarketMonitor initiates signal generation
- Signals are passed to the SignalGenerator for further processing and alert dispatch
- All thresholds are defined in the config.yaml file under analysis.confluence_thresholds
"""

import functools
import logging
import time
import asyncio
import traceback
import json
import os
import signal
import sys
import uuid
import gc
from typing import Dict, List, Any, Optional, Callable, Union, Tuple
from datetime import datetime, timezone, timedelta
import pandas as pd
import numpy as np

# Import and apply matplotlib silencing before matplotlib imports
from src.utils.matplotlib_utils import silence_matplotlib_logs
silence_matplotlib_logs()

import matplotlib
import psutil
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import io
import base64
from decimal import Decimal

# Import local modules
from src.monitoring.alert_manager import AlertManager
from src.monitoring.metrics_manager import MetricsManager
from src.monitoring.health_monitor import HealthMonitor
# Import the new MarketReporter
from src.monitoring.market_reporter import MarketReporter

from src.core.formatting import AnalysisFormatter, format_analysis_result, LogFormatter

from src.indicators.orderflow_indicators import DataUnavailableError
from src.signal_generation.signal_generator import SignalGenerator
import tracemalloc
from collections import defaultdict
import datetime as dt  # Import datetime module separately for time
import copy  # For deep copying data structures

# Import matplotlib for visualization
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter
import io
import base64
from pathlib import Path

from src.core.validation import (
    ValidationService,
    AsyncValidationService,
    ValidationContext,
    ValidationResult,
    TimeRangeRule,
    SymbolRule
)
from src.core.error.models import ErrorContext, ErrorSeverity
from src.monitoring.metrics_manager import MetricsManager
from src.monitoring.alert_manager import AlertManager
from src.core.market.top_symbols import TopSymbolsManager
from src.core.analysis.confluence import ConfluenceAnalyzer
from src.core.market.market_data_manager import MarketDataManager  # Import the new MarketDataManager
from src.monitoring.health_monitor import HealthMonitor
from src.core.exchanges.websocket_manager import WebSocketManager  # Import WebSocketManager

# Add import for liquidation cache
from src.utils.liquidation_cache import liquidation_cache

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

tracemalloc.start()

def ccxt_time_to_minutes(timeframe: str) -> int:
    """Convert CCXT timeframe string to minutes.
    
    Args:
        timeframe: Timeframe string (e.g., '1m', '5m', '1h', '1d')
        
    Returns:
        Number of minutes in the timeframe
    """
    if not timeframe:
        return 0
    
    unit = timeframe[-1]
    value = int(timeframe[:-1]) if timeframe[:-1].isdigit() else 0
    
    if unit == 'm':
        return value
    elif unit == 'h':
        return value * 60
    elif unit == 'd':
        return value * 1440  # 24 * 60
    elif unit == 'w':
        return value * 10080  # 7 * 24 * 60
    else:
        return int(timeframe) if timeframe.isdigit() else 0

def handle_monitoring_error(func: Callable) -> Callable:
    """Decorator to handle monitoring errors.
    
    Args:
        func: Function to wrap
        
    Returns:
        Wrapped function with error handling
    """
    @functools.wraps(func)
    async def wrapper(*args, **kwargs) -> Any:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            logger.debug(traceback.format_exc())
            return None
    return wrapper

class TimeRangeValidationRule:
    """Validation rule for checking time ranges."""
    
    def __init__(self, monitor):
        self.monitor = monitor
        self.name = "time_range"
        
    async def validate(self, data: Dict[str, Any]) -> bool:
        """Validate time range for data.
        
        Args:
            data: Data dictionary containing timestamp
            
        Returns:
            bool: True if time range is valid
        """
        try:
            timestamp = data.get('timestamp')
            if not timestamp:
                return False
                
            # Convert timestamp to datetime if needed
            if isinstance(timestamp, (int, float)):
                timestamp = datetime.fromtimestamp(timestamp)
                
            # Get current time
            now = datetime.now()
            
            # Check if timestamp is within acceptable range
            max_age = timedelta(hours=24)  # Max 24 hours old
            min_time = now - max_age
            
            return min_time <= timestamp <= now
            
        except Exception as e:
            self.monitor.logger.error(f"Error validating time range: {str(e)}")
            return False

class MarketDataValidator:
    """Comprehensive validation system for market data."""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        
        # Define validation rules for different data types
        self.validation_rules = {
            'ohlcv': self._validate_ohlcv,
            'orderbook': self._validate_orderbook,
            'trades': self._validate_trades,
            'ticker': self._validate_ticker,
            'funding': self._validate_funding
        }
        
        # Track validation statistics
        self.validation_stats = {
            'total_validations': 0,
            'passed_validations': 0,
            'failed_validations': 0,
            'warnings': 0,
            'data_types': {}
        }
    
    async def validate_market_data(self, market_data):
        """Comprehensive validation of entire market data.
        
        This method is asynchronous for consistency with the rest of the codebase,
        although the individual validation methods are synchronous.
        
        Args:
            market_data: The market data to validate
            
        Returns:
            bool: True if validation passes, False otherwise
        """
        self.logger.debug("\n=== Starting comprehensive market data validation (async) ===")
        
        # Increment total validations
        self.validation_stats['total_validations'] += 1
        
        # Check if market_data is a dictionary
        if not isinstance(market_data, dict):
            self.logger.error(f"Market data must be a dictionary, got {type(market_data)}")
            self.validation_stats['failed_validations'] += 1
            return False
        
        # Check for required base fields
        required_fields = ['symbol', 'timestamp']
        missing_fields = [field for field in required_fields if field not in market_data]
        if missing_fields:
            self.logger.error(f"Missing required fields in market data: {missing_fields}")
            self.validation_stats['failed_validations'] += 1
            return False
        
        # Check each data type using appropriate validator
        validation_results = {}
        overall_result = True
        
        for data_type, validator in self.validation_rules.items():
            if data_type in market_data:
                # Initialize stats tracking for this data type if needed
                if data_type not in self.validation_stats['data_types']:
                    self.validation_stats['data_types'][data_type] = {
                        'validations': 0,
                        'passes': 0,
                        'failures': 0
                    }
                
                self.validation_stats['data_types'][data_type]['validations'] += 1
                
                # Run validation - all validators are still synchronous
                result = validator(market_data[data_type])
                validation_results[data_type] = result
                
                # Update stats
                if result:
                    self.validation_stats['data_types'][data_type]['passes'] += 1
                else:
                    self.validation_stats['data_types'][data_type]['failures'] += 1
                    overall_result = False
        
        # Update overall stats
        if overall_result:
            self.validation_stats['passed_validations'] += 1
        else:
            self.validation_stats['failed_validations'] += 1
            
        self.logger.debug(f"Validation results: {validation_results}")
        self.logger.debug(f"Overall validation result: {'PASS' if overall_result else 'FAIL'}")
        
        return overall_result
    
    def _validate_ohlcv(self, ohlcv_data):
        """Validate OHLCV data for all timeframes."""
        if not isinstance(ohlcv_data, dict):
            self.logger.error(f"OHLCV data must be a dictionary, got {type(ohlcv_data)}")
            return False
        
        # Required timeframes
        required_timeframes = ['base', 'ltf', 'mtf', 'htf']
        available_timeframes = set(ohlcv_data.keys())
        missing_timeframes = set(required_timeframes) - available_timeframes
        
        if missing_timeframes:
            self.logger.warning(f"Missing timeframes in OHLCV data: {missing_timeframes}")
            # We can still validate with some missing timeframes, but at least one should be present
            if len(available_timeframes) == 0:
                self.logger.error("No timeframes available in OHLCV data")
                return False
        
        # Validate each timeframe
        valid_timeframes = 0
        for timeframe, data in ohlcv_data.items():
            if isinstance(data, pd.DataFrame):
                if data.empty:
                    self.logger.warning(f"Empty DataFrame for timeframe {timeframe}")
                    continue
                
                # Check required columns
                required_columns = ['open', 'high', 'low', 'close', 'volume']
                missing_columns = [col for col in required_columns if col not in data.columns]
                
                if missing_columns:
                    self.logger.warning(f"Timeframe {timeframe} missing columns: {missing_columns}")
                    continue
                
                # Check for NaN values
                nan_counts = data.isna().sum()
                if nan_counts.sum() > 0:
                    self.logger.warning(f"NaN values found in timeframe {timeframe}: {nan_counts}")
                
                # Check that high >= low
                invalid_hl = data[data['high'] < data['low']]
                if not invalid_hl.empty:
                    self.logger.error(f"Found {len(invalid_hl)} candles where high < low in {timeframe}")
                    return False
                
                # Check that high >= open and high >= close
                invalid_ho = data[data['high'] < data['open']]
                invalid_hc = data[data['high'] < data['close']]
                if not invalid_ho.empty or not invalid_hc.empty:
                    self.logger.error(f"Found candles with invalid high values in {timeframe}")
                    return False
                
                # Check that low <= open and low <= close
                invalid_lo = data[data['low'] > data['open']]
                invalid_lc = data[data['low'] > data['close']]
                if not invalid_lo.empty or not invalid_lc.empty:
                    self.logger.error(f"Found candles with invalid low values in {timeframe}")
                    return False
                
                # Check for negative values
                negative_values = data[(data[['open', 'high', 'low', 'close', 'volume']] < 0).any(axis=1)]
                if not negative_values.empty:
                    self.logger.error(f"Found {len(negative_values)} candles with negative values in {timeframe}")
                    return False
                
                # Check for chronological order and duplicates
                if data.index.name == 'timestamp' and isinstance(data.index, pd.DatetimeIndex):
                    if not data.index.is_monotonic_increasing:
                        self.logger.error(f"Timestamps in {timeframe} are not in chronological order")
                        return False
                    
                    if len(data.index) != len(data.index.unique()):
                        self.logger.error(f"Duplicate timestamps found in {timeframe}")
                        return False
                
                # Mark this timeframe as valid
                valid_timeframes += 1
        
        # Return True if at least one timeframe is valid
        if valid_timeframes > 0:
            self.logger.debug(f"OHLCV validation passed with {valid_timeframes} valid timeframes")
            return True
        
        self.logger.error("No valid timeframes found in OHLCV data")
        return False
    
    def _validate_orderbook(self, orderbook_data):
        """Validate orderbook data."""
        # Handle nested orderbook structure
        if isinstance(orderbook_data, dict):
            # Check for the Bybit structure with result containing 'b' and 'a'
            if 'result' in orderbook_data and isinstance(orderbook_data['result'], dict):
                result_data = orderbook_data['result']
                if 'b' in result_data and 'a' in result_data:
                    # Extract bids and asks from the result structure
                    orderbook_data = {
                        'bids': result_data.get('b', []),
                        'asks': result_data.get('a', [])
                    }
        
        if not isinstance(orderbook_data, dict):
            self.logger.error(f"Orderbook data must be a dictionary, got {type(orderbook_data)}")
            return False
        
        # Check for required fields
        required_fields = ['bids', 'asks']
        missing_fields = [field for field in required_fields if field not in orderbook_data]
        if missing_fields:
            self.logger.error(f"Missing required fields in orderbook data: {missing_fields}")
            self.logger.debug(f"Orderbook keys: {list(orderbook_data.keys())}")
            return False
        
        # Check that bids and asks are lists
        if not isinstance(orderbook_data['bids'], list) or not isinstance(orderbook_data['asks'], list):
            self.logger.error("Bids and asks must be lists")
            self.logger.debug(f"Bids type: {type(orderbook_data['bids'])}, Asks type: {type(orderbook_data['asks'])}")
            return False
        
        # Check that bids and asks are not empty
        if len(orderbook_data['bids']) == 0 and len(orderbook_data['asks']) == 0:
            self.logger.warning("Both bids and asks are empty in orderbook")
            return False
        
        # Check format of bids and asks
        if orderbook_data['bids']:
            if not all(isinstance(bid, list) and len(bid) >= 2 for bid in orderbook_data['bids']):
                self.logger.error("Invalid bid format in orderbook")
                return False
        
        if orderbook_data['asks']:
            if not all(isinstance(ask, list) and len(ask) >= 2 for ask in orderbook_data['asks']):
                self.logger.error("Invalid ask format in orderbook")
                return False
        
        # Check that bids are in descending order (highest to lowest)
        if len(orderbook_data['bids']) > 1:
            for i in range(len(orderbook_data['bids']) - 1):
                if orderbook_data['bids'][i][0] < orderbook_data['bids'][i+1][0]:
                    self.logger.error("Bids are not in descending order")
                    return False
        
        # Check that asks are in ascending order (lowest to highest)
        if len(orderbook_data['asks']) > 1:
            for i in range(len(orderbook_data['asks']) - 1):
                if orderbook_data['asks'][i][0] > orderbook_data['asks'][i+1][0]:
                    self.logger.error("Asks are not in ascending order")
                    return False
        
        # Check for crossed book (highest bid > lowest ask)
        if orderbook_data['bids'] and orderbook_data['asks']:
            highest_bid = orderbook_data['bids'][0][0]
            lowest_ask = orderbook_data['asks'][0][0]
            if highest_bid >= lowest_ask:
                self.logger.error(f"Crossed orderbook detected: highest bid {highest_bid} >= lowest ask {lowest_ask}")
                return False
        
        self.logger.debug("Orderbook validation passed")
        return True
    
    def _validate_trades(self, trades_data):
        """Validate trades data."""
        # Handle common case where trades data is nested
        self.logger.debug("=== TRADES VALIDATION DEBUG ===")
        self.logger.debug(f"Trades data type: {type(trades_data)}")
        
        if isinstance(trades_data, dict):
            # Try to extract trades list from common nested structures
            self.logger.debug(f"Trades data keys: {list(trades_data.keys())}")
            
            if 'result' in trades_data and isinstance(trades_data['result'], dict) and 'list' in trades_data['result']:
                self.logger.debug("Found trades in result.list structure")
                trades_data = trades_data['result']['list']
            elif 'list' in trades_data:
                self.logger.debug("Found trades in list structure")
                trades_data = trades_data['list']
            elif 'result' in trades_data and isinstance(trades_data['result'], list):
                # Sometimes result itself might be the list
                self.logger.debug("Found trades in result structure")
                trades_data = trades_data['result']
        
        if not isinstance(trades_data, (list, pd.DataFrame)):
            self.logger.error(f"Trades data must be a list or DataFrame, got {type(trades_data)}")
            self.logger.debug(f"Trades data keys: {list(trades_data.keys()) if isinstance(trades_data, dict) else 'N/A'}")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return False
        
        # If it's a DataFrame, check it's not empty
        if isinstance(trades_data, pd.DataFrame):
            if trades_data.empty:
                self.logger.warning("Trades DataFrame is empty")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return True  # Empty trades data is technically valid
            
            # Check required columns - look for either amount or size field
            required_columns = ['timestamp', 'price']
            size_field_options = ['amount', 'size', 'quantity', 'qty']
            
            # Check if at least one size field option exists
            has_size_field = any(field in trades_data.columns for field in size_field_options)
            
            missing_columns = [col for col in required_columns if col not in trades_data.columns]
            if missing_columns or not has_size_field:
                missing_str = ", ".join(missing_columns)
                size_msg = "no size field (need one of: amount, size, quantity, qty)" if not has_size_field else ""
                self.logger.warning(f"Trades DataFrame missing columns: {missing_str} {size_msg}")
            
            # Check for negative prices or amounts
            if 'price' in trades_data.columns:
                neg_prices = trades_data[trades_data['price'] <= 0]
                if not neg_prices.empty:
                    self.logger.error(f"Found {len(neg_prices)} trades with negative/zero prices")
                    self.logger.debug(f"Sample negative price trade: {neg_prices.iloc[0].to_dict()}")
            
            # Check any available size field
            for size_field in size_field_options:
                if size_field in trades_data.columns:
                    neg_amounts = trades_data[trades_data[size_field] <= 0]
                    if not neg_amounts.empty:
                        self.logger.error(f"Found {len(neg_amounts)} trades with negative/zero {size_field}")
                        self.logger.debug(f"Sample negative {size_field} trade: {neg_amounts.iloc[0].to_dict()}")
                    break
            
            # Check timestamp order if available
            if 'timestamp' in trades_data.columns and trades_data['timestamp'].is_monotonic_decreasing:
                self.logger.warning("Trades are not in chronological order (newest first)")
            
            self.logger.debug(f"Trades validation passed with {len(trades_data)} trades")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return True
        
        # If it's a list, check it's not empty
        if len(trades_data) == 0:
            self.logger.warning("Trades list is empty")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return True  # Empty trades data is technically valid
        
        # Log some info about the trades list
        self.logger.debug(f"Trades list contains {len(trades_data)} trades")
        if trades_data:
            self.logger.debug(f"First trade type: {type(trades_data[0])}")
            if isinstance(trades_data[0], dict):
                self.logger.debug(f"First trade keys: {list(trades_data[0].keys())}")
                self.logger.debug(f"First trade values: {trades_data[0]}")
        
        # Check each trade
        for i, trade in enumerate(trades_data[:100]):  # Check at most 100 trades
            if not isinstance(trade, dict):
                self.logger.error(f"Trade at index {i} is not a dictionary, type: {type(trade)}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for price field
            if 'price' not in trade:
                self.logger.error(f"Trade at index {i} missing price field")
                self.logger.debug(f"Trade keys: {list(trade.keys())}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for amount/quantity/size field (different exchanges use different names)
            size_field = None
            for field in ['amount', 'size', 'quantity', 'qty']:
                if field in trade:
                    size_field = field
                    break
                    
            if not size_field:
                self.logger.error(f"Trade at index {i} missing size field (looked for: amount, size, quantity, qty)")
                self.logger.debug(f"Trade keys: {list(trade.keys())}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for positive price
            price = trade['price']
            # Only log if there's an issue
            try:
                price_value = float(price) if isinstance(price, str) else price
                if price_value <= 0:
                    self.logger.error(f"Trade at index {i} has invalid price: {price} (converted to {price_value})")
                    self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                    return False
            except (ValueError, TypeError) as e:
                self.logger.error(f"Error converting price {price} to float: {str(e)}")
                self.logger.debug(f"Price validation failed for trade {i}: price={price} (type: {type(price)})")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for positive amount/size
            amount = trade[size_field]
            # Only log if there's an issue
            try:
                amount_value = float(amount) if isinstance(amount, str) else amount
                if amount_value <= 0:
                    self.logger.error(f"Trade at index {i} has invalid {size_field}: {amount} (converted to {amount_value})")
                    self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                    return False
            except (ValueError, TypeError) as e:
                self.logger.error(f"Error converting {size_field} {amount} to float: {str(e)}")
                self.logger.debug(f"Size validation failed for trade {i}: {size_field}={amount} (type: {type(amount)})")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
        
        self.logger.debug(f"Trades validation passed with {len(trades_data)} trades")
        self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
        return True
    
    def _validate_ticker(self, ticker_data):
        """Validate ticker data."""
        if not isinstance(ticker_data, dict):
            self.logger.error(f"Ticker data must be a dictionary, got {type(ticker_data)}")
            return False
        
        # Add volume field if it's missing but available in raw data
        if 'volume' not in ticker_data and hasattr(self, 'exchange') and self.exchange:
            try:
                # Try to get volume from the exchange's last ticker response
                if hasattr(self.exchange, 'last_ticker_response') and self.exchange.last_ticker_response:
                    raw_ticker = self.exchange.last_ticker_response
                    if isinstance(raw_ticker, dict) and 'volume' in raw_ticker:
                        ticker_data['volume'] = raw_ticker['volume']
                        self.logger.debug(f"Added missing volume field to ticker: {ticker_data['volume']}")
            except Exception as e:
                self.logger.debug(f"Could not add volume field to ticker: {str(e)}")
        
        # Check for critical fields
        critical_fields = ['last']
        missing_critical = [field for field in critical_fields if field not in ticker_data]
        if missing_critical:
            self.logger.error(f"Missing critical fields in ticker data: {missing_critical}")
            return False
        
        # Check for recommended fields
        recommended_fields = ['bid', 'ask', 'high', 'low', 'volume']
        missing_recommended = [field for field in recommended_fields if field not in ticker_data]
        if missing_recommended:
            self.logger.warning(f"Missing recommended fields in ticker data: {missing_recommended}")
        
        # Check that numeric fields have valid values
        numeric_fields = ['last', 'bid', 'ask', 'high', 'low', 'volume']
        for field in numeric_fields:
            if field in ticker_data and ticker_data[field] is not None:
                if not isinstance(ticker_data[field], (int, float)):
                    self.logger.error(f"Ticker field {field} is not numeric: {ticker_data[field]}")
                    return False
                
                if field != 'change24h' and ticker_data[field] <= 0:
                    self.logger.error(f"Ticker field {field} has invalid value: {ticker_data[field]}")
                    return False
        
        # Check for crossed values
        if 'bid' in ticker_data and 'ask' in ticker_data:
            if ticker_data['bid'] is not None and ticker_data['ask'] is not None:
                if ticker_data['bid'] >= ticker_data['ask']:
                    self.logger.error(f"Crossed ticker values: bid ({ticker_data['bid']}) >= ask ({ticker_data['ask']})")
                    return False
        
        # Check high/low consistency
        if 'high' in ticker_data and 'low' in ticker_data:
            if ticker_data['high'] is not None and ticker_data['low'] is not None:
                if ticker_data['high'] < ticker_data['low']:
                    self.logger.error(f"Invalid ticker values: high ({ticker_data['high']}) < low ({ticker_data['low']})")
                    return False
        
        self.logger.debug("Ticker validation passed")
        return True
    
    def _validate_funding(self, funding_data):
        """Validate funding rate data."""
        if not isinstance(funding_data, dict):
            self.logger.error(f"Funding data must be a dictionary, got {type(funding_data)}")
            return False
        
        # Check for required fields
        required_fields = ['fundingRate']
        missing_fields = [field for field in required_fields if field not in funding_data]
        if missing_fields:
            self.logger.warning(f"Missing fields in funding data: {missing_fields}")
            # Not critical, could still be valid
        
        # Check that funding rate is within reasonable bounds (typically -1% to 1% for most exchanges)
        if 'fundingRate' in funding_data:
            funding_rate = funding_data['fundingRate']
            if not isinstance(funding_rate, (int, float)):
                self.logger.error(f"Funding rate is not numeric: {funding_rate}")
                return False
            
            # Check for extreme values
            if abs(funding_rate) > 0.05:  # 5% threshold
                self.logger.warning(f"Extreme funding rate detected: {funding_rate}")
        
        # Check nextFundingTime is in the future
        if 'nextFundingTime' in funding_data:
            next_time = funding_data['nextFundingTime']
            if isinstance(next_time, (int, float)):
                current_time = int(time.time() * 1000) if next_time > 1000000000000 else int(time.time())
                if next_time < current_time:
                    self.logger.warning("Next funding time is in the past")
        
        self.logger.debug("Funding data validation passed")
        return True
    
    def get_validation_stats(self):
        """Get validation statistics."""
        return self.validation_stats

class LoggingUtility:
    """Centralized logging utility to standardize and consolidate logging operations."""
    
    def __init__(self, logger):
        """Initialize the logging utility.
        
        Args:
            logger: Logger instance to use for logging
        """
        self.logger = logger
    
    def log_raw_response(self, data_type: str, symbol: str, data: Any) -> None:
        """Log raw API responses with detailed structure analysis.
        
        Args:
            data_type: Type of data (OHLCV, Orderbook, Trades)
            symbol: Symbol being processed
            data: Raw response data
        """
        try:
            self.logger.debug(f"\n=== Raw {data_type} Response for {symbol} ===")
            
            # Handle different data types appropriately
            if data_type == 'OHLCV':
                self._log_ohlcv_data(data)
            elif data_type == 'Orderbook':
                self._log_orderbook_data(data)
            elif data_type == 'Trades':
                self._log_trades_data(data)
            else:
                # Generic logging for other data types
                if isinstance(data, dict):
                    self.logger.debug(f"Dictionary with {len(data)} keys: {list(data.keys())}")
                    for key in list(data.keys())[:5]:  # Show first 5 keys
                        value = data[key]
                        self.logger.debug(f"  {key}: {type(value)} {self._format_sample(value)}")
                elif isinstance(data, list):
                    self.logger.debug(f"List with {len(data)} items")
                    for i, item in enumerate(data[:3]):  # Show first 3 items
                        self.logger.debug(f"  [{i}]: {type(item)} {self._format_sample(item)}")
                elif isinstance(data, pd.DataFrame):
                    self.logger.debug(f"DataFrame with shape {data.shape}")
                    self.logger.debug(f"Columns: {list(data.columns)}")
                    if not data.empty:
                        self.logger.debug(f"Sample:\n{data.head(2)}")
                else:
                    self.logger.debug(f"Type: {type(data)}")
                    self.logger.debug(f"Value: {self._format_sample(data)}")
            
            self.logger.debug(f"=== End Raw {data_type} Response ===\n")
        except Exception as e:
            self.logger.warning(f"Error logging raw {data_type} response: {str(e)}")
    
    def _log_ohlcv_data(self, data: Any) -> None:
        """Log OHLCV data with detailed analysis."""
        if isinstance(data, dict):
            # Handle timeframe dictionary
            self.logger.debug(f"OHLCV contains {len(data)} timeframes: {list(data.keys())}")
            
            for tf, tf_data in data.items():
                self.logger.debug(f"\nTimeframe: {tf}")
                
                if isinstance(tf_data, pd.DataFrame):
                    self._log_ohlcv_dataframe(tf_data)
                elif isinstance(tf_data, dict) and 'data' in tf_data and isinstance(tf_data['data'], pd.DataFrame):
                    self._log_ohlcv_dataframe(tf_data['data'])
                else:
                    self.logger.debug(f"  Type: {type(tf_data)}")
        
        elif isinstance(data, pd.DataFrame):
            # Handle direct DataFrame
            self._log_ohlcv_dataframe(data)
        
        elif isinstance(data, list):
            # Handle list format (common in raw API responses)
            self.logger.debug(f"OHLCV data is a list with {len(data)} items")
            
            if data:
                # Check first item to determine format
                first_item = data[0]
                if isinstance(first_item, list):
                    self.logger.debug(f"List format with {len(first_item)} columns")
                    self.logger.debug(f"First candle: {first_item}")
                    
                    # Try to convert to DataFrame for better analysis
                    try:
                        columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
                        if len(first_item) == len(columns):
                            df = pd.DataFrame(data, columns=columns)
                            self._log_ohlcv_dataframe(df)
                    except Exception as e:
                        self.logger.debug(f"Could not convert to DataFrame: {str(e)}")
    
    def _log_ohlcv_dataframe(self, df: pd.DataFrame) -> None:
        """Log details of OHLCV DataFrame."""
        self.logger.debug(f"  Shape: {df.shape}")
        self.logger.debug(f"  Columns: {list(df.columns)}")
        self.logger.debug(f"  Index type: {type(df.index)}")
        
        if not df.empty:
            # Show basic statistics
            self.logger.debug("\n  Basic statistics:")
            stats = df.describe().transpose()
            self.logger.debug(f"{stats}")
            
            # Check for NaN values
            nan_count = df.isna().sum().sum()
            if nan_count > 0:
                self.logger.debug(f"\n  Contains {nan_count} NaN values")
                self.logger.debug(f"  NaN counts by column:\n{df.isna().sum()}")
            
            # Log time range
            if hasattr(df, 'index') and isinstance(df.index, pd.DatetimeIndex):
                self.logger.debug(f"\n  Time range: {df.index.min()} to {df.index.max()}")
                self.logger.debug(f"  Duration: {df.index.max() - df.index.min()}")
            
            # Show first and last candles
            self.logger.debug("\n  First candle:")
            self.logger.debug(f"{df.iloc[0]}")
            self.logger.debug("\n  Last candle:")
            self.logger.debug(f"{df.iloc[-1]}")
    
    def _log_orderbook_data(self, data: Any) -> None:
        """Log orderbook data with detailed analysis."""
        if not isinstance(data, dict):
            self.logger.debug(f"Orderbook is not a dictionary: {type(data)}")
            return
        
        # Check for required fields
        required_fields = ['bids', 'asks']
        missing_fields = [field for field in required_fields if field not in data]
        
        if missing_fields:
            self.logger.debug(f"Orderbook missing required fields: {missing_fields}")
        
        # Log basic statistics
        if 'bids' in data and isinstance(data['bids'], list):
            bids = data['bids']
            self.logger.debug(f"Bids: {len(bids)} levels")
            if bids:
                self.logger.debug(f"  Top 3 bids:")
                for i, bid in enumerate(bids[:3]):
                    self.logger.debug(f"    {i+1}. Price: {bid[0]}, Qty: {bid[1]}")
                
                # Calculate bid sum
                bid_qty_sum = sum(bid[1] for bid in bids)
                self.logger.debug(f"  Total bid quantity: {bid_qty_sum}")
        
        if 'asks' in data and isinstance(data['asks'], list):
            asks = data['asks']
            self.logger.debug(f"Asks: {len(asks)} levels")
            if asks:
                self.logger.debug(f"  Top 3 asks:")
                for i, ask in enumerate(asks[:3]):
                    self.logger.debug(f"    {i+1}. Price: {ask[0]}, Qty: {ask[1]}")
                
                # Calculate ask sum
                ask_qty_sum = sum(ask[1] for ask in asks)
                self.logger.debug(f"  Total ask quantity: {ask_qty_sum}")
        
        # Calculate bid-ask spread if possible
        if 'bids' in data and 'asks' in data and data['bids'] and data['asks']:
            best_bid = data['bids'][0][0]
            best_ask = data['asks'][0][0]
            spread = best_ask - best_bid
            spread_pct = (spread / best_bid) * 100
            
            self.logger.debug(f"\nBid-ask spread: {spread} ({spread_pct:.4f}%)")
            self.logger.debug(f"Best bid: {best_bid}, Best ask: {best_ask}")
    
    def _log_trades_data(self, data: Any) -> None:
        """Log trades data with detailed analysis."""
        if isinstance(data, list):
            self.logger.debug(f"Trades: {len(data)} trades")
            
            if not data:
                return
            
            # Show sample trades
            self.logger.debug("\nSample trades:")
            for i, trade in enumerate(data[:3]):
                self.logger.debug(f"  Trade {i+1}: {trade}")
            
            # Try to calculate some statistics
            try:
                # Extract prices and volumes more efficiently
                prices = []
                volumes = []
                for trade in data:
                    if 'price' in trade:
                        prices.append(float(trade.get('price', 0)))
                    if 'amount' in trade or 'quantity' in trade:
                        volumes.append(float(trade.get('amount', trade.get('quantity', 0))))
                
                if prices:
                    # Use numpy for efficient statistics calculation
                    prices_array = np.array(prices)
                    avg_price = np.mean(prices_array)
                    min_price = np.min(prices_array)
                    max_price = np.max(prices_array)
                    
                    self.logger.debug(f"\nPrice statistics:")
                    self.logger.debug(f"  Min: {min_price}, Max: {max_price}, Avg: {avg_price:.4f}")
                
                if volumes:
                    # Use numpy for efficient volume statistics
                    volumes_array = np.array(volumes)
                    total_volume = np.sum(volumes_array)
                    avg_volume = np.mean(volumes_array)
                    
                    self.logger.debug(f"\nVolume statistics:")
                    self.logger.debug(f"  Total: {total_volume}, Avg: {avg_volume:.4f}")
                
                # Count buy vs sell trades if side is available
                if data and len(data) > 0 and 'side' in data[0]:
                    # Count trades by side in one pass through the data
                    buy_count = 0
                    sell_count = 0
                    for trade in data:
                        side = trade.get('side')
                        if side == 'buy':
                            buy_count += 1
                        elif side == 'sell':
                            sell_count += 1
                
                    self.logger.debug(f"\nTrade directions:")
                    self.logger.debug(f"  Buy: {buy_count} ({buy_count/len(data)*100:.1f}%)")
                    self.logger.debug(f"  Sell: {sell_count} ({sell_count/len(data)*100:.1f}%)")
            
            except Exception as e:
                self.logger.debug(f"Error calculating trade statistics: {str(e)}")
        
        elif isinstance(data, pd.DataFrame):
            self.logger.debug(f"Trades DataFrame with shape {data.shape}")
            
            if data.empty:
                return
            
            self.logger.debug(f"Columns: {list(data.columns)}")
            self.logger.debug(f"\nSample trades:\n{data.head(3)}")
            
            # Show basic statistics
            try:
                if 'price' in data.columns:
                    self.logger.debug(f"\nPrice statistics:")
                    self.logger.debug(f"  Min: {data['price'].min()}, Max: {data['price'].max()}, "
                                     f"Avg: {data['price'].mean():.4f}")
                
                if 'amount' in data.columns:
                    self.logger.debug(f"\nVolume statistics:")
                    self.logger.debug(f"  Total: {data['amount'].sum()}, Avg: {data['amount'].mean():.4f}")
                
                # Count buy vs sell trades if side is available
                if 'side' in data.columns:
                    counts = data['side'].value_counts()
                    self.logger.debug(f"\nTrade directions:\n{counts}")
            
            except Exception as e:
                self.logger.debug(f"Error calculating trade statistics: {str(e)}")
    
    def _format_sample(self, value: Any) -> str:
        """Format a sample of a value for logging."""
        if isinstance(value, (str, int, float, bool)):
            return str(value)
        elif isinstance(value, dict):
            return f"{{...}} with {len(value)} keys"
        elif isinstance(value, list):
            return f"[...] with {len(value)} items"
        elif isinstance(value, pd.DataFrame):
            return f"DataFrame with shape {value.shape}"
        else:
            return str(type(value))

    def log_operation(self, operation_name: str, details: Optional[Dict[str, Any]] = None) -> None:
        """Log the start of an operation with standardized formatting.
        
        Args:
            operation_name: Name of the operation being performed
            details: Optional dictionary of details to log
        """
        self.logger.debug(f"\n=== {operation_name} ===")
        if details:
            for key, value in details.items():
                self.logger.debug(f"{key}: {value}")

    def log_operation_result(self, operation_name: str, success: bool, details: Optional[Dict[str, Any]] = None) -> None:
        """Log the result of an operation with standardized formatting.
        
        Args:
            operation_name: Name of the operation that was performed
            success: Whether the operation was successful
            details: Optional dictionary of details to log
        """
        status = "SUCCESS" if success else "FAILED"
        self.logger.debug(f"=== {operation_name}: {status} ===")
        if details:
            for key, value in details.items():
                self.logger.debug(f"{key}: {value}")
        self.logger.debug("\n")

class TimestampUtility:
    """Utility class for standardized timestamp handling."""
    
    @staticmethod
    def get_utc_timestamp(as_ms: bool = True) -> int:
        """Get current UTC timestamp.
        
        Args:
            as_ms: If True, return millisecond timestamp, else seconds
        
        Returns:
            Current UTC timestamp in milliseconds or seconds
        """
        ts = datetime.now(timezone.utc).timestamp()
        return int(ts * 1000) if as_ms else int(ts)
    
    @staticmethod
    def format_utc_time(timestamp_ms: int) -> str:
        """Format millisecond timestamp to human-readable UTC time.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            Formatted UTC time string
        """
        dt_object = datetime.fromtimestamp(timestamp_ms / 1000, tz=timezone.utc)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] + ' UTC'
    
    @staticmethod
    def milliseconds_to_datetime(timestamp_ms: int) -> datetime:
        """Convert millisecond timestamp to datetime object.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            datetime object in UTC timezone
        """
        return datetime.fromtimestamp(timestamp_ms / 1000, tz=timezone.utc)
    
    @staticmethod
    def datetime_to_milliseconds(dt_object: datetime) -> int:
        """Convert datetime object to millisecond timestamp.
        
        Args:
            dt_object: datetime object
            
        Returns:
            Timestamp in milliseconds
        """
        return int(dt_object.timestamp() * 1000)
    
    @staticmethod
    def get_age_seconds(timestamp_ms: int) -> float:
        """Calculate age in seconds from a millisecond timestamp.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            Age in seconds
        """
        current_ms = TimestampUtility.get_utc_timestamp(as_ms=True)
        return (current_ms - timestamp_ms) / 1000
    
    @staticmethod
    def is_timestamp_fresh(timestamp_ms: int, max_age_seconds: float) -> bool:
        """Check if a timestamp is fresh within a maximum age.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            max_age_seconds: Maximum age in seconds
            
        Returns:
            True if timestamp is fresh, False otherwise
        """
        return TimestampUtility.get_age_seconds(timestamp_ms) <= max_age_seconds

class MarketMonitor:
    """Class for monitoring market data from exchanges."""
    
    def __init__(
        self,
        exchange=None,
        symbol: Optional[str] = None,
        exchange_manager=None,
        database_client=None,
        portfolio_analyzer=None,
        confluence_analyzer=None,
        timeframes: Optional[Dict[str, str]] = None,
        logger: Optional[logging.Logger] = None,
        metrics_manager: Optional[MetricsManager] = None,
        health_monitor: Optional[HealthMonitor] = None,
        validation_config: Optional[Dict[str, Any]] = None,
        config: Optional[Dict[str, Any]] = None,
        alert_manager=None,
        signal_generator=None,
        top_symbols_manager=None,
        market_data_manager=None,
        **kwargs
    ):
        """Initialize MarketMonitor.
        
        Args:
            exchange: ccxt exchange instance (can be None if exchange_manager is provided)
            symbol: Trading pair symbol (can be None if using TopSymbolsManager)
            exchange_manager: Exchange manager instance
            database_client: Database client for storing analysis results
            portfolio_analyzer: Portfolio analyzer for position management
            confluence_analyzer: Confluence analyzer for market analysis
            timeframes: Dictionary of timeframes to fetch (keys: 'ltf', 'mtf', 'htf', values: timeframe strings)
            logger: Optional logger instance
            metrics_manager: Optional metrics manager instance
            health_monitor: Optional health monitor instance
            validation_config: Optional validation configuration
            config: Optional configuration dictionary
            alert_manager: Optional alert manager instance
            signal_generator: Optional signal generator instance
            top_symbols_manager: Optional top symbols manager instance
            market_data_manager: Optional market data manager instance
        """
        # Store core components
        self.exchange_manager = exchange_manager
        self.database_client = database_client
        self.portfolio_analyzer = portfolio_analyzer
        self.confluence_analyzer = confluence_analyzer
        self.alert_manager = alert_manager
        self.signal_generator = signal_generator
        self.top_symbols_manager = top_symbols_manager
        self.market_data_manager = market_data_manager
        
        # Store exchange info
        self.exchange = exchange
        self.exchange_id = getattr(exchange, 'id', None) if exchange else 'unknown'
        
        # Store full configuration
        self.config = config or {}
        
        # Setup symbol (can be None if using TopSymbolsManager)
        self.symbol = symbol
        self.symbol_str = symbol.replace('/', '') if symbol else None  # Remove / for filename compatibility
        
        # Set up logger
        self.logger = logger or logging.getLogger(__name__)
        
        # Set up logging utility
        self.logging_utility = LoggingUtility(self.logger)
        
        # Set up metrics manager
        self.metrics_manager = metrics_manager or MetricsManager()
        
        # Set up health monitor
        self.health_monitor = health_monitor
        if self.health_monitor and self.exchange_id:
            # Register this exchange with the health monitor
            self.health_monitor.register_api(self.exchange_id)
        
        # Initialize timeframes
        default_timeframes = {'ltf': '1m', 'mtf': '15m', 'htf': '1h'}
        self.timeframes = default_timeframes.copy()
        if timeframes:
            self.timeframes.update(timeframes)
        
        # Add base timeframe (use the lowest timeframe)
        timeframe_values = sorted(self.timeframes.values(), key=lambda x: ccxt_time_to_minutes(x))
        self.timeframes['base'] = timeframe_values[0] if timeframe_values else '1m'
        
        # Configure validation
        default_validation = {
            'max_ohlcv_age_seconds': 300,  # Max age of newest candle in seconds
            'min_ohlcv_candles': 20,       # Minimum number of candles required
            'max_orderbook_age_seconds': 60, # Max age of orderbook in seconds
            'min_orderbook_levels': 5,      # Minimum orderbook levels required
            'max_trades_age_seconds': 300,  # Max age of newest trade in seconds
            'min_trades_count': 5,          # Minimum number of trades required
        }
        self.validation_config = default_validation.copy()
        if validation_config:
            self.validation_config.update(validation_config)
        
        # Rate limiting configuration
        self.rate_limit_config = kwargs.get('rate_limit_config', {
            'enabled': True,
            'max_requests_per_second': 5,  # Default: 5 requests per second
            'timeout_seconds': 10          # Default: 10 second timeout
        })
        
        # Retry configuration
        self.retry_config = kwargs.get('retry_config', {
            'max_retries': 3,              # Default: 3 retries
            'retry_delay_seconds': 2,      # Default: 2 second delay between retries
            'retry_exponential_backoff': True  # Exponential backoff
        })
        
        # Debug configuration
        self.debug_config = kwargs.get('debug_config', {
            'log_raw_responses': False,    # Log raw API responses
            'verbose_validation': False,   # Verbose validation logs
            'save_visualizations': False,  # Save visualizations to disk
            'visualization_dir': 'visualizations'  # Directory for saved visualizations
        })
        
        # Create visualization directory if needed
        if self.debug_config.get('save_visualizations', False):
            Path(self.debug_config.get('visualization_dir', 'visualizations')).mkdir(parents=True, exist_ok=True)
        
        # WebSocket configuration and initialization
        self.websocket_config = kwargs.get('websocket_config', {
            'enabled': True,               # Enable/disable WebSocket
            'use_ws_for_orderbook': True,  # Use WebSocket for orderbook updates
            'use_ws_for_trades': True,     # Use WebSocket for trades updates
            'use_ws_for_tickers': True     # Use WebSocket for ticker updates
        })
        
        # Initialize WebSocket Manager if enabled
        self.ws_manager = None
        self.ws_data = {
            'orderbook': None,             # Latest orderbook from WebSocket
            'trades': [],                  # Recent trades from WebSocket
            'ticker': None,                # Latest ticker from WebSocket
            'kline': {},                   # OHLCV data from WebSocket
            'last_update_time': {          # Last update timestamp for each data type
                'orderbook': 0,
                'trades': 0,
                'ticker': 0,
                'kline': 0
            }
        }
        
        # Initialize monitoring state variables
        self.running = False
        self._last_update_time = 0
        self._error_count = 0
        self.last_report_time = None
        # Ensure report_times is properly initialized at startup
        self.report_times = self._calculate_report_times() if hasattr(self, '_calculate_report_times') else []
        self._stats = {
            'total_messages': 0,
            'invalid_messages': 0,
            'delayed_messages': 0,
            'error_count': 0
        }
        self.interval = self.config.get('monitoring', {}).get('interval', 60)
        self.data_validator = MarketDataValidator(self.logger) if 'MarketDataValidator' in globals() else None
        
        # Initialize active state for monitoring
        self._active = True
        
        # Initialize large order monitoring
        self.large_order_config = self.config.get('monitoring', {}).get('large_orders', {
            'enabled': True,
            'threshold_usd': 1000000,  # $1M default threshold
            'cooldown': 300,           # 5 minutes between alerts for same symbol
            'min_price': 0,            # No minimum price filter
            'excluded_symbols': [],    # No excluded symbols 
            'included_symbols': []     # Monitor all symbols
        })
        self._last_check = {}
        
        # Initialize WebSocket if enabled and we have a symbol
        if self.websocket_config.get('enabled', True) and self.symbol_str:
            self._initialize_websocket()
        
        # Initialize market data cache for fetch_market_data method
        self._market_data_cache = {}
        self._cache_ttl = 300  # 5 minutes default cache TTL
        self._last_ohlcv_update = {}
        self._ohlcv_cache = {}  # Initialize OHLCV cache
        
        self.logger.info(f"Initialized MarketMonitor for {self.exchange_id}")
        
        # Add TimestampUtility instance
        self.timestamp_utility = TimestampUtility()
        
        # Initialize MarketReporter
        self.market_reporter = MarketReporter(
            exchange=self.exchange,
            logger=self.logger,
            top_symbols_manager=top_symbols_manager,
            alert_manager=alert_manager
        )

    def _initialize_websocket(self) -> None:
        """Initialize WebSocket connection for real-time data."""
        try:
            # Skip if no symbol is provided
            if not self.symbol_str:
                self.logger.info("Skipping WebSocket initialization: No symbol provided")
                return
                
            # Initialize WebSocket Manager with the same config
            self.ws_manager = WebSocketManager(self.config)
            
            # Register callback for WebSocket messages
            self.ws_manager.register_message_callback(self._process_websocket_message)
            
            # Create the list of symbols for the WebSocket manager to track
            symbols = [self.symbol_str]
            
            # Initialize asynchronously using create_task
            # Note: This will be executed when the event loop is running
            asyncio.create_task(self.ws_manager.initialize(symbols))
            
            self.logger.info(f"WebSocket integration initialized for {self.symbol}")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize WebSocket: {str(e)}")
            self.logger.debug(traceback.format_exc())
            self.ws_manager = None
            
            # Update health status if available
            if self.health_monitor:
                self.health_monitor._create_alert(
                    level="warning",
                    source=f"websocket:{self.exchange_id}",
                    message=f"Failed to initialize WebSocket: {str(e)}"
                )

    async def _process_websocket_message(self, symbol: str, topic: str, message: Dict[str, Any]) -> None:
        """Process WebSocket message and update internal data.
        
        Args:
            symbol: Trading pair symbol
            topic: Message topic
            message: WebSocket message data
        """
        try:
            # Start performance tracking
            operation = self.metrics_manager.start_operation(f"process_ws_message_{topic}")
            
            # Check if the message is for the symbol we're monitoring
            if symbol != self.symbol_str:
                self.metrics_manager.end_operation(operation)
                return
                
            # Process based on topic type
            if "tickers" in topic:
                await self._process_ticker_update(message)
            elif "kline" in topic:
                await self._process_kline_update(message)
            elif "orderbook" in topic:
                await self._process_orderbook_update(message)
            elif "publicTrade" in topic:
                await self._process_trade_update(message)
            elif "liquidation" in topic:
                await self._process_liquidation_update(message)
            else:
                self.logger.debug(f"Received unhandled topic: {topic}")
                
            # Record metrics
            self.metrics_manager.record_metric("websocket_messages_processed", 1)
            self.metrics_manager.record_metric(f"websocket_messages_{topic}", 1)
            
            # End operation
            self.metrics_manager.end_operation(operation)
            
        except Exception as e:
            self.logger.error(f"Error processing WebSocket message for {topic}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # End operation if it was started
            if 'operation' in locals():
                self.metrics_manager.end_operation(operation, success=False)

    # Add placeholder methods for processing different message types
    async def _process_ticker_update(self, message: Dict[str, Any]) -> None:
        """Process ticker update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract ticker data
            ticker_data = data.get('data', {})
            if not ticker_data:
                return
                
            # Format ticker data
            ticker = {
                'symbol': self.symbol,
                'last': float(ticker_data.get('lastPrice', 0)),
                'bid': float(ticker_data.get('bid1Price', 0)),
                'ask': float(ticker_data.get('ask1Price', 0)),
                'high': float(ticker_data.get('highPrice24h', 0)),
                'low': float(ticker_data.get('lowPrice24h', 0)),
                'volume': float(ticker_data.get('volume24h', 0)),
                'timestamp': int(ticker_data.get('time', timestamp))
            }
            
            # Add additional data if available
            if 'openInterest' in ticker_data:
                ticker['openInterest'] = float(ticker_data['openInterest'])
            if 'fundingRate' in ticker_data:
                ticker['fundingRate'] = float(ticker_data['fundingRate'])
            if 'nextFundingTime' in ticker_data:
                ticker['nextFundingTime'] = int(ticker_data['nextFundingTime'])
            
            # Update internal state
            self.ws_data['ticker'] = ticker
            self.ws_data['last_update_time']['ticker'] = timestamp
            
            # Log update
            self.logger.debug(f"Updated ticker data from WebSocket: Last price: {ticker['last']}")
            
        except Exception as e:
            self.logger.error(f"Error processing ticker update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_kline_update(self, message: Dict[str, Any]) -> None:
        """Process OHLCV update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract kline data
            kline_data = data.get('data', [])
            if not kline_data:
                return
                
            # Get interval from topic
            topic = message.get('topic', '')
            interval = '1'  # Default to 1 minute
            if '.' in topic:
                parts = topic.split('.')
                if len(parts) > 1:
                    interval = parts[1]  # Extract interval from topic
            
            # Map to standard timeframe key
            timeframe_map = {
                '1': 'base',
                '5': 'ltf',
                '30': 'mtf',
                '60': 'mtf',
                '240': 'htf',
                '1D': 'htf'
            }
            
            tf_key = timeframe_map.get(interval, 'base')
            
            # Format candle data
            candles = []
            for candle in kline_data:
                formatted_candle = {
                    'timestamp': int(candle.get('timestamp', 0) or candle.get('start', 0)),
                    'open': float(candle.get('open', 0)),
                    'high': float(candle.get('high', 0)),
                    'low': float(candle.get('low', 0)),
                    'close': float(candle.get('close', 0)),
                    'volume': float(candle.get('volume', 0))
                }
                candles.append(formatted_candle)
            
            # Create DataFrame
            if candles:
                df = pd.DataFrame(candles)
                if 'timestamp' in df.columns:
                    df.set_index('timestamp', inplace=True)
                    df.index = pd.to_datetime(df.index, unit='ms')
                
                # Update internal state
                if tf_key not in self.ws_data['kline']:
                    self.ws_data['kline'][tf_key] = df
                else:
                    # Merge with existing data
                    existing_df = self.ws_data['kline'][tf_key]
                    merged_df = pd.concat([existing_df, df])
                    
                    # Remove duplicates
                    merged_df = merged_df[~merged_df.index.duplicated(keep='last')]
                    
                    # Sort by index
                    merged_df.sort_index(inplace=True)
                    
                    # Keep only the latest candles (max 1000)
                    if len(merged_df) > 1000:
                        merged_df = merged_df.iloc[-1000:]
                        
                    self.ws_data['kline'][tf_key] = merged_df
                
                self.ws_data['last_update_time']['kline'] = timestamp
                
                # Log update
                self.logger.debug(f"Updated {tf_key} OHLCV data from WebSocket: {len(candles)} candles")
                
        except Exception as e:
            self.logger.error(f"Error processing kline update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_orderbook_update(self, message: Dict[str, Any]) -> None:
        """Process orderbook update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract orderbook data
            orderbook_data = data.get('data', {})
            if not orderbook_data:
                return
            
            # Format orderbook data
            orderbook = {
                'symbol': self.symbol,
                'timestamp': int(orderbook_data.get('timestamp', timestamp)),
                'bids': orderbook_data.get('bids', []),
                'asks': orderbook_data.get('asks', [])
            }
            
            # Sort bids (desc) and asks (asc)
            if orderbook['bids']:
                orderbook['bids'] = sorted(orderbook['bids'], key=lambda x: float(x[0]), reverse=True)
            if orderbook['asks']:
                orderbook['asks'] = sorted(orderbook['asks'], key=lambda x: float(x[0]))
            
            # Update internal state
            self.ws_data['orderbook'] = orderbook
            self.ws_data['last_update_time']['orderbook'] = timestamp
            
            # Log update
            self.logger.debug(f"Updated orderbook from WebSocket: {len(orderbook['bids'])} bids, {len(orderbook['asks'])} asks")
            
        except Exception as e:
            self.logger.error(f"Error processing orderbook update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_trade_update(self, message: Dict[str, Any]) -> None:
        """Process trade update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract trade data
            trade_data = data.get('data', [])
            if not trade_data:
                return
            
            # Format trade data
            trades = []
            for trade in trade_data:
                formatted_trade = {
                    'id': trade.get('tradeId', str(timestamp) + str(len(self.ws_data['trades']))),
                    'timestamp': int(trade.get('timestamp', timestamp)),
                    'price': float(trade.get('price', 0)),
                    'amount': float(trade.get('size', 0)),
                    'side': trade.get('side', 'unknown').lower(),
                    'symbol': self.symbol
                }
                trades.append(formatted_trade)
            
            # Update internal state - keep only most recent 1000 trades
            self.ws_data['trades'] = (trades + self.ws_data['trades'])[:1000]
            self.ws_data['last_update_time']['trades'] = timestamp
            
            # Log update
            self.logger.debug(f"Added {len(trades)} new trades from WebSocket. Total: {len(self.ws_data['trades'])}")
            
        except Exception as e:
            self.logger.error(f"Error processing trade update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_liquidation_update(self, message: Dict[str, Any]) -> None:
        """Process liquidation update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Enhanced debug logging for incoming message
            self.logger.debug(f"RECEIVED LIQUIDATION MSG: {json.dumps(message, default=str)[:200]}...")
            
            # Extract liquidation data
            liquidation_data = data.get('data', {})
            if not liquidation_data:
                self.logger.warning("Empty liquidation data received")
                return
            
            # Format liquidation data
            liquidation = {
                'symbol': self.symbol,
                'timestamp': int(liquidation_data.get('timestamp', timestamp)),
                'price': float(liquidation_data.get('price', 0)),
                'size': float(liquidation_data.get('size', 0)),
                'side': liquidation_data.get('side', 'unknown').lower(),
                'source': 'websocket'
            }
            
            # Log liquidation event
            self.logger.warning(f"Liquidation detected: {liquidation['side']} {liquidation['size']} {self.symbol} @ {liquidation['price']}")
            
            # Save to cache
            symbol_str = self._get_symbol_string(self.symbol)
            self.logger.debug(f"SAVING TO LIQUIDATION CACHE: {symbol_str} -> {liquidation}")
            liquidation_cache.append(liquidation, symbol_str)
            
            # Verify cache immediately after saving
            cached_data = liquidation_cache.load(symbol_str)
            self.logger.debug(f"VERIFICATION - CACHE NOW CONTAINS: {len(cached_data) if cached_data else 0} events")
            
            # If health monitor is available, create alert
            if self.health_monitor:
                self.health_monitor._create_alert(
                    level="info",
                    source=f"liquidation:{self.exchange_id}:{self.symbol_str}",
                    message=f"Liquidation: {liquidation['side']} {liquidation['size']} {self.symbol} @ {liquidation['price']}"
                )
            
        except Exception as e:
            self.logger.error(f"Error processing liquidation update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def close(self) -> None:
        """Close connections and clean up resources."""
        try:
            self.logger.info(f"Closing MarketMonitor for {self.symbol}")
            
            # Close WebSocket connection if available
            if self.ws_manager:
                await self.ws_manager.close()
                self.logger.info("WebSocket connection closed")
            
        except Exception as e:
            self.logger.error(f"Error closing MarketMonitor: {str(e)}")
            self.logger.debug(traceback.format_exc())

    def get_websocket_status(self) -> Dict[str, Any]:
        """Get current WebSocket status."""
        if self.ws_manager:
            status = self.ws_manager.get_status()
            # Add data freshness
            status['data_freshness'] = {
                data_type: time.time() - timestamp/1000 if timestamp > 0 else float('inf')
                for data_type, timestamp in self.ws_data['last_update_time'].items()
            }
            return status
        else:
            return {
                'connected': False,
                'enabled': self.websocket_config.get('enabled', False)
            }

    def _handle_shutdown(self, signum, frame):
        """Handle shutdown signals gracefully"""
        logger.info("\nShutdown signal received. Cleaning up...")
        self.running = False

    async def start(self):
        """Start the market monitor."""
        try:
            # Check for required components
            if not self.exchange_manager:
                self.logger.error("No exchange manager available")
                return
                
            if not self.top_symbols_manager:
                self.logger.error("No top symbols manager available")
                return
                
            if not self.market_data_manager:
                self.logger.error("No market data manager available")
                return
            
            # Get primary exchange from exchange manager if not already set
            if not self.exchange:
                self.exchange = await self.exchange_manager.get_primary_exchange()
                if not self.exchange:
                    self.logger.error("No primary exchange available")
                    return
                    
                # Update exchange ID
                self.exchange_id = self.exchange.exchange_id
                
            self.logger.debug(f"Exchange instance retrieved: {bool(self.exchange)}")
            self.logger.debug("Initializing exchange...")
            
            # Initialize exchange
            await self.exchange.initialize()
            
            self.logger.debug("Setting exchange in TopSymbolsManager...")
            self.top_symbols_manager.set_exchange(self.exchange)
            
            self.logger.info("Waiting for initial data collection...")
            self.logger.info("Updating top symbols...")
            
            # Get symbols directly from top_symbols_manager
            self.symbols = await self.top_symbols_manager.get_symbols()
            if not self.symbols:
                self.logger.warning("No symbols to monitor")
                return
                
            self.logger.info(f"Monitoring symbols: {self.symbols}")
            
            # Extract symbol strings from symbol dictionaries for components that need simple strings
            symbol_strings = []
            for symbol_data in self.symbols:
                if isinstance(symbol_data, dict) and 'symbol' in symbol_data:
                    symbol_strings.append(symbol_data['symbol'])
                elif isinstance(symbol_data, str):
                    symbol_strings.append(symbol_data)
                    
            # Initialize market data manager with symbols to monitor
            await self.market_data_manager.initialize(symbol_strings)
            
            # Start data monitoring in the market data manager
            await self.market_data_manager.start_monitoring()
            
            # Initialize monitoring state
            self.running = True
            self._last_update_time = time.time()
            
            # Start monitoring tasks
            self.logger.info("Starting monitoring tasks...")
            self._monitoring_task = asyncio.create_task(self._run_monitoring_loop())
            
            # Start metrics manager if available
            if self.metrics_manager:
                self.logger.info("Starting metrics manager...")
                await self.metrics_manager.start()
                
            # Start alert manager if available  
            if self.alert_manager:
                self.logger.info("Starting alert manager...")
                await self.alert_manager.start()
                
            # Start the scheduled market reports task if market_reporter is available
            if hasattr(self, 'market_reporter') and self.market_reporter is not None:
                self.logger.info("Starting scheduled market reports service...")
                # Create a background task for the market reporter's scheduled reports
                self._market_report_task = asyncio.create_task(self.market_reporter.run_scheduled_reports())
                self.logger.info("Scheduled market reports service started")
                
            # Generate initial market report on startup, regardless of scheduled times
            self.logger.info("Generating initial market report...")
            try:
                await self._generate_market_report()
                self.last_report_time = datetime.now(timezone.utc)
                self.logger.info("Initial market report generated successfully")
            except Exception as e:
                self.logger.error(f"Error generating initial market report: {str(e)}")
                self.logger.debug(traceback.format_exc())
                
            self.logger.info("Market monitor started successfully")
            
        except Exception as e:
            self.logger.error(f"Error starting monitor: {str(e)}")
            self.logger.debug(traceback.format_exc())
            raise

    async def _monitoring_cycle(self):
        """Run a single monitoring cycle."""
        try:
            logger.debug("=== Starting Monitoring Cycle ===")
            
            # Get symbols to monitor
            symbols = await self.top_symbols_manager.get_symbols()
            symbol_display = [s['symbol'] if isinstance(s, dict) and 'symbol' in s else s for s in symbols[:5]]
            if len(symbols) > 5:
                symbol_display.append('...')
            logger.debug(f"Symbol manager returned {len(symbols)} symbols: {symbol_display}")
            
            if not symbols:
                logger.warning("Empty symbol list detected!")
                return
            
            # Process each symbol - now using MarketDataManager for efficient data fetching
            for symbol in symbols:
                try:
                    await self._process_symbol(symbol)
                except Exception as e:
                    self.logger.error(f"Error processing {symbol}: {str(e)}")
                    continue
                
            # Check if it's time for a report
            current_time = datetime.now(timezone.utc)
            should_generate_report = (
                # Generate a report if none has been generated yet
                not self.last_report_time or
                # Or if it's a scheduled report time and enough time has passed
                (self._is_report_time() and 
                 (current_time - self.last_report_time).total_seconds() > 300)
            )
            
            if should_generate_report:
                await self._generate_market_report()
                self.last_report_time = current_time
            
        except Exception as e:
            self.logger.error(f"Monitoring cycle error: {str(e)}")
            self.logger.debug(f"Stack trace:\n{traceback.format_exc()}")

    async def _process_symbol(self, symbol: str) -> None:
        """Process a single symbol for market analysis.
        
        This method fetches market data, validates it, runs analysis, and generates alerts.
        
        Args:
            symbol: Symbol to process
        """
        if not self.exchange_manager:
            self.logger.error("Exchange manager not available")
            return

        if not self.alert_manager:
            self.logger.warning("Alert manager not initialized")
        
        try:
            # Extract symbol string if needed
            symbol_str = symbol['symbol'] if isinstance(symbol, dict) and 'symbol' in symbol else symbol
            
            # Get market data from MarketDataManager
            self.logger.info(f"=== Starting analysis process for {symbol_str} ===")
            market_data = await self.fetch_market_data(symbol_str)
            if not market_data:
                self.logger.error(f"No market data available for {symbol_str}")
                return

            # Log what data we have
            data_components = []
            if 'ticker' in market_data and market_data['ticker']:
                data_components.append('ticker')
            if 'orderbook' in market_data and market_data['orderbook']:
                data_components.append('orderbook')
            if 'trades' in market_data and isinstance(market_data['trades'], list):
                data_components.append(f"trades ({len(market_data['trades'])})")
            if 'ohlcv' in market_data and market_data['ohlcv']:
                timeframes = market_data['ohlcv'].keys()
                data_components.append(f"ohlcv ({', '.join(timeframes)})")
            
            self.logger.info(f"Market data for {symbol_str} contains: {', '.join(data_components)}")

            # Validate market data
            self.logger.info(f"Validating market data for {symbol_str}")
            try:
                # Now we can properly await the validate_market_data method since it's async
                validation_result = await self.validate_market_data(market_data)
                if not validation_result:
                    self.logger.error(f"Invalid data for {symbol_str}")
                    return
            except TypeError as e:
                self.logger.error(f"Error validating market data: {str(e)}")
                # Fall back to legacy sync validation method if needed
                if hasattr(self, 'validate_market_data_sync'):
                    try:
                        validation_result = self.validate_market_data_sync(market_data)
                        if not validation_result:
                            self.logger.error(f"Invalid data for {symbol_str} (sync validation)")
                            return
                    except Exception as inner_e:
                        self.logger.error(f"Error in sync validation: {str(inner_e)}")
                        self.logger.error("Continuing with analysis anyway despite validation errors")
                else:
                    self.logger.error("Cannot validate market data. Continuing with analysis anyway.")
            except Exception as e:
                self.logger.error(f"Unexpected error during validation: {str(e)}")
                self.logger.debug(traceback.format_exc())
                self.logger.warning("Continuing with analysis despite validation errors")

            # Check if we have the confluence analyzer
            if not self.confluence_analyzer:
                self.logger.error("Confluence analyzer not initialized - cannot perform analysis")
                return

            # Process analysis
            self.logger.info(f"=== Running confluence analysis for {symbol_str} ===")
            start_time = time.time()
            try:
                analysis_result = await self.confluence_analyzer.analyze(market_data)
                elapsed = time.time() - start_time
                self.logger.info(f"Confluence analysis completed in {elapsed:.2f}s")
                
                # Log analysis results
                if analysis_result:
                    score = analysis_result.get('score', analysis_result.get('confluence_score', 0))
                    reliability = analysis_result.get('reliability', 0)
                    # Format safely before using in f-string
                    score_str = f"{score:.2f}" if score is not None else "N/A"
                    reliability_str = f"{reliability:.2f}" if reliability is not None else "N/A"
                    self.logger.info(f"Confluence score: {score_str} (reliability: {reliability_str})")
                    
                    # Log component scores if available
                    components = analysis_result.get('components', {})
                    if components:
                        component_scores = []
                        for name, value in components.items():
                            if isinstance(value, dict) and 'score' in value:
                                component_scores.append(f"{name}: {value['score']:.2f}")
                            elif isinstance(value, (int, float)):
                                component_scores.append(f"{name}: {value:.2f}")
                        if component_scores:
                            self.logger.info(f"Component scores: {', '.join(component_scores)}")
                else:
                    self.logger.warning(f"Confluence analysis returned no results for {symbol_str}")
            except Exception as e:
                elapsed = time.time() - start_time
                self.logger.error(f"Confluence analysis failed after {elapsed:.2f}s: {str(e)}")
                self.logger.error(traceback.format_exc())
                return
            
            # Process the analysis result (generate signals, etc.)
            self.logger.info(f"Processing analysis results for {symbol_str}")
            await self._process_analysis_result(symbol_str, analysis_result)
            self.logger.info(f"=== Completed analysis process for {symbol_str} ===")

        except AttributeError as e:
            self.logger.error(f"Missing component: {str(e)}")
        except Exception as e:
            self.logger.error(f"Processing failed for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def fetch_market_data(self, symbol: str) -> Dict[str, Any]:
        """Fetch market data for a symbol from the market data manager.
        
        Args:
            symbol: The symbol to fetch market data for
            
        Returns:
            Dict[str, Any]: Market data dictionary with various components
        """
        try:
            if not self.market_data_manager:
                self.logger.error("Market data manager not initialized")
                return None
                
            # Fetch market data through the manager
            market_data = await self.market_data_manager.get_market_data(symbol)
            
            if not market_data:
                self.logger.warning(f"No market data returned for {symbol}")
                return None
                
            return market_data
        except Exception as e:
            self.logger.error(f"Error fetching market data for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return None

    async def stop(self) -> None:
        """Stop the market monitor."""
        self.running = False
        
        # Stop the market data manager
        await self.market_data_manager.stop()
        
        # Cancel the scheduled market reports task if it exists
        if hasattr(self, '_market_report_task') and self._market_report_task is not None:
            self.logger.info("Stopping scheduled market reports service...")
            self._market_report_task.cancel()
            try:
                await self._market_report_task
            except asyncio.CancelledError:
                pass  # Expected behavior when cancelling a task
            self.logger.info("Scheduled market reports service stopped")
        
        # Cleanup other resources
        await self._cleanup()

    async def _cleanup(self):
        """Cleanup resources."""
        try:
            logger.info("Cleaning up resources...")
            # Close exchange connections
            if hasattr(self, 'exchange_manager'):
                await self.exchange_manager.close()
            
            # Cancel any pending tasks
            for task in asyncio.all_tasks():
                if task is not asyncio.current_task():
                    task.cancel()
                    
            # Close WebSocket connection
            if hasattr(self, '_ws') and self._ws:
                await self._ws.close()
            
            logger.info("Cleanup completed")
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")

    async def _run_monitoring_loop(self) -> None:
        """Main monitoring loop."""
        self.logger.info("Starting monitoring loop...")
        
        while self.running:
            try:
                # Run monitoring cycle
                await self._monitoring_cycle()
                
                # Update monitoring metrics
                await self._update_metrics(None)  # We'll update metrics without analysis results
                
                # Check system health
                health_status = await self._check_system_health()
                if health_status['status'] != 'healthy':
                    self.logger.warning(f"System health check: {health_status['status']}")
                    
                    # Check if any component is critical
                    critical_components = [
                        comp for comp, status in health_status['components'].items()
                        if status.get('status') == 'critical'
                    ]
                    
                    if critical_components:
                        self.logger.error(f"Critical components: {critical_components}")
                        
                        # Generate alert for critical components
                        await self._generate_alert(
                            f"Critical system health issues detected: {', '.join(critical_components)}"
                        )
                
                # Check and update market data manager statistics
                mdm_stats = self.market_data_manager.get_stats()
                if self.metrics_manager:
                    await self.metrics_manager.update_system_metrics({
                        'market_data_manager': {
                            'rest_calls': mdm_stats.get('rest_calls', 0),
                            'websocket_updates': mdm_stats.get('websocket_updates', 0),
                            'websocket_connected': mdm_stats.get('websocket', {}).get('connected', False)
                        }
                    })
                
                # Sleep until next cycle
                await asyncio.sleep(self.interval)
                
            except asyncio.CancelledError:
                self.logger.info("Monitoring loop cancelled")
                break
                
            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {str(e)}")
                self.logger.debug(traceback.format_exc())
                self._error_count += 1
                
                # Back off on errors
                await asyncio.sleep(5)

    async def validate_market_data(self, market_data: Dict[str, Any]) -> bool:
        """Validate market data using the data validator.
        
        This is the central validation method that replaces multiple redundant implementations.
        It performs comprehensive validation of market data structure, freshness, and content.
        
        Args:
            market_data: Market data to validate
            
        Returns:
            bool: True if market data is valid
        """
        # Start performance tracking
        operation = self.metrics_manager.start_operation("validate_market_data")
        
        try:
            # Log validation operation
            self.logger.debug(f"Validating market data for {market_data.get('symbol', 'unknown')}")
            
            # First check basic structure
            if not isinstance(market_data, dict):
                self.logger.error("Market data must be a dictionary")
                return False
            
            # Check for required base fields
            required_fields = ['symbol', 'timestamp']
            missing_fields = [field for field in required_fields if field not in market_data]
            if missing_fields:
                self.logger.error(f"Missing required fields in market data: {missing_fields}")
                return False
            
            # Use the data validator for comprehensive validation
            # Now data_validator.validate_market_data() is an ASYNCHRONOUS method
            if hasattr(self, 'data_validator') and self.data_validator is not None:
                # Call the validator's method asynchronously
                self.logger.debug("Calling data_validator.validate_market_data asynchronously")
                result = await self.data_validator.validate_market_data(market_data)
                
                # Get validation stats for metrics
                validation_stats = self.data_validator.get_validation_stats()
                
                # Record validation metrics
                self.metrics_manager.record_metric("validation.total", validation_stats['total_validations'])
                self.metrics_manager.record_metric("validation.passed", validation_stats['passed_validations'])
                self.metrics_manager.record_metric("validation.failed", validation_stats['failed_validations'])
            else:
                # If no data validator is available, do basic validation
                self.logger.warning("No data validator available, performing basic validation only")
                result = True  # Assume valid if basic checks pass
            
            # Check timeframes if ohlcv data is present
            if 'ohlcv' in market_data and hasattr(self, 'validate_timeframes'):
                # Make sure we're not awaiting the result if it's not a coroutine
                if callable(self.validate_timeframes):
                    try:
                        # Check if validate_timeframes is a coroutine function
                        if asyncio.iscoroutinefunction(self.validate_timeframes):
                            timeframe_results = await self.validate_timeframes(market_data.get('ohlcv', {}))
                        else:
                            # Call directly if it's a regular function
                            timeframe_results = self.validate_timeframes(market_data.get('ohlcv', {}))
                        if isinstance(timeframe_results, dict) and not any(timeframe_results.values()):
                            self.logger.error("All timeframes validation failed")
                            result = False
                    except Exception as e:
                        self.logger.error(f"Error validating timeframes: {str(e)}")
            
            # End performance tracking
            self.metrics_manager.end_operation(operation)
            return result
            
        except Exception as e:
            self.logger.error(f"Error validating market data: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # End operation with failure
            self.metrics_manager.end_operation(operation, success=False)
            return False


    async def validate_market_data_sync(self, market_data: Dict[str, Any]) -> bool:
        """Synchronous version of validate_market_data that delegates to the data validator.
        
        This method is now also asynchronous since the data validator's method is asynchronous,
        but it's kept for backward compatibility.
        
        Args:
            market_data: Market data to validate
            
        Returns:
            bool: True if market data is valid
        """
        try:
            self.logger.debug("=== SYNC VALIDATION DEBUG START ===")
            self.logger.debug(f"Processing market data with keys: {list(market_data.keys())}")
            
            # Preprocess market data to ensure numeric types are properly handled
            self.logger.debug("Starting data preprocessing")
            processed_market_data = self._preprocess_market_data_for_validation(market_data)
            self.logger.debug("Preprocessing completed")
            
            # Call validation on preprocessed data
            self.logger.debug("Starting validation on preprocessed data")
            validation_result = await self.data_validator.validate_market_data(processed_market_data)
            self.logger.debug(f"Validation result on preprocessed data: {validation_result}")
            
            self.logger.debug("=== SYNC VALIDATION DEBUG END ===")
            return validation_result
            
        except Exception as e:
            self.logger.error(f"Error in synchronous market data validation: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Try to continue with unprocessed data if preprocessing failed
            try:
                self.logger.debug("=== FALLBACK VALIDATION DEBUG START ===")
                self.logger.debug("Starting fallback validation with original data")
                fallback_result = await self.data_validator.validate_market_data(market_data)
                self.logger.debug(f"Fallback validation result: {fallback_result}")
                self.logger.debug("=== FALLBACK VALIDATION DEBUG END ===")
                return fallback_result
            except Exception as inner_e:
                self.logger.error(f"Fallback validation also failed: {str(inner_e)}")
                self.logger.debug(traceback.format_exc())
            return False
            
    def _preprocess_market_data_for_validation(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Preprocess market data to ensure numeric values are handled correctly.
        
        This method converts string numeric values to float in common fields that might
        cause comparison issues during validation.
        
        Args:
            market_data: Raw market data dictionary
            
        Returns:
            Dict[str, Any]: Processed market data with numeric conversions
        """
        # Create a deep copy to avoid modifying the original data
        processed = copy.deepcopy(market_data)
        
        try:
            # Add enhanced debugging - log the input structure but only at start
            self.logger.debug("=== PREPROCESSING MARKET DATA DEBUG ===")
            self.logger.debug(f"Market data keys: {list(market_data.keys())}")
            
            # Process orderbook data
            if 'orderbook' in processed and isinstance(processed['orderbook'], dict):
                self.logger.debug(f"Processing orderbook with keys: {list(processed['orderbook'].keys())}")
                
                # Handle nested orderbook structure (Bybit format with result.b and result.a)
                if 'result' in processed['orderbook'] and isinstance(processed['orderbook']['result'], dict):
                    result_data = processed['orderbook']['result']
                    if 'b' in result_data and 'a' in result_data:
                        # Copy bids and asks to standard format
                        processed['orderbook']['bids'] = result_data['b']
                        processed['orderbook']['asks'] = result_data['a']
                        self.logger.debug("Restructured orderbook from result.b/result.a format")
                
                # Log summary of processing, not individual levels
                for side in ['bids', 'asks']:
                    if side in processed['orderbook'] and isinstance(processed['orderbook'][side], list):
                        levels_count = len(processed['orderbook'][side])
                        converted_count = 0
                        
                        for i, level in enumerate(processed['orderbook'][side]):
                            if isinstance(level, (list, tuple)) and len(level) >= 2:
                                # Convert price and size to float
                                try:
                                    if isinstance(level[0], str):
                                        processed['orderbook'][side][i] = [float(level[0]), float(level[1])]
                                        converted_count += 1
                                except (ValueError, TypeError):
                                    # Leave as is if conversion fails
                                    pass
                        
                        # Log summary instead of each conversion
                        if converted_count > 0:
                            self.logger.debug(f"Converted {converted_count}/{levels_count} orderbook {side} levels to float")
            
            # Process trades data with less verbose logging
            if 'trades' in processed:
                self.logger.debug(f"Processing trades data, type: {type(processed['trades'])}")
                original_trades = processed['trades']
                
                # Detect the structure of trades data and extract/normalize the list
                trades_list = None
                trades_structure = 'unknown'
                
                # If trades are in a dict (like API responses with metadata)
                if isinstance(original_trades, dict):
                    # Check for Bybit specific nested structure (result.list)
                    if 'result' in original_trades and isinstance(original_trades['result'], dict) and 'list' in original_trades['result']:
                        trades_list = original_trades['result']['list']
                        trades_structure = 'result.list'
                    # Check if trades are nested in a 'list' key
                    elif 'list' in original_trades:
                        trades_list = original_trades['list']
                        trades_structure = 'list'
                    # Check for 'result' containing a list directly
                    elif 'result' in original_trades and isinstance(original_trades['result'], list):
                        trades_list = original_trades['result']
                        trades_structure = 'result'
                    # Handle other potential structures
                    elif 'data' in original_trades and isinstance(original_trades['data'], list):
                        trades_list = original_trades['data']
                        trades_structure = 'data'
                # If trades is already a list
                elif isinstance(original_trades, list):
                    trades_list = original_trades
                    trades_structure = 'direct_list'
                
                # If we successfully extracted a trades list, process it
                if trades_list is not None:
                    self.logger.debug(f"Extracted trades list from '{trades_structure}' structure with {len(trades_list)} trades")
                    processed['trades_original_format'] = trades_structure
                    
                    # Process the extracted trades list
                    processed_trades = []
                    total_conversions = 0
                    
                    for i, trade in enumerate(trades_list):
                        if isinstance(trade, dict):
                            # Create a processed copy of the trade
                            processed_trade = trade.copy()
                            
                            # Log only the first trade as a sample
                            if i == 0:
                                self.logger.debug(f"Sample trade keys: {list(trade.keys())}")
                                
                            # Convert price and amount/size to float
                            converted_fields = 0
                            for field in trade:
                                if field not in ['symbol', 'side', 'id', 'execId', 'direction', 'tickDirection'] and isinstance(trade[field], str):
                                    try:
                                        processed_trade[field] = float(trade[field])
                                        converted_fields += 1
                                    except (ValueError, TypeError):
                                        pass
                            
                            total_conversions += converted_fields
                            
                            # Handle field mapping but log only first trade
                            if 'price' not in processed_trade and 'execPrice' in processed_trade:
                                processed_trade['price'] = processed_trade['execPrice']
                                if i == 0:
                                    self.logger.debug(f"Mapped execPrice to price field")
                            
                            if 'size' not in processed_trade and 'execQty' in processed_trade:
                                processed_trade['size'] = processed_trade['execQty']
                                if i == 0:
                                    self.logger.debug(f"Mapped execQty to size field")
                            
                            if 'amount' not in processed_trade and 'size' in processed_trade:
                                processed_trade['amount'] = processed_trade['size']
                                if i == 0:
                                    self.logger.debug(f"Added amount field from size")

                            # Map execId to id (needed for orderflow analysis)
                            if "id" not in processed_trade and "execId" in processed_trade:
                                processed_trade["id"] = processed_trade["execId"]
                                if i == 0:
                                    self.logger.debug(f"Mapped execId to id field")
                            
                            processed_trades.append(processed_trade)
                    
                    # Update processed data with the normalized trades list
                    processed['trades'] = processed_trades
                    self.logger.debug(f"Processed {len(processed_trades)} trades with {total_conversions} field conversions")
                else:
                    # Enhanced fallback handling for unknown trade structures
                    self.logger.warning(f"Could not extract trades list from trades data, attempting additional fallbacks")
                    
                    # If it's a dictionary, try to identify any list that might contain trades
                    if isinstance(original_trades, dict):
                        potential_trade_lists = []
                        
                        # Try to find any list with trade-like data in the dict
                        for key, value in original_trades.items():
                            if isinstance(value, list) and len(value) > 0:
                                potential_trade_lists.append((key, value))
                        
                        # If we found potential trade lists, use the one that looks most like trades
                        if potential_trade_lists:
                            # Sort by length - assume longer lists are more likely to be trades
                            potential_trade_lists.sort(key=lambda x: len(x[1]), reverse=True)
                            
                            # Check if items in the list look like trades (has price or amount)
                            for key, value in potential_trade_lists:
                                if value and isinstance(value[0], dict):
                                    # Check if it has trade-like fields
                                    first_item = value[0]
                                    trade_fields = ['price', 'amount', 'size', 'quantity', 'side', 'time', 'timestamp']
                                    
                                    # If it has at least one trade-like field, use this list
                                    if any(field in first_item for field in trade_fields):
                                        trades_list = value
                                        trades_structure = f"extracted_from_{key}"
                                        self.logger.debug(f"Found trade-like list in key '{key}' with {len(trades_list)} items")
                                        
                                        # Process this list similarly to the normal case
                                        processed_trades = []
                                        for trade in trades_list:
                                            if isinstance(trade, dict):
                                                processed_trade = trade.copy()
                                                for field in trade:
                                                    if field not in ['symbol', 'side', 'id'] and isinstance(trade[field], str):
                                                        try:
                                                            processed_trade[field] = float(trade[field])
                                                        except (ValueError, TypeError):
                                                            pass
                                                processed_trades.append(processed_trade)
                                        
                                        processed['trades'] = processed_trades
                                        self.logger.debug(f"Processed {len(processed_trades)} trades from fallback approach")
                                        break
                        
                        # If we still don't have a list, create an empty one
                        if 'trades' not in processed or not isinstance(processed['trades'], list):
                            self.logger.warning("All fallback approaches failed, creating empty trades list")
                            processed['trades'] = []
                    else:
                        # If it's not a dict or list, create an empty list
                        self.logger.warning(f"Trades data type {type(original_trades)} is not supported, creating empty list")
                        processed['trades'] = []
            
            # Process ticker data more efficiently
            if 'ticker' in processed and isinstance(processed['ticker'], dict):
                converted_count = 0
                ticker_fields = list(processed['ticker'].keys())
                
                for field in ticker_fields:
                    value = processed['ticker'][field]
                    if isinstance(value, str) and field not in ['symbol']:
                        try:
                            processed['ticker'][field] = float(value)
                            converted_count += 1
                        except (ValueError, TypeError):
                            pass
                
                # Log summary instead of individual conversions
                if converted_count > 0:
                    self.logger.debug(f"Converted {converted_count}/{len(ticker_fields)} ticker fields to float")
            
            self.logger.debug("=== END PREPROCESSING DEBUG ===")        
            return processed
            
        except Exception as e:
            self.logger.error(f"Error preprocessing market data: {str(e)}")
            self.logger.error(traceback.format_exc())
            # Return original data if preprocessing fails
            return market_data

    async def _update_metrics(self, analysis_results: Optional[List[Dict[str, Any]]]) -> None:
        """Update monitoring metrics."""
        try:
            if not self.metrics_manager:
                return
                
            # Convert all values to float and ensure they are numeric
            metrics = {
                'total_messages': float(self._stats['total_messages']),
                'invalid_messages': float(self._stats['invalid_messages']),
                'delayed_messages': float(self._stats['delayed_messages']),
                'error_count': float(self._stats['error_count']),
                'last_update_time': float(time.time())
            }
            
            # Add market data manager stats
            mdm_stats = self.market_data_manager.get_stats()
            metrics.update({
                'rest_calls': float(mdm_stats.get('rest_calls', 0)),
                'websocket_updates': float(mdm_stats.get('websocket_updates', 0)),
            })
            
            # Update monitoring metrics as system metrics
            await self.metrics_manager.update_system_metrics(metrics)
            
        except Exception as e:
            error_context = ErrorContext(
                component="market_monitor",
                operation="update_metrics"
            )
            if self.error_handler:
                await self.error_handler.handle_error(
                    error=e,
                    context=error_context,
                    severity=ErrorSeverity.LOW
                )
            else:
                logger.error(f"Error updating metrics: {str(e)}")
                
    async def _check_system_health(self) -> Dict[str, Any]:
        """Check overall system health."""
        try:
            health_status = {
                'status': 'healthy',
                'components': {
                    'exchange': await self._check_exchange_health(),
                    'database': await self._check_database_health(),
                    'memory': await self._check_memory_usage(),
                    'cpu': await self._check_cpu_usage(),
                    'market_data_manager': await self._check_market_data_manager_health()
                }
            }
            
            # Check if any component is unhealthy
            for component, status in health_status['components'].items():
                if status.get('status') != 'healthy':
                    health_status['status'] = 'warning'
                    
                    if status.get('status') == 'critical':
                        health_status['status'] = 'critical'
                        break
                    
            return health_status
            
        except Exception as e:
            self.logger.error(f"Error checking system health: {str(e)}")
            return {'status': 'error', 'message': str(e)}

    async def _check_exchange_health(self) -> Dict[str, Any]:
        """Check exchange connectivity and response times."""
        try:
            if not self.exchange_manager:
                return {'status': 'error', 'message': 'Exchange not initialized'}
                
            # Test API connection
            await self.exchange_manager.ping()
            return {'status': 'healthy'}
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_market_data_manager_health(self) -> Dict[str, Any]:
        """Check health of the market data manager."""
        try:
            stats = self.market_data_manager.get_stats()
            websocket_status = stats.get('websocket', {})
            
            # Check WebSocket connection
            if not websocket_status.get('connected', False):
                return {
                    'status': 'warning',
                    'message': 'WebSocket not connected',
                    'details': websocket_status
                }
            
            # Check time since last WebSocket message
            seconds_since_last_message = websocket_status.get('seconds_since_last_message', 0)
            if seconds_since_last_message > 60:  # No message in last minute
                return {
                    'status': 'warning',
                    'message': f'No WebSocket message received in {seconds_since_last_message:.1f}s',
                    'details': websocket_status
                }
            
            # Check data freshness (oldest symbol data)
            if 'data_freshness' in stats:
                max_age = 0
                oldest_symbol = None
                
                for symbol, freshness in stats['data_freshness'].items():
                    age = freshness.get('age_seconds', 0)
                    if age > max_age:
                        max_age = age
                        oldest_symbol = symbol
                
                if max_age > 300:  # Data older than 5 minutes
                    return {
                        'status': 'warning',
                        'message': f'Data for {oldest_symbol} is {max_age:.1f}s old',
                        'details': {
                            'oldest_symbol': oldest_symbol,
                            'age_seconds': max_age
                        }
                    }
            
            # All checks passed
            return {'status': 'healthy'}
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_database_health(self) -> Dict[str, Any]:
        """Check database connectivity."""
        try:
            # Add your database health check logic here
            return {'status': 'healthy'}
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_memory_usage(self) -> Dict[str, Any]:
        """Check system memory usage."""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                'status': 'healthy' if memory.percent < 90 else 'warning',
                'usage': memory.percent
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_cpu_usage(self) -> Dict[str, Any]:
        """Check CPU usage."""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=1)
            return {
                'status': 'healthy' if cpu_percent < 80 else 'warning',
                'usage': cpu_percent
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_thresholds(self) -> None:
        """Check monitoring thresholds and generate alerts."""
        try:
            # Check invalid message ratio
            if self._stats['total_messages'] > 0:
                invalid_ratio = (
                    self._stats['invalid_messages'] /
                    self._stats['total_messages']
                )
                
                if invalid_ratio > self.config.get('max_invalid_ratio', 0.1):
                    await self._generate_alert(
                        f"High invalid message ratio: {invalid_ratio:.2%}"
                    )
                    
            # Check delayed message ratio
            if self._stats['total_messages'] > 0:
                delayed_ratio = (
                    self._stats['delayed_messages'] /
                    self._stats['total_messages']
                )
                
                if delayed_ratio > self.config.get('max_delayed_ratio', 0.1):
                    await self._generate_alert(
                        f"High delayed message ratio: {delayed_ratio:.2%}"
                    )
                    
            # Check error count
            if self._stats['error_count'] > self.config.get('max_errors', 100):
                await self._generate_alert(
                    f"High error count: {self._stats['error_count']}"
                )
                
        except Exception as e:
            logger.error(f"Error checking thresholds: {str(e)}")
            
    async def _generate_alert(self, message: str) -> None:
        """Generate monitoring alert.
        
        Args:
            message: Alert message
        """
        try:
            if self.alert_manager:
                await self.alert_manager.send_alert(
                    message=message,
                    level="warning",
                    component="monitor"
                )
            else:
                logger.warning(f"Monitor alert: {message}")
                
        except Exception as e:
            logger.error(f"Error generating alert: {str(e)}")
            
    @property
    def stats(self) -> Dict[str, Any]:
        """Get monitoring statistics."""
        # Combine internal stats with market data manager stats
        combined_stats = self._stats.copy()
        
        # Add market data manager stats if available
        if hasattr(self, 'market_data_manager'):
            mdm_stats = self.market_data_manager.get_stats()
            combined_stats['market_data_manager'] = mdm_stats
        
        return combined_stats

    async def _process_analysis_result(self, symbol: str, result: Dict[str, Any]) -> None:
        """Process analysis result and generate signals if appropriate."""
        try:
            # Extract key information
            confluence_score = result.get('score', result.get('confluence_score', 0))
            reliability = result.get('reliability', 0)
            components = result.get('components', {})
            
            # Get thresholds from config.confluence section for consistency
            confluence_config = self.config.get('confluence', {})
            threshold_config = confluence_config.get('thresholds', {})
            buy_threshold = float(threshold_config.get('buy', 60))
            sell_threshold = float(threshold_config.get('sell', 40))
            neutral_buffer = float(threshold_config.get('neutral_buffer', 5))
            
            # Log component scores
            self.logger.debug("\n=== Component Scores ===")
            for component, score in components.items():
                self.logger.debug(f"{component}: {score}")
                
            # Display comprehensive confluence score table with top components and interpretations
            from src.core.formatting import LogFormatter
            formatted_table = LogFormatter.format_enhanced_confluence_score_table(
                symbol=symbol,
                confluence_score=confluence_score,
                components=components,
                results=result.get('results', {}),
                weights=result.get('metadata', {}).get('weights', {}),
                reliability=reliability
            )
            self.logger.info(formatted_table)
            
            # Log detailed market interpretations separately to ensure they appear in the logs
            
            # Generate signal if score meets thresholds
            self.logger.debug(f"=== Generating Signal ===")
            # Store threshold information in result for downstream processing
            result['buy_threshold'] = buy_threshold
            result['sell_threshold'] = sell_threshold
            
            # Format score safely for logging
            score_str = f"{confluence_score:.2f}" if confluence_score is not None else "N/A"
            buy_threshold_str = f"{buy_threshold:.2f}" if buy_threshold is not None else "N/A"
            sell_threshold_str = f"{sell_threshold:.2f}" if sell_threshold is not None else "N/A"
            
            if confluence_score >= buy_threshold:
                await self._generate_signal(symbol, result)
                self.logger.info(f"Generated BUY signal for {symbol} with score {score_str} (threshold: {buy_threshold_str})")
            elif confluence_score <= sell_threshold:
                await self._generate_signal(symbol, result)
                self.logger.info(f"Generated SELL signal for {symbol} with score {score_str} (threshold: {sell_threshold_str})")
            else:
                self.logger.debug(f"No signal generated - score {score_str} in neutral zone (buy: {buy_threshold_str}, sell: {sell_threshold_str})")

            # Update metrics
            if self.metrics_manager:
                await self.metrics_manager.update_analysis_metrics(symbol, result)

        except Exception as e:
            self.logger.error(f"Error processing analysis result: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _generate_signal(self, symbol: str, analysis_result: Dict[str, Any]) -> None:
        """Generate trading signal based on analysis results with enhanced validation and tracking."""
        if not hasattr(self, 'signal_generator') or self.signal_generator is None:
            self.logger.error(f"Signal generator not available for {symbol}")
            return

        try:
            # Generate transaction and signal IDs for tracking
            transaction_id = str(uuid.uuid4())
            signal_id = str(uuid.uuid4())[:8]
            
            # Log the start of signal generation with transaction ID
            self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating signal for {symbol}")
            
            # Extract critical information
            if not analysis_result or not isinstance(analysis_result, dict):
                self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Invalid analysis result for {symbol}")
                return
                
            # Extract data from analysis result
            confluence_score = analysis_result.get('confluence_score', 0)
            components = analysis_result.get('components', {})
            results = analysis_result.get('results', {})
            
            # Get reliability score
            reliability = analysis_result.get('reliability', 0.5)
            
            # Get price information
            price = None
            if 'price' in analysis_result:
                price = analysis_result['price']
            elif 'market_data' in analysis_result and 'ticker' in analysis_result['market_data']:
                ticker = analysis_result['market_data']['ticker']
                price = ticker.get('last', ticker.get('close', None))
            
            if price is None and hasattr(self, 'market_data_manager'):
                try:
                    market_data = await self.market_data_manager.get_market_data(symbol)
                    if market_data and 'ticker' in market_data:
                        price = float(market_data['ticker'].get('last', market_data['ticker'].get('close', 0)))
                except Exception as e:
                    self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] Error getting price from market data: {str(e)}")
            
            # Get thresholds from config
            config = self.config.get('confluence', {}).get('thresholds', {})
            buy_threshold = float(config.get('buy', 70))
            sell_threshold = float(config.get('sell', 30))
            
            # Create enhanced signal data
            signal_data = {
                'symbol': symbol,
                'confluence_score': confluence_score,
                'components': components,
                'results': results,
                'weights': analysis_result.get('metadata', {}).get('weights', {}),
                'reliability': reliability,
                'price': price,
                'transaction_id': transaction_id,
                'signal_id': signal_id,
                'buy_threshold': buy_threshold,
                'sell_threshold': sell_threshold
            }
            
            # Add enhanced analysis data if available
            if 'market_interpretations' in analysis_result:
                signal_data['market_interpretations'] = analysis_result['market_interpretations']
            
            if 'actionable_insights' in analysis_result:
                signal_data['actionable_insights'] = analysis_result['actionable_insights']
                
            if 'influential_components' in analysis_result:
                signal_data['influential_components'] = analysis_result['influential_components']
            
            # Determine signal type based on thresholds
            signal_type = "NEUTRAL"
            if confluence_score >= buy_threshold:
                signal_type = "BUY"
            elif confluence_score <= sell_threshold:
                signal_type = "SELL"
            
            signal_data['signal_type'] = signal_type
            
            # Format values for logging, handling None cases
            score_str = f"{confluence_score:.2f}" if confluence_score is not None else 'N/A'
            reliability_str = f"{reliability:.2f}" if reliability is not None else 'N/A'
            price_str = f"${price:.2f}" if price is not None else 'N/A'
            
            # Log signal data before setting trade parameters
            signal_log = (
                f"[TXN:{transaction_id}][SIG:{signal_id}] {symbol} - "
                f"Score: {score_str} ({signal_type}) - "
                f"Reliability: {reliability_str} - "
                f"Price: {price_str}"
            )
            self.logger.info(signal_log)
            
            # Set trade parameters based on config
            try:
                signal_data['trade_params'] = self._calculate_trade_parameters(
                    symbol=symbol,
                    price=price,
                    signal_type=signal_type,
                    score=confluence_score,
                    reliability=reliability
                )
            except Exception as e:
                self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error calculating trade parameters: {str(e)}")
                self.logger.debug(traceback.format_exc())
                # Set default trade parameters on error
                signal_data['trade_params'] = {
                    'entry_price': price,
                    'stop_loss': None,
                    'take_profit': None,
                    'position_size': None,
                    'risk_reward_ratio': None,
                    'risk_percentage': None,
                    'confidence': min(confluence_score / 100, 1.0) if confluence_score is not None else 0.5,
                    'timeframe': 'auto'
                }
            
            # Add timestamp
            signal_data['timestamp'] = int(time.time() * 1000)
            
            # Generate enhanced formatted data if signal_generator is available
            if hasattr(self, 'signal_generator') and self.signal_generator:
                try:
                    self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating enhanced formatted data for {symbol}")
                    enhanced_data = self.signal_generator._generate_enhanced_formatted_data(
                        symbol,
                        confluence_score,
                        components,
                        results,
                        reliability,
                        buy_threshold,
                        sell_threshold
                    )
                    # Add enhanced data to signal_data
                    if enhanced_data:
                        signal_data.update(enhanced_data)
                        self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}] Successfully added enhanced data: market_interpretations={len(enhanced_data.get('market_interpretations', []))}, actionable_insights={len(enhanced_data.get('actionable_insights', []))}")
                except Exception as e:
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error generating enhanced data: {str(e)}")
                    self.logger.debug(traceback.format_exc())
            
            # Generate alert
            if self.alert_manager:
                await self.alert_manager.send_signal_alert(signal_data)
            else:
                self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] Alert manager not available for {symbol}")
            
            # Generate report if enabled
            if self.signal_generator and hasattr(self.signal_generator, 'report_generator'):
                try:
                    self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating report for {symbol}")
                    
                    # Get cached OHLCV data for the report
                    ohlcv_data = self.get_ohlcv_for_report(symbol)
                    
                    if ohlcv_data is not None:
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Using cached OHLCV data for {symbol} report ({len(ohlcv_data)} records)")
                        
                        # Measure report generation time
                        report_start_time = time.time()
                        
                        # Add debug info about the report generator
                        self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Using report generator: {type(self.signal_generator.report_generator).__name__}")
                        if hasattr(self.signal_generator.report_generator, "pdf_enabled"):
                            pdf_enabled = self.signal_generator.report_generator.pdf_enabled
                            self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF generation is {'enabled' if pdf_enabled else 'disabled'}")
                        
                        # Call generate_report with timing
                        report_result = await self.signal_generator.report_generator.generate_report(
                            signal_data=signal_data,
                            ohlcv_data=ohlcv_data
                        )
                        
                        # Calculate time taken
                        report_generation_time = time.time() - report_start_time
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report generation completed in {report_generation_time:.2f}s")
                        
                        # Check and log report result
                        if report_result:
                            self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report result type: {type(report_result)}")
                            
                            # Check if result is a dict and has a pdf_path
                            if isinstance(report_result, dict) and 'pdf_path' in report_result:
                                pdf_path = report_result['pdf_path']
                                self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF path from result: {pdf_path}")
                                
                                # Verify the PDF file exists and check its size
                                if os.path.exists(pdf_path):
                                    pdf_size = os.path.getsize(pdf_path)
                                    self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF file exists - Size: {pdf_size/1024:.2f} KB")
                                    
                                    # Record success metric
                                    if self.metrics_manager:
                                        self.metrics_manager.record_metric('signal_pdf_generation_success', 1)
                                        self.metrics_manager.record_metric('signal_pdf_size_kb', pdf_size/1024)
                                else:
                                    self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF path was returned but file does not exist: {pdf_path}")
                            else:
                                self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report result keys: {list(report_result.keys()) if isinstance(report_result, dict) else 'Not a dict'}")
                    else:
                        self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] No cached OHLCV data for {symbol} report, using simulated data")
                        
                        # Measure report generation time
                        report_start_time = time.time()
                        
                        # Call report generator with simulated data
                        report_result = await self.signal_generator.report_generator.generate_report(
                            signal_data=signal_data
                        )
                        
                        # Calculate time taken
                        report_generation_time = time.time() - report_start_time
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report generation with simulated data completed in {report_generation_time:.2f}s")
                        
                        # Check report result
                        if report_result and isinstance(report_result, dict) and 'pdf_path' in report_result:
                            pdf_path = report_result['pdf_path']
                            if os.path.exists(pdf_path):
                                pdf_size = os.path.getsize(pdf_path)
                                self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF file with simulated data exists - Size: {pdf_size/1024:.2f} KB")
                except Exception as e:
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error generating report: {str(e)}")
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Error type: {type(e).__name__}")
                    self.logger.error(traceback.format_exc())
                    
                    # Record failure metric
                    if self.metrics_manager:
                        self.metrics_manager.record_metric('signal_pdf_generation_failure', 1)
                        error_type = type(e).__name__
                        self.metrics_manager.record_metric(f'signal_pdf_error_{error_type}', 1)
            
            # Return generated signal
            return signal_data
            
        except Exception as e:
            self.logger.error(f"Error generating signal for {symbol}: {str(e)}")
            self.logger.error(traceback.format_exc())
            return None

    def _calculate_trade_parameters(self, symbol: str, price: float, signal_type: str, score: float, reliability: float) -> Dict[str, Any]:
        """
        Calculate trade parameters based on signal data and configuration.
        
        Args:
            symbol: Trading symbol
            price: Current price
            signal_type: Signal type (BUY, SELL, NEUTRAL)
            score: Confluence score (0-100)
            reliability: Signal reliability (0-1)
            
        Returns:
            Dict with calculated trade parameters
        """
        try:
            # Handle None values gracefully
            if price is None:
                self.logger.warning(f"Price is None for {symbol}, using default trade parameters")
                return {
                    'entry_price': None,
                    'stop_loss': None,
                    'take_profit': None,
                    'position_size': None,
                    'risk_reward_ratio': None,
                    'risk_percentage': None,
                    'confidence': min(score / 100, 1.0) if score is not None else None,
                    'timeframe': 'auto'
                }
                
            if score is None:
                score = 50.0  # Default to neutral score
                self.logger.warning(f"Score is None for {symbol}, using default value of {score}")
                
            if reliability is None:
                reliability = 0.5  # Default to medium reliability
                self.logger.warning(f"Reliability is None for {symbol}, using default value of {reliability}")
            
            # Default trade parameters
            trade_params = {
                'entry_price': price,
                'stop_loss': None,
                'take_profit': None,
                'position_size': None,
                'risk_reward_ratio': None,
                'risk_percentage': None,
                'confidence': min(score / 100, 1.0) if score is not None else 0.5,
                'timeframe': 'auto'
            }
            
            # If neutral signal, return default params
            if signal_type == "NEUTRAL":
                self.logger.debug(f"Neutral signal for {symbol}, using default trade parameters")
                return trade_params
                
            # Get trading config
            trading_config = self.config.get('trading', {})
            risk_config = trading_config.get('risk', {})
            
            # Calculate position size based on risk percentage
            account_balance = trading_config.get('account_balance', 10000)
            default_risk = risk_config.get('default_risk_percentage', 1.0)
            
            # Adjust risk based on reliability
            adjusted_risk = default_risk * reliability
            
            # Min and max risk bounds
            min_risk = risk_config.get('min_risk_percentage', 0.5)
            max_risk = risk_config.get('max_risk_percentage', 2.0)
            
            # Ensure risk is within bounds
            risk_percentage = max(min_risk, min(max_risk, adjusted_risk))
            
            # Risk amount in USD
            risk_amount = account_balance * (risk_percentage / 100)
            
            # Default stop percentages
            stop_percentage = 0.0
            if signal_type == "BUY":
                stop_percentage = risk_config.get('long_stop_percentage', 3.0)
            else:
                stop_percentage = risk_config.get('short_stop_percentage', 3.0)
                
            # Calculate stop loss price
            stop_loss = 0.0
            if signal_type == "BUY":
                stop_loss = price * (1 - stop_percentage / 100)
            else:
                stop_loss = price * (1 + stop_percentage / 100)
                
            # Calculate position size
            position_size = 0.0
            if abs(price - stop_loss) > 0:
                position_size = risk_amount / abs(price - stop_loss)
            
            # Calculate take profit based on risk:reward ratio
            risk_reward_ratio = risk_config.get('risk_reward_ratio', 2.0)
            
            # Higher confidence = higher RR potential
            adjusted_rr = risk_reward_ratio * (1 + (score - 50) / 100)
            
            # Calculate take profit price
            take_profit = 0.0
            if signal_type == "BUY":
                take_profit = price + (adjusted_rr * (price - stop_loss))
            else:
                take_profit = price - (adjusted_rr * (stop_loss - price))
                
            # Update trade params
            trade_params.update({
                'stop_loss': round(stop_loss, 8),
                'take_profit': round(take_profit, 8),
                'position_size': round(position_size, 8),
                'risk_reward_ratio': round(adjusted_rr, 2),
                'risk_percentage': round(risk_percentage, 2),
                'risk_amount': round(risk_amount, 2)
            })
            
            self.logger.debug(f"Calculated trade parameters for {symbol}: {trade_params}")
            return trade_params
            
        except Exception as e:
            self.logger.error(f"Error calculating trade parameters for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Return default parameters on error
            return {
                'entry_price': price,
                'stop_loss': None,
                'take_profit': None,
                'position_size': None,
                'risk_reward_ratio': None,
                'risk_percentage': None,
                'confidence': min(score / 100, 1.0),
                'timeframe': 'auto'
            }

    async def _monitor_volume(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor volume indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['volume_change']:
                await self._handle_volume_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring volume: {str(e)}")

    async def _monitor_orderflow(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor orderflow indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['orderflow_change']:
                await self._handle_orderflow_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring orderflow: {str(e)}")

    async def _monitor_orderbook(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor orderbook indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['orderbook_change']:
                await self._handle_orderbook_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring orderbook: {str(e)}")

    async def _monitor_position(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor position indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['position_change']:
                await self._handle_position_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring position: {str(e)}")

    async def _monitor_sentiment(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor sentiment indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['sentiment_change']:
                await self._handle_sentiment_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring sentiment: {str(e)}")

    # Add corresponding alert handlers
    async def _handle_momentum_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant momentum changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant momentum change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("MOMENTUM", message, level="INFO")

    async def _handle_volume_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant volume changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant volume change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("VOLUME", message, level="INFO")

    async def _handle_orderflow_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant orderflow changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant orderflow change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("ORDERFLOW", message, level="INFO")

    async def _handle_orderbook_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant orderbook changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant orderbook change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("ORDERBOOK", message, level="INFO")

    async def _handle_position_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant position changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant position change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("POSITION", message, level="INFO")

    async def _handle_sentiment_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant sentiment changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant sentiment change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("SENTIMENT", message, level="INFO")

    async def _process_market_data(self, symbol: str, market_data: Dict[str, Any]) -> None:
        """Process market data for monitoring and analysis.
        
        This consolidated method handles all market data processing functionality,
        replacing multiple redundant implementations.
        
        Args:
            symbol: Trading pair symbol
            market_data: Market data dictionary
        """
        try:
            # Start performance tracking
            operation = self.metrics_manager.start_operation("process_market_data")
            
            # Log the operation
            self.logger.debug(f"\n=== Processing market data for {symbol} ===")
            
            # If symbol is not provided in the arguments, try to get it from the market data
            symbol = symbol or market_data.get('symbol', 'unknown')
            
            # Validate market data (now asynchronous)
            self.logger.debug("Calling validate_market_data asynchronously for {}".format(symbol))
            if not await self.validate_market_data(market_data):
                self.logger.error(f"Invalid market data for {symbol}")
                self.metrics_manager.end_operation(operation, success=False)
                return
            
            # Calculate metrics
            ticker = market_data.get('ticker', {})
            price = float(ticker.get('last', 0))
            change24h = float(ticker.get('change24h', 0))
            volume = float(ticker.get('volume', 0))
            funding_rate = float(ticker.get('fundingRate', 0))

            # Monitor significant price changes
            if abs(change24h) > 5.0:  # 5% threshold
                await self.alert_manager.send_alert(
                    "PRICE_CHANGE",
                    f"Significant price change for {symbol}: {change24h:+.2f}%",
                    level="INFO"
                )

            # Monitor funding rate extremes
            if abs(funding_rate) > 0.001:  # 0.1% threshold
                await self.alert_manager.send_alert(
                    "FUNDING_RATE",
                    f"High funding rate for {symbol}: {funding_rate:.4f}",
                    level="INFO"
                )

            # Monitor orderbook imbalances
            orderbook = market_data.get('orderbook', {})
            if orderbook:
                bids = orderbook.get('bids', [])
                asks = orderbook.get('asks', [])
                
                if bids and asks:
                    # Extract the quantities from the first 5 levels and convert to numpy array for efficient calculations
                    bid_quantities = np.array([float(bid[1]) for bid in bids[:5]])
                    ask_quantities = np.array([float(ask[1]) for ask in asks[:5]])
                    
                    bid_sum = np.sum(bid_quantities)
                    ask_sum = np.sum(ask_quantities)
                    
                    if bid_sum > 0 and ask_sum > 0:
                        imbalance = (bid_sum - ask_sum) / (bid_sum + ask_sum)
                        if abs(imbalance) > 0.2:  # 20% imbalance threshold
                            direction = "buy" if imbalance > 0 else "sell"
                            await self.alert_manager.send_alert(
                                "ORDERBOOK_IMBALANCE",
                                f"Large {direction} imbalance for {symbol}: {abs(imbalance):.1%}",
                                level="INFO"
                            )
            
            # Update indicators
            self._update_indicators(market_data)
            
            # Generate signals
            signals = self._generate_signals(market_data)
            
            # Process alerts
            self._process_alerts(signals)
            
            # Record metrics
            self.metrics_manager.record_metric(f"market_data_processed.{symbol}", 1)
            
            # End performance tracking
            self.metrics_manager.end_operation(operation)
            self.logger.debug(f"Successfully processed market data for {symbol}")
            
        except Exception as e:
            self.logger.error(f"Error processing market data for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Record error metric
            self.metrics_manager.record_metric(f"market_data_errors.{symbol}", 1)
            
            # End operation with failure if it was started
            if 'operation' in locals():
                self.metrics_manager.end_operation(operation, success=False)
    
    # This synchronous version is kept for backward compatibility and delegates to the async version
    def _process_market_data_sync(self, market_data: Dict) -> None:
        """Synchronous wrapper for _process_market_data.
        
        Args:
            market_data: Market data dictionary
        """
        try:
            symbol = market_data.get('symbol', 'unknown')
            loop = asyncio.get_event_loop()
            
            if loop.is_running():
                asyncio.create_task(self._process_market_data(symbol, market_data))
            else:
                loop.run_until_complete(self._process_market_data(symbol, market_data))
                
        except Exception as e:
            self.logger.error(f"Error in synchronous market data processing: {str(e)}")

def handle_monitoring_error(func: Callable) -> Callable:
    """Decorator to handle monitoring errors.
    
    Args:
        func: Function to wrap
        
    Returns:
        Wrapped function with error handling
    """
    @functools.wraps(func)
    async def wrapper(*args, **kwargs) -> Any:
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            logger.debug(traceback.format_exc())
            return None
    return wrapper

class TimeRangeValidationRule:
    """Validation rule for checking time ranges."""
    
    def __init__(self, monitor):
        self.monitor = monitor
        self.name = "time_range"
        
    async def validate(self, data: Dict[str, Any]) -> bool:
        """Validate time range for data.
        
        Args:
            data: Data dictionary containing timestamp
            
        Returns:
            bool: True if time range is valid
        """
        try:
            timestamp = data.get('timestamp')
            if not timestamp:
                return False
                
            # Convert timestamp to datetime if needed
            if isinstance(timestamp, (int, float)):
                timestamp = datetime.fromtimestamp(timestamp)
                
            # Get current time
            now = datetime.now()
            
            # Check if timestamp is within acceptable range
            max_age = timedelta(hours=24)  # Max 24 hours old
            min_time = now - max_age
            
            return min_time <= timestamp <= now
            
        except Exception as e:
            self.monitor.logger.error(f"Error validating time range: {str(e)}")
            return False

class MarketDataValidator:
    """Comprehensive validation system for market data."""
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        
        # Define validation rules for different data types
        self.validation_rules = {
            'ohlcv': self._validate_ohlcv,
            'orderbook': self._validate_orderbook,
            'trades': self._validate_trades,
            'ticker': self._validate_ticker,
            'funding': self._validate_funding
        }
        
        # Track validation statistics
        self.validation_stats = {
            'total_validations': 0,
            'passed_validations': 0,
            'failed_validations': 0,
            'warnings': 0,
            'data_types': {}
        }
    
    async def validate_market_data(self, market_data):
        """Comprehensive validation of entire market data.
        
        This method is asynchronous for consistency with the rest of the codebase,
        although the individual validation methods are synchronous.
        
        Args:
            market_data: The market data to validate
            
        Returns:
            bool: True if validation passes, False otherwise
        """
        self.logger.debug("\n=== Starting comprehensive market data validation (async) ===")
        
        # Increment total validations
        self.validation_stats['total_validations'] += 1
        
        # Check if market_data is a dictionary
        if not isinstance(market_data, dict):
            self.logger.error(f"Market data must be a dictionary, got {type(market_data)}")
            self.validation_stats['failed_validations'] += 1
            return False
        
        # Check for required base fields
        required_fields = ['symbol', 'timestamp']
        missing_fields = [field for field in required_fields if field not in market_data]
        if missing_fields:
            self.logger.error(f"Missing required fields in market data: {missing_fields}")
            self.validation_stats['failed_validations'] += 1
            return False
        
        # Check each data type using appropriate validator
        validation_results = {}
        overall_result = True
        
        for data_type, validator in self.validation_rules.items():
            if data_type in market_data:
                # Initialize stats tracking for this data type if needed
                if data_type not in self.validation_stats['data_types']:
                    self.validation_stats['data_types'][data_type] = {
                        'validations': 0,
                        'passes': 0,
                        'failures': 0
                    }
                
                self.validation_stats['data_types'][data_type]['validations'] += 1
                
                # Run validation - all validators are still synchronous
                result = validator(market_data[data_type])
                validation_results[data_type] = result
                
                # Update stats
                if result:
                    self.validation_stats['data_types'][data_type]['passes'] += 1
                else:
                    self.validation_stats['data_types'][data_type]['failures'] += 1
                    overall_result = False
        
        # Update overall stats
        if overall_result:
            self.validation_stats['passed_validations'] += 1
        else:
            self.validation_stats['failed_validations'] += 1
            
        self.logger.debug(f"Validation results: {validation_results}")
        self.logger.debug(f"Overall validation result: {'PASS' if overall_result else 'FAIL'}")
        
        return overall_result
    
    def _validate_ohlcv(self, ohlcv_data):
        """Validate OHLCV data for all timeframes."""
        if not isinstance(ohlcv_data, dict):
            self.logger.error(f"OHLCV data must be a dictionary, got {type(ohlcv_data)}")
            return False
        
        # Required timeframes
        required_timeframes = ['base', 'ltf', 'mtf', 'htf']
        available_timeframes = set(ohlcv_data.keys())
        missing_timeframes = set(required_timeframes) - available_timeframes
        
        if missing_timeframes:
            self.logger.warning(f"Missing timeframes in OHLCV data: {missing_timeframes}")
            # We can still validate with some missing timeframes, but at least one should be present
            if len(available_timeframes) == 0:
                self.logger.error("No timeframes available in OHLCV data")
                return False
        
        # Validate each timeframe
        valid_timeframes = 0
        for timeframe, data in ohlcv_data.items():
            if isinstance(data, pd.DataFrame):
                if data.empty:
                    self.logger.warning(f"Empty DataFrame for timeframe {timeframe}")
                    continue
                
                # Check required columns
                required_columns = ['open', 'high', 'low', 'close', 'volume']
                missing_columns = [col for col in required_columns if col not in data.columns]
                
                if missing_columns:
                    self.logger.warning(f"Timeframe {timeframe} missing columns: {missing_columns}")
                    continue
                
                # Check for NaN values
                nan_counts = data.isna().sum()
                if nan_counts.sum() > 0:
                    self.logger.warning(f"NaN values found in timeframe {timeframe}: {nan_counts}")
                
                # Check that high >= low
                invalid_hl = data[data['high'] < data['low']]
                if not invalid_hl.empty:
                    self.logger.error(f"Found {len(invalid_hl)} candles where high < low in {timeframe}")
                    return False
                
                # Check that high >= open and high >= close
                invalid_ho = data[data['high'] < data['open']]
                invalid_hc = data[data['high'] < data['close']]
                if not invalid_ho.empty or not invalid_hc.empty:
                    self.logger.error(f"Found candles with invalid high values in {timeframe}")
                    return False
                
                # Check that low <= open and low <= close
                invalid_lo = data[data['low'] > data['open']]
                invalid_lc = data[data['low'] > data['close']]
                if not invalid_lo.empty or not invalid_lc.empty:
                    self.logger.error(f"Found candles with invalid low values in {timeframe}")
                    return False
                
                # Check for negative values
                negative_values = data[(data[['open', 'high', 'low', 'close', 'volume']] < 0).any(axis=1)]
                if not negative_values.empty:
                    self.logger.error(f"Found {len(negative_values)} candles with negative values in {timeframe}")
                    return False
                
                # Check for chronological order and duplicates
                if data.index.name == 'timestamp' and isinstance(data.index, pd.DatetimeIndex):
                    if not data.index.is_monotonic_increasing:
                        self.logger.error(f"Timestamps in {timeframe} are not in chronological order")
                        return False
                    
                    if len(data.index) != len(data.index.unique()):
                        self.logger.error(f"Duplicate timestamps found in {timeframe}")
                        return False
                
                # Mark this timeframe as valid
                valid_timeframes += 1
        
        # Return True if at least one timeframe is valid
        if valid_timeframes > 0:
            self.logger.debug(f"OHLCV validation passed with {valid_timeframes} valid timeframes")
            return True
        
        self.logger.error("No valid timeframes found in OHLCV data")
        return False
    
    def _validate_orderbook(self, orderbook_data):
        """Validate orderbook data."""
        # Handle nested orderbook structure
        if isinstance(orderbook_data, dict):
            # Check for the Bybit structure with result containing 'b' and 'a'
            if 'result' in orderbook_data and isinstance(orderbook_data['result'], dict):
                result_data = orderbook_data['result']
                if 'b' in result_data and 'a' in result_data:
                    # Extract bids and asks from the result structure
                    orderbook_data = {
                        'bids': result_data.get('b', []),
                        'asks': result_data.get('a', [])
                    }
        
        if not isinstance(orderbook_data, dict):
            self.logger.error(f"Orderbook data must be a dictionary, got {type(orderbook_data)}")
            return False
        
        # Check for required fields
        required_fields = ['bids', 'asks']
        missing_fields = [field for field in required_fields if field not in orderbook_data]
        if missing_fields:
            self.logger.error(f"Missing required fields in orderbook data: {missing_fields}")
            self.logger.debug(f"Orderbook keys: {list(orderbook_data.keys())}")
            return False
        
        # Check that bids and asks are lists
        if not isinstance(orderbook_data['bids'], list) or not isinstance(orderbook_data['asks'], list):
            self.logger.error("Bids and asks must be lists")
            self.logger.debug(f"Bids type: {type(orderbook_data['bids'])}, Asks type: {type(orderbook_data['asks'])}")
            return False
        
        # Check that bids and asks are not empty
        if len(orderbook_data['bids']) == 0 and len(orderbook_data['asks']) == 0:
            self.logger.warning("Both bids and asks are empty in orderbook")
            return False
        
        # Check format of bids and asks
        if orderbook_data['bids']:
            if not all(isinstance(bid, list) and len(bid) >= 2 for bid in orderbook_data['bids']):
                self.logger.error("Invalid bid format in orderbook")
                return False
        
        if orderbook_data['asks']:
            if not all(isinstance(ask, list) and len(ask) >= 2 for ask in orderbook_data['asks']):
                self.logger.error("Invalid ask format in orderbook")
                return False
        
        # Check that bids are in descending order (highest to lowest)
        if len(orderbook_data['bids']) > 1:
            for i in range(len(orderbook_data['bids']) - 1):
                if orderbook_data['bids'][i][0] < orderbook_data['bids'][i+1][0]:
                    self.logger.error("Bids are not in descending order")
                    return False
        
        # Check that asks are in ascending order (lowest to highest)
        if len(orderbook_data['asks']) > 1:
            for i in range(len(orderbook_data['asks']) - 1):
                if orderbook_data['asks'][i][0] > orderbook_data['asks'][i+1][0]:
                    self.logger.error("Asks are not in ascending order")
                    return False
        
        # Check for crossed book (highest bid > lowest ask)
        if orderbook_data['bids'] and orderbook_data['asks']:
            highest_bid = orderbook_data['bids'][0][0]
            lowest_ask = orderbook_data['asks'][0][0]
            if highest_bid >= lowest_ask:
                self.logger.error(f"Crossed orderbook detected: highest bid {highest_bid} >= lowest ask {lowest_ask}")
                return False
        
        self.logger.debug("Orderbook validation passed")
        return True
    
    def _validate_trades(self, trades_data):
        """Validate trades data."""
        # Handle common case where trades data is nested
        self.logger.debug("=== TRADES VALIDATION DEBUG ===")
        self.logger.debug(f"Trades data type: {type(trades_data)}")
        
        if isinstance(trades_data, dict):
            # Try to extract trades list from common nested structures
            self.logger.debug(f"Trades data keys: {list(trades_data.keys())}")
            
            if 'result' in trades_data and isinstance(trades_data['result'], dict) and 'list' in trades_data['result']:
                self.logger.debug("Found trades in result.list structure")
                trades_data = trades_data['result']['list']
            elif 'list' in trades_data:
                self.logger.debug("Found trades in list structure")
                trades_data = trades_data['list']
            elif 'result' in trades_data and isinstance(trades_data['result'], list):
                # Sometimes result itself might be the list
                self.logger.debug("Found trades in result structure")
                trades_data = trades_data['result']
        
        if not isinstance(trades_data, (list, pd.DataFrame)):
            self.logger.error(f"Trades data must be a list or DataFrame, got {type(trades_data)}")
            self.logger.debug(f"Trades data keys: {list(trades_data.keys()) if isinstance(trades_data, dict) else 'N/A'}")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return False
        
        # If it's a DataFrame, check it's not empty
        if isinstance(trades_data, pd.DataFrame):
            if trades_data.empty:
                self.logger.warning("Trades DataFrame is empty")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return True  # Empty trades data is technically valid
            
            # Check required columns - look for either amount or size field
            required_columns = ['timestamp', 'price']
            size_field_options = ['amount', 'size', 'quantity', 'qty']
            
            # Check if at least one size field option exists
            has_size_field = any(field in trades_data.columns for field in size_field_options)
            
            missing_columns = [col for col in required_columns if col not in trades_data.columns]
            if missing_columns or not has_size_field:
                missing_str = ", ".join(missing_columns)
                size_msg = "no size field (need one of: amount, size, quantity, qty)" if not has_size_field else ""
                self.logger.warning(f"Trades DataFrame missing columns: {missing_str} {size_msg}")
            
            # Check for negative prices or amounts
            if 'price' in trades_data.columns:
                neg_prices = trades_data[trades_data['price'] <= 0]
                if not neg_prices.empty:
                    self.logger.error(f"Found {len(neg_prices)} trades with negative/zero prices")
                    self.logger.debug(f"Sample negative price trade: {neg_prices.iloc[0].to_dict()}")
            
            # Check any available size field
            for size_field in size_field_options:
                if size_field in trades_data.columns:
                    neg_amounts = trades_data[trades_data[size_field] <= 0]
                    if not neg_amounts.empty:
                        self.logger.error(f"Found {len(neg_amounts)} trades with negative/zero {size_field}")
                        self.logger.debug(f"Sample negative {size_field} trade: {neg_amounts.iloc[0].to_dict()}")
                    break
            
            # Check timestamp order if available
            if 'timestamp' in trades_data.columns and trades_data['timestamp'].is_monotonic_decreasing:
                self.logger.warning("Trades are not in chronological order (newest first)")
            
            self.logger.debug(f"Trades validation passed with {len(trades_data)} trades")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return True
        
        # If it's a list, check it's not empty
        if len(trades_data) == 0:
            self.logger.warning("Trades list is empty")
            self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
            return True  # Empty trades data is technically valid
        
        # Log some info about the trades list
        self.logger.debug(f"Trades list contains {len(trades_data)} trades")
        if trades_data:
            self.logger.debug(f"First trade type: {type(trades_data[0])}")
            if isinstance(trades_data[0], dict):
                self.logger.debug(f"First trade keys: {list(trades_data[0].keys())}")
                self.logger.debug(f"First trade values: {trades_data[0]}")
        
        # Check each trade
        for i, trade in enumerate(trades_data[:100]):  # Check at most 100 trades
            if not isinstance(trade, dict):
                self.logger.error(f"Trade at index {i} is not a dictionary, type: {type(trade)}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for price field
            if 'price' not in trade:
                self.logger.error(f"Trade at index {i} missing price field")
                self.logger.debug(f"Trade keys: {list(trade.keys())}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for amount/quantity/size field (different exchanges use different names)
            size_field = None
            for field in ['amount', 'size', 'quantity', 'qty']:
                if field in trade:
                    size_field = field
                    break
                    
            if not size_field:
                self.logger.error(f"Trade at index {i} missing size field (looked for: amount, size, quantity, qty)")
                self.logger.debug(f"Trade keys: {list(trade.keys())}")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for positive price
            price = trade['price']
            # Only log if there's an issue
            try:
                price_value = float(price) if isinstance(price, str) else price
                if price_value <= 0:
                    self.logger.error(f"Trade at index {i} has invalid price: {price} (converted to {price_value})")
                    self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                    return False
            except (ValueError, TypeError) as e:
                self.logger.error(f"Error converting price {price} to float: {str(e)}")
                self.logger.debug(f"Price validation failed for trade {i}: price={price} (type: {type(price)})")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
            
            # Check for positive amount/size
            amount = trade[size_field]
            # Only log if there's an issue
            try:
                amount_value = float(amount) if isinstance(amount, str) else amount
                if amount_value <= 0:
                    self.logger.error(f"Trade at index {i} has invalid {size_field}: {amount} (converted to {amount_value})")
                    self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                    return False
            except (ValueError, TypeError) as e:
                self.logger.error(f"Error converting {size_field} {amount} to float: {str(e)}")
                self.logger.debug(f"Size validation failed for trade {i}: {size_field}={amount} (type: {type(amount)})")
                self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
                return False
        
        self.logger.debug(f"Trades validation passed with {len(trades_data)} trades")
        self.logger.debug("=== END TRADES VALIDATION DEBUG ===")
        return True
    
    def _validate_ticker(self, ticker_data):
        """Validate ticker data."""
        if not isinstance(ticker_data, dict):
            self.logger.error(f"Ticker data must be a dictionary, got {type(ticker_data)}")
            return False
        
        # Add volume field if it's missing but available in raw data
        if 'volume' not in ticker_data and hasattr(self, 'exchange') and self.exchange:
            try:
                # Try to get volume from the exchange's last ticker response
                if hasattr(self.exchange, 'last_ticker_response') and self.exchange.last_ticker_response:
                    raw_ticker = self.exchange.last_ticker_response
                    if isinstance(raw_ticker, dict) and 'volume' in raw_ticker:
                        ticker_data['volume'] = raw_ticker['volume']
                        self.logger.debug(f"Added missing volume field to ticker: {ticker_data['volume']}")
            except Exception as e:
                self.logger.debug(f"Could not add volume field to ticker: {str(e)}")
        
        # Check for critical fields
        critical_fields = ['last']
        missing_critical = [field for field in critical_fields if field not in ticker_data]
        if missing_critical:
            self.logger.error(f"Missing critical fields in ticker data: {missing_critical}")
            return False
        
        # Check for recommended fields
        recommended_fields = ['bid', 'ask', 'high', 'low', 'volume']
        missing_recommended = [field for field in recommended_fields if field not in ticker_data]
        if missing_recommended:
            self.logger.warning(f"Missing recommended fields in ticker data: {missing_recommended}")
        
        # Check that numeric fields have valid values
        numeric_fields = ['last', 'bid', 'ask', 'high', 'low', 'volume']
        for field in numeric_fields:
            if field in ticker_data and ticker_data[field] is not None:
                if not isinstance(ticker_data[field], (int, float)):
                    self.logger.error(f"Ticker field {field} is not numeric: {ticker_data[field]}")
                    return False
                
                if field != 'change24h' and ticker_data[field] <= 0:
                    self.logger.error(f"Ticker field {field} has invalid value: {ticker_data[field]}")
                    return False
        
        # Check for crossed values
        if 'bid' in ticker_data and 'ask' in ticker_data:
            if ticker_data['bid'] is not None and ticker_data['ask'] is not None:
                if ticker_data['bid'] >= ticker_data['ask']:
                    self.logger.error(f"Crossed ticker values: bid ({ticker_data['bid']}) >= ask ({ticker_data['ask']})")
                    return False
        
        # Check high/low consistency
        if 'high' in ticker_data and 'low' in ticker_data:
            if ticker_data['high'] is not None and ticker_data['low'] is not None:
                if ticker_data['high'] < ticker_data['low']:
                    self.logger.error(f"Invalid ticker values: high ({ticker_data['high']}) < low ({ticker_data['low']})")
                    return False
        
        self.logger.debug("Ticker validation passed")
        return True
    
    def _validate_funding(self, funding_data):
        """Validate funding rate data."""
        if not isinstance(funding_data, dict):
            self.logger.error(f"Funding data must be a dictionary, got {type(funding_data)}")
            return False
        
        # Check for required fields
        required_fields = ['fundingRate']
        missing_fields = [field for field in required_fields if field not in funding_data]
        if missing_fields:
            self.logger.warning(f"Missing fields in funding data: {missing_fields}")
            # Not critical, could still be valid
        
        # Check that funding rate is within reasonable bounds (typically -1% to 1% for most exchanges)
        if 'fundingRate' in funding_data:
            funding_rate = funding_data['fundingRate']
            if not isinstance(funding_rate, (int, float)):
                self.logger.error(f"Funding rate is not numeric: {funding_rate}")
                return False
            
            # Check for extreme values
            if abs(funding_rate) > 0.05:  # 5% threshold
                self.logger.warning(f"Extreme funding rate detected: {funding_rate}")
        
        # Check nextFundingTime is in the future
        if 'nextFundingTime' in funding_data:
            next_time = funding_data['nextFundingTime']
            if isinstance(next_time, (int, float)):
                current_time = int(time.time() * 1000) if next_time > 1000000000000 else int(time.time())
                if next_time < current_time:
                    self.logger.warning("Next funding time is in the past")
        
        self.logger.debug("Funding data validation passed")
        return True
    
    def get_validation_stats(self):
        """Get validation statistics."""
        return self.validation_stats

class LoggingUtility:
    """Centralized logging utility to standardize and consolidate logging operations."""
    
    def __init__(self, logger):
        """Initialize the logging utility.
        
        Args:
            logger: Logger instance to use for logging
        """
        self.logger = logger
    
    def log_raw_response(self, data_type: str, symbol: str, data: Any) -> None:
        """Log raw API responses with detailed structure analysis.
        
        Args:
            data_type: Type of data (OHLCV, Orderbook, Trades)
            symbol: Symbol being processed
            data: Raw response data
        """
        try:
            self.logger.debug(f"\n=== Raw {data_type} Response for {symbol} ===")
            
            # Handle different data types appropriately
            if data_type == 'OHLCV':
                self._log_ohlcv_data(data)
            elif data_type == 'Orderbook':
                self._log_orderbook_data(data)
            elif data_type == 'Trades':
                self._log_trades_data(data)
            else:
                # Generic logging for other data types
                if isinstance(data, dict):
                    self.logger.debug(f"Dictionary with {len(data)} keys: {list(data.keys())}")
                    for key in list(data.keys())[:5]:  # Show first 5 keys
                        value = data[key]
                        self.logger.debug(f"  {key}: {type(value)} {self._format_sample(value)}")
                elif isinstance(data, list):
                    self.logger.debug(f"List with {len(data)} items")
                    for i, item in enumerate(data[:3]):  # Show first 3 items
                        self.logger.debug(f"  [{i}]: {type(item)} {self._format_sample(item)}")
                elif isinstance(data, pd.DataFrame):
                    self.logger.debug(f"DataFrame with shape {data.shape}")
                    self.logger.debug(f"Columns: {list(data.columns)}")
                    if not data.empty:
                        self.logger.debug(f"Sample:\n{data.head(2)}")
                else:
                    self.logger.debug(f"Type: {type(data)}")
                    self.logger.debug(f"Value: {self._format_sample(data)}")
            
            self.logger.debug(f"=== End Raw {data_type} Response ===\n")
        except Exception as e:
            self.logger.warning(f"Error logging raw {data_type} response: {str(e)}")
    
    def _log_ohlcv_data(self, data: Any) -> None:
        """Log OHLCV data with detailed analysis."""
        if isinstance(data, dict):
            # Handle timeframe dictionary
            self.logger.debug(f"OHLCV contains {len(data)} timeframes: {list(data.keys())}")
            
            for tf, tf_data in data.items():
                self.logger.debug(f"\nTimeframe: {tf}")
                
                if isinstance(tf_data, pd.DataFrame):
                    self._log_ohlcv_dataframe(tf_data)
                elif isinstance(tf_data, dict) and 'data' in tf_data and isinstance(tf_data['data'], pd.DataFrame):
                    self._log_ohlcv_dataframe(tf_data['data'])
                else:
                    self.logger.debug(f"  Type: {type(tf_data)}")
        
        elif isinstance(data, pd.DataFrame):
            # Handle direct DataFrame
            self._log_ohlcv_dataframe(data)
        
        elif isinstance(data, list):
            # Handle list format (common in raw API responses)
            self.logger.debug(f"OHLCV data is a list with {len(data)} items")
            
            if data:
                # Check first item to determine format
                first_item = data[0]
                if isinstance(first_item, list):
                    self.logger.debug(f"List format with {len(first_item)} columns")
                    self.logger.debug(f"First candle: {first_item}")
                    
                    # Try to convert to DataFrame for better analysis
                    try:
                        columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
                        if len(first_item) == len(columns):
                            df = pd.DataFrame(data, columns=columns)
                            self._log_ohlcv_dataframe(df)
                    except Exception as e:
                        self.logger.debug(f"Could not convert to DataFrame: {str(e)}")
    
    def _log_ohlcv_dataframe(self, df: pd.DataFrame) -> None:
        """Log details of OHLCV DataFrame."""
        self.logger.debug(f"  Shape: {df.shape}")
        self.logger.debug(f"  Columns: {list(df.columns)}")
        self.logger.debug(f"  Index type: {type(df.index)}")
        
        if not df.empty:
            # Show basic statistics
            self.logger.debug("\n  Basic statistics:")
            stats = df.describe().transpose()
            self.logger.debug(f"{stats}")
            
            # Check for NaN values
            nan_count = df.isna().sum().sum()
            if nan_count > 0:
                self.logger.debug(f"\n  Contains {nan_count} NaN values")
                self.logger.debug(f"  NaN counts by column:\n{df.isna().sum()}")
            
            # Log time range
            if hasattr(df, 'index') and isinstance(df.index, pd.DatetimeIndex):
                self.logger.debug(f"\n  Time range: {df.index.min()} to {df.index.max()}")
                self.logger.debug(f"  Duration: {df.index.max() - df.index.min()}")
            
            # Show first and last candles
            self.logger.debug("\n  First candle:")
            self.logger.debug(f"{df.iloc[0]}")
            self.logger.debug("\n  Last candle:")
            self.logger.debug(f"{df.iloc[-1]}")
    
    def _log_orderbook_data(self, data: Any) -> None:
        """Log orderbook data with detailed analysis."""
        if not isinstance(data, dict):
            self.logger.debug(f"Orderbook is not a dictionary: {type(data)}")
            return
        
        # Check for required fields
        required_fields = ['bids', 'asks']
        missing_fields = [field for field in required_fields if field not in data]
        
        if missing_fields:
            self.logger.debug(f"Orderbook missing required fields: {missing_fields}")
        
        # Log basic statistics
        if 'bids' in data and isinstance(data['bids'], list):
            bids = data['bids']
            self.logger.debug(f"Bids: {len(bids)} levels")
            if bids:
                self.logger.debug(f"  Top 3 bids:")
                for i, bid in enumerate(bids[:3]):
                    self.logger.debug(f"    {i+1}. Price: {bid[0]}, Qty: {bid[1]}")
                
                # Calculate bid sum
                bid_qty_sum = sum(bid[1] for bid in bids)
                self.logger.debug(f"  Total bid quantity: {bid_qty_sum}")
        
        if 'asks' in data and isinstance(data['asks'], list):
            asks = data['asks']
            self.logger.debug(f"Asks: {len(asks)} levels")
            if asks:
                self.logger.debug(f"  Top 3 asks:")
                for i, ask in enumerate(asks[:3]):
                    self.logger.debug(f"    {i+1}. Price: {ask[0]}, Qty: {ask[1]}")
                
                # Calculate ask sum
                ask_qty_sum = sum(ask[1] for ask in asks)
                self.logger.debug(f"  Total ask quantity: {ask_qty_sum}")
        
        # Calculate bid-ask spread if possible
        if 'bids' in data and 'asks' in data and data['bids'] and data['asks']:
            best_bid = data['bids'][0][0]
            best_ask = data['asks'][0][0]
            spread = best_ask - best_bid
            spread_pct = (spread / best_bid) * 100
            
            self.logger.debug(f"\nBid-ask spread: {spread} ({spread_pct:.4f}%)")
            self.logger.debug(f"Best bid: {best_bid}, Best ask: {best_ask}")
    
    def _log_trades_data(self, data: Any) -> None:
        """Log trades data with detailed analysis."""
        if isinstance(data, list):
            self.logger.debug(f"Trades: {len(data)} trades")
            
            if not data:
                return
            
            # Show sample trades
            self.logger.debug("\nSample trades:")
            for i, trade in enumerate(data[:3]):
                self.logger.debug(f"  Trade {i+1}: {trade}")
            
            # Try to calculate some statistics
            try:
                # Extract prices and volumes more efficiently
                prices = []
                volumes = []
                for trade in data:
                    if 'price' in trade:
                        prices.append(float(trade.get('price', 0)))
                    if 'amount' in trade or 'quantity' in trade:
                        volumes.append(float(trade.get('amount', trade.get('quantity', 0))))
                
                if prices:
                    # Use numpy for efficient statistics calculation
                    prices_array = np.array(prices)
                    avg_price = np.mean(prices_array)
                    min_price = np.min(prices_array)
                    max_price = np.max(prices_array)
                    
                    self.logger.debug(f"\nPrice statistics:")
                    self.logger.debug(f"  Min: {min_price}, Max: {max_price}, Avg: {avg_price:.4f}")
                
                if volumes:
                    # Use numpy for efficient volume statistics
                    volumes_array = np.array(volumes)
                    total_volume = np.sum(volumes_array)
                    avg_volume = np.mean(volumes_array)
                    
                    self.logger.debug(f"\nVolume statistics:")
                    self.logger.debug(f"  Total: {total_volume}, Avg: {avg_volume:.4f}")
                
                # Count buy vs sell trades if side is available
                if data and len(data) > 0 and 'side' in data[0]:
                    # Count trades by side in one pass through the data
                    buy_count = 0
                    sell_count = 0
                    for trade in data:
                        side = trade.get('side')
                        if side == 'buy':
                            buy_count += 1
                        elif side == 'sell':
                            sell_count += 1
                
                    self.logger.debug(f"\nTrade directions:")
                    self.logger.debug(f"  Buy: {buy_count} ({buy_count/len(data)*100:.1f}%)")
                    self.logger.debug(f"  Sell: {sell_count} ({sell_count/len(data)*100:.1f}%)")
            
            except Exception as e:
                self.logger.debug(f"Error calculating trade statistics: {str(e)}")
        
        elif isinstance(data, pd.DataFrame):
            self.logger.debug(f"Trades DataFrame with shape {data.shape}")
            
            if data.empty:
                return
            
            self.logger.debug(f"Columns: {list(data.columns)}")
            self.logger.debug(f"\nSample trades:\n{data.head(3)}")
            
            # Show basic statistics
            try:
                if 'price' in data.columns:
                    self.logger.debug(f"\nPrice statistics:")
                    self.logger.debug(f"  Min: {data['price'].min()}, Max: {data['price'].max()}, "
                                     f"Avg: {data['price'].mean():.4f}")
                
                if 'amount' in data.columns:
                    self.logger.debug(f"\nVolume statistics:")
                    self.logger.debug(f"  Total: {data['amount'].sum()}, Avg: {data['amount'].mean():.4f}")
                
                # Count buy vs sell trades if side is available
                if 'side' in data.columns:
                    counts = data['side'].value_counts()
                    self.logger.debug(f"\nTrade directions:\n{counts}")
            
            except Exception as e:
                self.logger.debug(f"Error calculating trade statistics: {str(e)}")
    
    def _format_sample(self, value: Any) -> str:
        """Format a sample of a value for logging."""
        if isinstance(value, (str, int, float, bool)):
            return str(value)
        elif isinstance(value, dict):
            return f"{{...}} with {len(value)} keys"
        elif isinstance(value, list):
            return f"[...] with {len(value)} items"
        elif isinstance(value, pd.DataFrame):
            return f"DataFrame with shape {value.shape}"
        else:
            return str(type(value))

    def log_operation(self, operation_name: str, details: Optional[Dict[str, Any]] = None) -> None:
        """Log the start of an operation with standardized formatting.
        
        Args:
            operation_name: Name of the operation being performed
            details: Optional dictionary of details to log
        """
        self.logger.debug(f"\n=== {operation_name} ===")
        if details:
            for key, value in details.items():
                self.logger.debug(f"{key}: {value}")

    def log_operation_result(self, operation_name: str, success: bool, details: Optional[Dict[str, Any]] = None) -> None:
        """Log the result of an operation with standardized formatting.
        
        Args:
            operation_name: Name of the operation that was performed
            success: Whether the operation was successful
            details: Optional dictionary of details to log
        """
        status = "SUCCESS" if success else "FAILED"
        self.logger.debug(f"=== {operation_name}: {status} ===")
        if details:
            for key, value in details.items():
                self.logger.debug(f"{key}: {value}")
        self.logger.debug("\n")

class TimestampUtility:
    """Utility class for standardized timestamp handling."""
    
    @staticmethod
    def get_utc_timestamp(as_ms: bool = True) -> int:
        """Get current UTC timestamp.
        
        Args:
            as_ms: If True, return millisecond timestamp, else seconds
        
        Returns:
            Current UTC timestamp in milliseconds or seconds
        """
        ts = datetime.now(timezone.utc).timestamp()
        return int(ts * 1000) if as_ms else int(ts)
    
    @staticmethod
    def format_utc_time(timestamp_ms: int) -> str:
        """Format millisecond timestamp to human-readable UTC time.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            Formatted UTC time string
        """
        dt_object = datetime.fromtimestamp(timestamp_ms / 1000, tz=timezone.utc)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3] + ' UTC'
    
    @staticmethod
    def milliseconds_to_datetime(timestamp_ms: int) -> datetime:
        """Convert millisecond timestamp to datetime object.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            datetime object in UTC timezone
        """
        return datetime.fromtimestamp(timestamp_ms / 1000, tz=timezone.utc)
    
    @staticmethod
    def datetime_to_milliseconds(dt_object: datetime) -> int:
        """Convert datetime object to millisecond timestamp.
        
        Args:
            dt_object: datetime object
            
        Returns:
            Timestamp in milliseconds
        """
        return int(dt_object.timestamp() * 1000)
    
    @staticmethod
    def get_age_seconds(timestamp_ms: int) -> float:
        """Calculate age in seconds from a millisecond timestamp.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            
        Returns:
            Age in seconds
        """
        current_ms = TimestampUtility.get_utc_timestamp(as_ms=True)
        return (current_ms - timestamp_ms) / 1000
    
    @staticmethod
    def is_timestamp_fresh(timestamp_ms: int, max_age_seconds: float) -> bool:
        """Check if a timestamp is fresh within a maximum age.
        
        Args:
            timestamp_ms: Timestamp in milliseconds
            max_age_seconds: Maximum age in seconds
            
        Returns:
            True if timestamp is fresh, False otherwise
        """
        return TimestampUtility.get_age_seconds(timestamp_ms) <= max_age_seconds

class MarketMonitor:
    """Class for monitoring market data from exchanges."""
    
    def __init__(
        self,
        exchange=None,
        symbol: Optional[str] = None,
        exchange_manager=None,
        database_client=None,
        portfolio_analyzer=None,
        confluence_analyzer=None,
        timeframes: Optional[Dict[str, str]] = None,
        logger: Optional[logging.Logger] = None,
        metrics_manager: Optional[MetricsManager] = None,
        health_monitor: Optional[HealthMonitor] = None,
        validation_config: Optional[Dict[str, Any]] = None,
        config: Optional[Dict[str, Any]] = None,
        alert_manager=None,
        signal_generator=None,
        top_symbols_manager=None,
        market_data_manager=None,
        **kwargs
    ):
        """Initialize MarketMonitor.
        
        Args:
            exchange: ccxt exchange instance (can be None if exchange_manager is provided)
            symbol: Trading pair symbol (can be None if using TopSymbolsManager)
            exchange_manager: Exchange manager instance
            database_client: Database client for storing analysis results
            portfolio_analyzer: Portfolio analyzer for position management
            confluence_analyzer: Confluence analyzer for market analysis
            timeframes: Dictionary of timeframes to fetch (keys: 'ltf', 'mtf', 'htf', values: timeframe strings)
            logger: Optional logger instance
            metrics_manager: Optional metrics manager instance
            health_monitor: Optional health monitor instance
            validation_config: Optional validation configuration
            config: Optional configuration dictionary
            alert_manager: Optional alert manager instance
            signal_generator: Optional signal generator instance
            top_symbols_manager: Optional top symbols manager instance
            market_data_manager: Optional market data manager instance
        """
        # Store core components
        self.exchange_manager = exchange_manager
        self.database_client = database_client
        self.portfolio_analyzer = portfolio_analyzer
        self.confluence_analyzer = confluence_analyzer
        self.alert_manager = alert_manager
        self.signal_generator = signal_generator
        self.top_symbols_manager = top_symbols_manager
        self.market_data_manager = market_data_manager
        
        # Store exchange info
        self.exchange = exchange
        self.exchange_id = getattr(exchange, 'id', None) if exchange else 'unknown'
        
        # Store full configuration
        self.config = config or {}
        
        # Setup symbol (can be None if using TopSymbolsManager)
        self.symbol = symbol
        self.symbol_str = symbol.replace('/', '') if symbol else None  # Remove / for filename compatibility
        
        # Set up logger
        self.logger = logger or logging.getLogger(__name__)
        
        # Set up logging utility
        self.logging_utility = LoggingUtility(self.logger)
        
        # Set up metrics manager
        self.metrics_manager = metrics_manager or MetricsManager()
        
        # Set up health monitor
        self.health_monitor = health_monitor
        if self.health_monitor and self.exchange_id:
            # Register this exchange with the health monitor
            self.health_monitor.register_api(self.exchange_id)
        
        # Initialize timeframes
        default_timeframes = {'ltf': '1m', 'mtf': '15m', 'htf': '1h'}
        self.timeframes = default_timeframes.copy()
        if timeframes:
            self.timeframes.update(timeframes)
        
        # Add base timeframe (use the lowest timeframe)
        timeframe_values = sorted(self.timeframes.values(), key=lambda x: ccxt_time_to_minutes(x))
        self.timeframes['base'] = timeframe_values[0] if timeframe_values else '1m'
        
        # Configure validation
        default_validation = {
            'max_ohlcv_age_seconds': 300,  # Max age of newest candle in seconds
            'min_ohlcv_candles': 20,       # Minimum number of candles required
            'max_orderbook_age_seconds': 60, # Max age of orderbook in seconds
            'min_orderbook_levels': 5,      # Minimum orderbook levels required
            'max_trades_age_seconds': 300,  # Max age of newest trade in seconds
            'min_trades_count': 5,          # Minimum number of trades required
        }
        self.validation_config = default_validation.copy()
        if validation_config:
            self.validation_config.update(validation_config)
        
        # Rate limiting configuration
        self.rate_limit_config = kwargs.get('rate_limit_config', {
            'enabled': True,
            'max_requests_per_second': 5,  # Default: 5 requests per second
            'timeout_seconds': 10          # Default: 10 second timeout
        })
        
        # Retry configuration
        self.retry_config = kwargs.get('retry_config', {
            'max_retries': 3,              # Default: 3 retries
            'retry_delay_seconds': 2,      # Default: 2 second delay between retries
            'retry_exponential_backoff': True  # Exponential backoff
        })
        
        # Debug configuration
        self.debug_config = kwargs.get('debug_config', {
            'log_raw_responses': False,    # Log raw API responses
            'verbose_validation': False,   # Verbose validation logs
            'save_visualizations': False,  # Save visualizations to disk
            'visualization_dir': 'visualizations'  # Directory for saved visualizations
        })
        
        # Create visualization directory if needed
        if self.debug_config.get('save_visualizations', False):
            Path(self.debug_config.get('visualization_dir', 'visualizations')).mkdir(parents=True, exist_ok=True)
        
        # WebSocket configuration and initialization
        self.websocket_config = kwargs.get('websocket_config', {
            'enabled': True,               # Enable/disable WebSocket
            'use_ws_for_orderbook': True,  # Use WebSocket for orderbook updates
            'use_ws_for_trades': True,     # Use WebSocket for trades updates
            'use_ws_for_tickers': True     # Use WebSocket for ticker updates
        })
        
        # Initialize WebSocket Manager if enabled
        self.ws_manager = None
        self.ws_data = {
            'orderbook': None,             # Latest orderbook from WebSocket
            'trades': [],                  # Recent trades from WebSocket
            'ticker': None,                # Latest ticker from WebSocket
            'kline': {},                   # OHLCV data from WebSocket
            'last_update_time': {          # Last update timestamp for each data type
                'orderbook': 0,
                'trades': 0,
                'ticker': 0,
                'kline': 0
            }
        }
        
        # Initialize monitoring state variables
        self.running = False
        self._last_update_time = 0
        self._error_count = 0
        self.last_report_time = None
        # Ensure report_times is properly initialized at startup
        self.report_times = self._calculate_report_times() if hasattr(self, '_calculate_report_times') else []
        self._stats = {
            'total_messages': 0,
            'invalid_messages': 0,
            'delayed_messages': 0,
            'error_count': 0
        }
        self.interval = self.config.get('monitoring', {}).get('interval', 60)
        self.data_validator = MarketDataValidator(self.logger) if 'MarketDataValidator' in globals() else None
        
        # Initialize active state for monitoring
        self._active = True
        
        # Initialize large order monitoring
        self.large_order_config = self.config.get('monitoring', {}).get('large_orders', {
            'enabled': True,
            'threshold_usd': 1000000,  # $1M default threshold
            'cooldown': 300,           # 5 minutes between alerts for same symbol
            'min_price': 0,            # No minimum price filter
            'excluded_symbols': [],    # No excluded symbols 
            'included_symbols': []     # Monitor all symbols
        })
        self._last_check = {}
        
        # Initialize WebSocket if enabled and we have a symbol
        if self.websocket_config.get('enabled', True) and self.symbol_str:
            self._initialize_websocket()
        
        # Initialize market data cache for fetch_market_data method
        self._market_data_cache = {}
        self._cache_ttl = 300  # 5 minutes default cache TTL
        self._last_ohlcv_update = {}
        self._ohlcv_cache = {}  # Initialize OHLCV cache
        
        self.logger.info(f"Initialized MarketMonitor for {self.exchange_id}")
        
        # Add TimestampUtility instance
        self.timestamp_utility = TimestampUtility()
        
        # Initialize MarketReporter
        self.market_reporter = MarketReporter(
            exchange=self.exchange,
            logger=self.logger,
            top_symbols_manager=top_symbols_manager,
            alert_manager=alert_manager
        )

    def _initialize_websocket(self) -> None:
        """Initialize WebSocket connection for real-time data."""
        try:
            # Skip if no symbol is provided
            if not self.symbol_str:
                self.logger.info("Skipping WebSocket initialization: No symbol provided")
                return
                
            # Initialize WebSocket Manager with the same config
            self.ws_manager = WebSocketManager(self.config)
            
            # Register callback for WebSocket messages
            self.ws_manager.register_message_callback(self._process_websocket_message)
            
            # Create the list of symbols for the WebSocket manager to track
            symbols = [self.symbol_str]
            
            # Initialize asynchronously using create_task
            # Note: This will be executed when the event loop is running
            asyncio.create_task(self.ws_manager.initialize(symbols))
            
            self.logger.info(f"WebSocket integration initialized for {self.symbol}")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize WebSocket: {str(e)}")
            self.logger.debug(traceback.format_exc())
            self.ws_manager = None
            
            # Update health status if available
            if self.health_monitor:
                self.health_monitor._create_alert(
                    level="warning",
                    source=f"websocket:{self.exchange_id}",
                    message=f"Failed to initialize WebSocket: {str(e)}"
                )

    async def _process_websocket_message(self, symbol: str, topic: str, message: Dict[str, Any]) -> None:
        """Process WebSocket message and update internal data.
        
        Args:
            symbol: Trading pair symbol
            topic: Message topic
            message: WebSocket message data
        """
        try:
            # Start performance tracking
            operation = self.metrics_manager.start_operation(f"process_ws_message_{topic}")
            
            # Check if the message is for the symbol we're monitoring
            if symbol != self.symbol_str:
                self.metrics_manager.end_operation(operation)
                return
                
            # Process based on topic type
            if "tickers" in topic:
                await self._process_ticker_update(message)
            elif "kline" in topic:
                await self._process_kline_update(message)
            elif "orderbook" in topic:
                await self._process_orderbook_update(message)
            elif "publicTrade" in topic:
                await self._process_trade_update(message)
            elif "liquidation" in topic:
                await self._process_liquidation_update(message)
            else:
                self.logger.debug(f"Received unhandled topic: {topic}")
                
            # Record metrics
            self.metrics_manager.record_metric("websocket_messages_processed", 1)
            self.metrics_manager.record_metric(f"websocket_messages_{topic}", 1)
            
            # End operation
            self.metrics_manager.end_operation(operation)
            
        except Exception as e:
            self.logger.error(f"Error processing WebSocket message for {topic}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # End operation if it was started
            if 'operation' in locals():
                self.metrics_manager.end_operation(operation, success=False)

    # Add placeholder methods for processing different message types
    async def _process_ticker_update(self, message: Dict[str, Any]) -> None:
        """Process ticker update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract ticker data
            ticker_data = data.get('data', {})
            if not ticker_data:
                return
                
            # Format ticker data
            ticker = {
                'symbol': self.symbol,
                'last': float(ticker_data.get('lastPrice', 0)),
                'bid': float(ticker_data.get('bid1Price', 0)),
                'ask': float(ticker_data.get('ask1Price', 0)),
                'high': float(ticker_data.get('highPrice24h', 0)),
                'low': float(ticker_data.get('lowPrice24h', 0)),
                'volume': float(ticker_data.get('volume24h', 0)),
                'timestamp': int(ticker_data.get('time', timestamp))
            }
            
            # Add additional data if available
            if 'openInterest' in ticker_data:
                ticker['openInterest'] = float(ticker_data['openInterest'])
            if 'fundingRate' in ticker_data:
                ticker['fundingRate'] = float(ticker_data['fundingRate'])
            if 'nextFundingTime' in ticker_data:
                ticker['nextFundingTime'] = int(ticker_data['nextFundingTime'])
            
            # Update internal state
            self.ws_data['ticker'] = ticker
            self.ws_data['last_update_time']['ticker'] = timestamp
            
            # Log update
            self.logger.debug(f"Updated ticker data from WebSocket: Last price: {ticker['last']}")
            
        except Exception as e:
            self.logger.error(f"Error processing ticker update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_kline_update(self, message: Dict[str, Any]) -> None:
        """Process OHLCV update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract kline data
            kline_data = data.get('data', [])
            if not kline_data:
                return
                
            # Get interval from topic
            topic = message.get('topic', '')
            interval = '1'  # Default to 1 minute
            if '.' in topic:
                parts = topic.split('.')
                if len(parts) > 1:
                    interval = parts[1]  # Extract interval from topic
            
            # Map to standard timeframe key
            timeframe_map = {
                '1': 'base',
                '5': 'ltf',
                '30': 'mtf',
                '60': 'mtf',
                '240': 'htf',
                '1D': 'htf'
            }
            
            tf_key = timeframe_map.get(interval, 'base')
            
            # Format candle data
            candles = []
            for candle in kline_data:
                formatted_candle = {
                    'timestamp': int(candle.get('timestamp', 0) or candle.get('start', 0)),
                    'open': float(candle.get('open', 0)),
                    'high': float(candle.get('high', 0)),
                    'low': float(candle.get('low', 0)),
                    'close': float(candle.get('close', 0)),
                    'volume': float(candle.get('volume', 0))
                }
                candles.append(formatted_candle)
            
            # Create DataFrame
            if candles:
                df = pd.DataFrame(candles)
                if 'timestamp' in df.columns:
                    df.set_index('timestamp', inplace=True)
                    df.index = pd.to_datetime(df.index, unit='ms')
                
                # Update internal state
                if tf_key not in self.ws_data['kline']:
                    self.ws_data['kline'][tf_key] = df
                else:
                    # Merge with existing data
                    existing_df = self.ws_data['kline'][tf_key]
                    merged_df = pd.concat([existing_df, df])
                    
                    # Remove duplicates
                    merged_df = merged_df[~merged_df.index.duplicated(keep='last')]
                    
                    # Sort by index
                    merged_df.sort_index(inplace=True)
                    
                    # Keep only the latest candles (max 1000)
                    if len(merged_df) > 1000:
                        merged_df = merged_df.iloc[-1000:]
                        
                    self.ws_data['kline'][tf_key] = merged_df
                
                self.ws_data['last_update_time']['kline'] = timestamp
                
                # Log update
                self.logger.debug(f"Updated {tf_key} OHLCV data from WebSocket: {len(candles)} candles")
                
        except Exception as e:
            self.logger.error(f"Error processing kline update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_orderbook_update(self, message: Dict[str, Any]) -> None:
        """Process orderbook update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract orderbook data
            orderbook_data = data.get('data', {})
            if not orderbook_data:
                return
            
            # Format orderbook data
            orderbook = {
                'symbol': self.symbol,
                'timestamp': int(orderbook_data.get('timestamp', timestamp)),
                'bids': orderbook_data.get('bids', []),
                'asks': orderbook_data.get('asks', [])
            }
            
            # Sort bids (desc) and asks (asc)
            if orderbook['bids']:
                orderbook['bids'] = sorted(orderbook['bids'], key=lambda x: float(x[0]), reverse=True)
            if orderbook['asks']:
                orderbook['asks'] = sorted(orderbook['asks'], key=lambda x: float(x[0]))
            
            # Update internal state
            self.ws_data['orderbook'] = orderbook
            self.ws_data['last_update_time']['orderbook'] = timestamp
            
            # Log update
            self.logger.debug(f"Updated orderbook from WebSocket: {len(orderbook['bids'])} bids, {len(orderbook['asks'])} asks")
            
        except Exception as e:
            self.logger.error(f"Error processing orderbook update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_trade_update(self, message: Dict[str, Any]) -> None:
        """Process trade update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Extract trade data
            trade_data = data.get('data', [])
            if not trade_data:
                return
            
            # Format trade data
            trades = []
            for trade in trade_data:
                formatted_trade = {
                    'id': trade.get('tradeId', str(timestamp) + str(len(self.ws_data['trades']))),
                    'timestamp': int(trade.get('timestamp', timestamp)),
                    'price': float(trade.get('price', 0)),
                    'amount': float(trade.get('size', 0)),
                    'side': trade.get('side', 'unknown').lower(),
                    'symbol': self.symbol
                }
                trades.append(formatted_trade)
            
            # Update internal state - keep only most recent 1000 trades
            self.ws_data['trades'] = (trades + self.ws_data['trades'])[:1000]
            self.ws_data['last_update_time']['trades'] = timestamp
            
            # Log update
            self.logger.debug(f"Added {len(trades)} new trades from WebSocket. Total: {len(self.ws_data['trades'])}")
            
        except Exception as e:
            self.logger.error(f"Error processing trade update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _process_liquidation_update(self, message: Dict[str, Any]) -> None:
        """Process liquidation update from WebSocket.
        
        Args:
            message: WebSocket message data
        """
        try:
            data = message.get('data', {})
            timestamp = message.get('timestamp', self.timestamp_utility.get_utc_timestamp())
            
            # Enhanced debug logging for incoming message
            self.logger.debug(f"RECEIVED LIQUIDATION MSG: {json.dumps(message, default=str)[:200]}...")
            
            # Extract liquidation data
            liquidation_data = data.get('data', {})
            if not liquidation_data:
                self.logger.warning("Empty liquidation data received")
                return
            
            # Format liquidation data
            liquidation = {
                'symbol': self.symbol,
                'timestamp': int(liquidation_data.get('timestamp', timestamp)),
                'price': float(liquidation_data.get('price', 0)),
                'size': float(liquidation_data.get('size', 0)),
                'side': liquidation_data.get('side', 'unknown').lower(),
                'source': 'websocket'
            }
            
            # Log liquidation event
            self.logger.warning(f"Liquidation detected: {liquidation['side']} {liquidation['size']} {self.symbol} @ {liquidation['price']}")
            
            # Save to cache
            symbol_str = self._get_symbol_string(self.symbol)
            self.logger.debug(f"SAVING TO LIQUIDATION CACHE: {symbol_str} -> {liquidation}")
            liquidation_cache.append(liquidation, symbol_str)
            
            # Verify cache immediately after saving
            cached_data = liquidation_cache.load(symbol_str)
            self.logger.debug(f"VERIFICATION - CACHE NOW CONTAINS: {len(cached_data) if cached_data else 0} events")
            
            # If health monitor is available, create alert
            if self.health_monitor:
                self.health_monitor._create_alert(
                    level="info",
                    source=f"liquidation:{self.exchange_id}:{self.symbol_str}",
                    message=f"Liquidation: {liquidation['side']} {liquidation['size']} {self.symbol} @ {liquidation['price']}"
                )
            
        except Exception as e:
            self.logger.error(f"Error processing liquidation update: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def close(self) -> None:
        """Close connections and clean up resources."""
        try:
            self.logger.info(f"Closing MarketMonitor for {self.symbol}")
            
            # Close WebSocket connection if available
            if self.ws_manager:
                await self.ws_manager.close()
                self.logger.info("WebSocket connection closed")
            
        except Exception as e:
            self.logger.error(f"Error closing MarketMonitor: {str(e)}")
            self.logger.debug(traceback.format_exc())

    def get_websocket_status(self) -> Dict[str, Any]:
        """Get current WebSocket status."""
        if self.ws_manager:
            status = self.ws_manager.get_status()
            # Add data freshness
            status['data_freshness'] = {
                data_type: time.time() - timestamp/1000 if timestamp > 0 else float('inf')
                for data_type, timestamp in self.ws_data['last_update_time'].items()
            }
            return status
        else:
            return {
                'connected': False,
                'enabled': self.websocket_config.get('enabled', False)
            }

    def _handle_shutdown(self, signum, frame):
        """Handle shutdown signals gracefully"""
        logger.info("\nShutdown signal received. Cleaning up...")
        self.running = False

    async def start(self):
        """Start the market monitor."""
        try:
            # Check for required components
            if not self.exchange_manager:
                self.logger.error("No exchange manager available")
                return
                
            if not self.top_symbols_manager:
                self.logger.error("No top symbols manager available")
                return
                
            if not self.market_data_manager:
                self.logger.error("No market data manager available")
                return
            
            # Get primary exchange from exchange manager if not already set
            if not self.exchange:
                self.exchange = await self.exchange_manager.get_primary_exchange()
                if not self.exchange:
                    self.logger.error("No primary exchange available")
                    return
                    
                # Update exchange ID
                self.exchange_id = self.exchange.exchange_id
                
            self.logger.debug(f"Exchange instance retrieved: {bool(self.exchange)}")
            self.logger.debug("Initializing exchange...")
            
            # Initialize exchange
            await self.exchange.initialize()
            
            self.logger.debug("Setting exchange in TopSymbolsManager...")
            self.top_symbols_manager.set_exchange(self.exchange)
            
            self.logger.info("Waiting for initial data collection...")
            self.logger.info("Updating top symbols...")
            
            # Get symbols directly from top_symbols_manager
            self.symbols = await self.top_symbols_manager.get_symbols()
            if not self.symbols:
                self.logger.warning("No symbols to monitor")
                return
                
            self.logger.info(f"Monitoring symbols: {self.symbols}")
            
            # Extract symbol strings from symbol dictionaries for components that need simple strings
            symbol_strings = []
            for symbol_data in self.symbols:
                if isinstance(symbol_data, dict) and 'symbol' in symbol_data:
                    symbol_strings.append(symbol_data['symbol'])
                elif isinstance(symbol_data, str):
                    symbol_strings.append(symbol_data)
                    
            # Initialize market data manager with symbols to monitor
            await self.market_data_manager.initialize(symbol_strings)
            
            # Start data monitoring in the market data manager
            await self.market_data_manager.start_monitoring()
            
            # Initialize monitoring state
            self.running = True
            self._last_update_time = time.time()
            
            # Start monitoring tasks
            self.logger.info("Starting monitoring tasks...")
            self._monitoring_task = asyncio.create_task(self._run_monitoring_loop())
            
            # Start metrics manager if available
            if self.metrics_manager:
                self.logger.info("Starting metrics manager...")
                await self.metrics_manager.start()
                
            # Start alert manager if available  
            if self.alert_manager:
                self.logger.info("Starting alert manager...")
                await self.alert_manager.start()
                
            # Start the scheduled market reports task if market_reporter is available
            if hasattr(self, 'market_reporter') and self.market_reporter is not None:
                self.logger.info("Starting scheduled market reports service...")
                # Create a background task for the market reporter's scheduled reports
                self._market_report_task = asyncio.create_task(self.market_reporter.run_scheduled_reports())
                self.logger.info("Scheduled market reports service started")
                
            # Generate initial market report on startup, regardless of scheduled times
            self.logger.info("Generating initial market report...")
            try:
                await self._generate_market_report()
                self.last_report_time = datetime.now(timezone.utc)
                self.logger.info("Initial market report generated successfully")
            except Exception as e:
                self.logger.error(f"Error generating initial market report: {str(e)}")
                self.logger.debug(traceback.format_exc())
                
            self.logger.info("Market monitor started successfully")
            
        except Exception as e:
            self.logger.error(f"Error starting monitor: {str(e)}")
            self.logger.debug(traceback.format_exc())
            raise

    async def _monitoring_cycle(self):
        """Run a single monitoring cycle."""
        try:
            logger.debug("=== Starting Monitoring Cycle ===")
            
            # Get symbols to monitor
            symbols = await self.top_symbols_manager.get_symbols()
            symbol_display = [s['symbol'] if isinstance(s, dict) and 'symbol' in s else s for s in symbols[:5]]
            if len(symbols) > 5:
                symbol_display.append('...')
            logger.debug(f"Symbol manager returned {len(symbols)} symbols: {symbol_display}")
            
            if not symbols:
                logger.warning("Empty symbol list detected!")
                return
            
            # Process each symbol - now using MarketDataManager for efficient data fetching
            for symbol in symbols:
                try:
                    await self._process_symbol(symbol)
                except Exception as e:
                    self.logger.error(f"Error processing {symbol}: {str(e)}")
                    continue
                
            # Check if it's time for a report
            current_time = datetime.now(timezone.utc)
            should_generate_report = (
                # Generate a report if none has been generated yet
                not self.last_report_time or
                # Or if it's a scheduled report time and enough time has passed
                (self._is_report_time() and 
                 (current_time - self.last_report_time).total_seconds() > 300)
            )
            
            if should_generate_report:
                await self._generate_market_report()
                self.last_report_time = current_time
            
        except Exception as e:
            self.logger.error(f"Monitoring cycle error: {str(e)}")
            self.logger.debug(f"Stack trace:\n{traceback.format_exc()}")

    async def _process_symbol(self, symbol: str) -> None:
        """Process a single symbol for market analysis.
        
        This method fetches market data, validates it, runs analysis, and generates alerts.
        
        Args:
            symbol: Symbol to process
        """
        if not self.exchange_manager:
            self.logger.error("Exchange manager not available")
            return

        if not self.alert_manager:
            self.logger.warning("Alert manager not initialized")
        
        try:
            # Extract symbol string if needed
            symbol_str = symbol['symbol'] if isinstance(symbol, dict) and 'symbol' in symbol else symbol
            
            # Get market data from MarketDataManager
            self.logger.info(f"=== Starting analysis process for {symbol_str} ===")
            market_data = await self.fetch_market_data(symbol_str)
            if not market_data:
                self.logger.error(f"No market data available for {symbol_str}")
                return

            # Log what data we have
            data_components = []
            if 'ticker' in market_data and market_data['ticker']:
                data_components.append('ticker')
            if 'orderbook' in market_data and market_data['orderbook']:
                data_components.append('orderbook')
            if 'trades' in market_data and isinstance(market_data['trades'], list):
                data_components.append(f"trades ({len(market_data['trades'])})")
            if 'ohlcv' in market_data and market_data['ohlcv']:
                timeframes = market_data['ohlcv'].keys()
                data_components.append(f"ohlcv ({', '.join(timeframes)})")
            
            self.logger.info(f"Market data for {symbol_str} contains: {', '.join(data_components)}")

            # Validate market data
            self.logger.info(f"Validating market data for {symbol_str}")
            try:
                # Now we can properly await the validate_market_data method since it's async
                validation_result = await self.validate_market_data(market_data)
                if not validation_result:
                    self.logger.error(f"Invalid data for {symbol_str}")
                    return
            except TypeError as e:
                self.logger.error(f"Error validating market data: {str(e)}")
                # Fall back to legacy sync validation method if needed
                if hasattr(self, 'validate_market_data_sync'):
                    try:
                        validation_result = self.validate_market_data_sync(market_data)
                        if not validation_result:
                            self.logger.error(f"Invalid data for {symbol_str} (sync validation)")
                            return
                    except Exception as inner_e:
                        self.logger.error(f"Error in sync validation: {str(inner_e)}")
                        self.logger.error("Continuing with analysis anyway despite validation errors")
                else:
                    self.logger.error("Cannot validate market data. Continuing with analysis anyway.")
            except Exception as e:
                self.logger.error(f"Unexpected error during validation: {str(e)}")
                self.logger.debug(traceback.format_exc())
                self.logger.warning("Continuing with analysis despite validation errors")

            # Check if we have the confluence analyzer
            if not self.confluence_analyzer:
                self.logger.error("Confluence analyzer not initialized - cannot perform analysis")
                return

            # Process analysis
            self.logger.info(f"=== Running confluence analysis for {symbol_str} ===")
            start_time = time.time()
            try:
                analysis_result = await self.confluence_analyzer.analyze(market_data)
                elapsed = time.time() - start_time
                self.logger.info(f"Confluence analysis completed in {elapsed:.2f}s")
                
                # Log analysis results
                if analysis_result:
                    score = analysis_result.get('score', analysis_result.get('confluence_score', 0))
                    reliability = analysis_result.get('reliability', 0)
                    # Format safely before using in f-string
                    score_str = f"{score:.2f}" if score is not None else "N/A"
                    reliability_str = f"{reliability:.2f}" if reliability is not None else "N/A"
                    self.logger.info(f"Confluence score: {score_str} (reliability: {reliability_str})")
                    
                    # Log component scores if available
                    components = analysis_result.get('components', {})
                    if components:
                        component_scores = []
                        for name, value in components.items():
                            if isinstance(value, dict) and 'score' in value:
                                component_scores.append(f"{name}: {value['score']:.2f}")
                            elif isinstance(value, (int, float)):
                                component_scores.append(f"{name}: {value:.2f}")
                        if component_scores:
                            self.logger.info(f"Component scores: {', '.join(component_scores)}")
                else:
                    self.logger.warning(f"Confluence analysis returned no results for {symbol_str}")
            except Exception as e:
                elapsed = time.time() - start_time
                self.logger.error(f"Confluence analysis failed after {elapsed:.2f}s: {str(e)}")
                self.logger.error(traceback.format_exc())
                return
            
            # Process the analysis result (generate signals, etc.)
            self.logger.info(f"Processing analysis results for {symbol_str}")
            await self._process_analysis_result(symbol_str, analysis_result)
            self.logger.info(f"=== Completed analysis process for {symbol_str} ===")

        except AttributeError as e:
            self.logger.error(f"Missing component: {str(e)}")
        except Exception as e:
            self.logger.error(f"Processing failed for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def stop(self) -> None:
        """Stop the market monitor."""
        self.running = False
        
        # Stop the market data manager
        await self.market_data_manager.stop()
        
        # Cancel the scheduled market reports task if it exists
        if hasattr(self, '_market_report_task') and self._market_report_task is not None:
            self.logger.info("Stopping scheduled market reports service...")
            self._market_report_task.cancel()
            try:
                await self._market_report_task
            except asyncio.CancelledError:
                pass  # Expected behavior when cancelling a task
            self.logger.info("Scheduled market reports service stopped")
        
        # Cleanup other resources
        await self._cleanup()

    async def fetch_market_data(self, symbol: str) -> Dict[str, Any]:
        """Fetch market data for a symbol from the market data manager.
        
        Args:
            symbol: The symbol to fetch market data for
            
        Returns:
            Dict[str, Any]: Market data dictionary with various components
        """
        try:
            if not self.market_data_manager:
                self.logger.error("Market data manager not initialized")
                return None
                
            # Fetch market data through the manager
            market_data = await self.market_data_manager.get_market_data(symbol)
            
            if not market_data:
                self.logger.warning(f"No market data returned for {symbol}")
                return None
                
            return market_data
        except Exception as e:
            self.logger.error(f"Error fetching market data for {symbol}: {{str(e)}}")
            self.logger.debug(traceback.format_exc())
            return None

    async def _cleanup(self):
        """Cleanup resources."""
        try:
            logger.info("Cleaning up resources...")
            # Close exchange connections
            if hasattr(self, 'exchange_manager'):
                await self.exchange_manager.close()
            
            # Cancel any pending tasks
            for task in asyncio.all_tasks():
                if task is not asyncio.current_task():
                    task.cancel()
                    
            # Close WebSocket connection
            if hasattr(self, '_ws') and self._ws:
                await self._ws.close()
            
            logger.info("Cleanup completed")
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")

    async def _run_monitoring_loop(self) -> None:
        """Main monitoring loop."""
        self.logger.info("Starting monitoring loop...")
        
        while self.running:
            try:
                # Run monitoring cycle
                await self._monitoring_cycle()
                
                # Update monitoring metrics
                await self._update_metrics(None)  # We'll update metrics without analysis results
                
                # Check system health
                health_status = await self._check_system_health()
                if health_status['status'] != 'healthy':
                    self.logger.warning(f"System health check: {health_status['status']}")
                    
                    # Check if any component is critical
                    critical_components = [
                        comp for comp, status in health_status['components'].items()
                        if status.get('status') == 'critical'
                    ]
                    
                    if critical_components:
                        self.logger.error(f"Critical system health issues detected: {', '.join(critical_components)}")
                        
                        # Generate alert for critical components
                        await self._generate_alert(
                            f"Critical system health issues detected: {', '.join(critical_components)}"
                        )
                
                # Check and update market data manager statistics
                mdm_stats = self.market_data_manager.get_stats()
                if self.metrics_manager:
                    await self.metrics_manager.update_system_metrics({
                        'market_data_manager': {
                            'rest_calls': mdm_stats.get('rest_calls', 0),
                            'websocket_updates': mdm_stats.get('websocket_updates', 0),
                            'websocket_connected': mdm_stats.get('websocket', {}).get('connected', False)
                        }
                    })
                
                # Sleep until next cycle
                await asyncio.sleep(self.interval)
                
            except asyncio.CancelledError:
                self.logger.info("Monitoring loop cancelled")
                break
                
            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {str(e)}")
                self.logger.debug(traceback.format_exc())
                self._error_count += 1
                
                # Back off on errors
                await asyncio.sleep(5)

    async def validate_market_data(self, market_data: Dict[str, Any]) -> bool:
        """Validate market data using the data validator.
        
        This is the central validation method that replaces multiple redundant implementations.
        It performs comprehensive validation of market data structure, freshness, and content.
        
        Args:
            market_data: Market data to validate
            
        Returns:
            bool: True if market data is valid
        """
        # Start performance tracking
        operation = self.metrics_manager.start_operation("validate_market_data")
        
        try:
            # Log validation operation
            self.logger.debug(f"Validating market data for {market_data.get('symbol', 'unknown')}")
            
            # First check basic structure
            if not isinstance(market_data, dict):
                self.logger.error("Market data must be a dictionary")
                return False
            
            # Check for required base fields
            required_fields = ['symbol', 'timestamp']
            missing_fields = [field for field in required_fields if field not in market_data]
            if missing_fields:
                self.logger.error(f"Missing required fields in market data: {missing_fields}")
                return False
            
            # Use the data validator for comprehensive validation
            # Now data_validator.validate_market_data() is an ASYNCHRONOUS method
            if hasattr(self, 'data_validator') and self.data_validator is not None:
                # Call the validator's method asynchronously
                self.logger.debug("Calling data_validator.validate_market_data asynchronously")
                result = await self.data_validator.validate_market_data(market_data)
                
                # Get validation stats for metrics
                validation_stats = self.data_validator.get_validation_stats()
                
                # Record validation metrics
                self.metrics_manager.record_metric("validation.total", validation_stats['total_validations'])
                self.metrics_manager.record_metric("validation.passed", validation_stats['passed_validations'])
                self.metrics_manager.record_metric("validation.failed", validation_stats['failed_validations'])
            else:
                # If no data validator is available, do basic validation
                self.logger.warning("No data validator available, performing basic validation only")
                result = True  # Assume valid if basic checks pass
            
            # Check timeframes if ohlcv data is present
            if 'ohlcv' in market_data and hasattr(self, 'validate_timeframes'):
                # Make sure we're not awaiting the result if it's not a coroutine
                if callable(self.validate_timeframes):
                    try:
                        # Check if validate_timeframes is a coroutine function
                        if asyncio.iscoroutinefunction(self.validate_timeframes):
                            timeframe_results = await self.validate_timeframes(market_data.get('ohlcv', {}))
                        else:
                            # Call directly if it's a regular function
                            timeframe_results = self.validate_timeframes(market_data.get('ohlcv', {}))
                        if isinstance(timeframe_results, dict) and not any(timeframe_results.values()):
                            self.logger.error("All timeframes validation failed")
                            result = False
                    except Exception as e:
                        self.logger.error(f"Error validating timeframes: {str(e)}")
            
            # End performance tracking
            self.metrics_manager.end_operation(operation)
            return result
            
        except Exception as e:
            self.logger.error(f"Error validating market data: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # End operation with failure
            self.metrics_manager.end_operation(operation, success=False)
            return False


    async def validate_market_data_sync(self, market_data: Dict[str, Any]) -> bool:
        """Synchronous version of validate_market_data that delegates to the data validator.
        
        This method is now also asynchronous since the data validator's method is asynchronous,
        but it's kept for backward compatibility.
        
        Args:
            market_data: Market data to validate
            
        Returns:
            bool: True if market data is valid
        """
        try:
            self.logger.debug("=== SYNC VALIDATION DEBUG START ===")
            self.logger.debug(f"Processing market data with keys: {list(market_data.keys())}")
            
            # Preprocess market data to ensure numeric types are properly handled
            self.logger.debug("Starting data preprocessing")
            processed_market_data = self._preprocess_market_data_for_validation(market_data)
            self.logger.debug("Preprocessing completed")
            
            # Call validation on preprocessed data
            self.logger.debug("Starting validation on preprocessed data")
            validation_result = await self.data_validator.validate_market_data(processed_market_data)
            self.logger.debug(f"Validation result on preprocessed data: {validation_result}")
            
            self.logger.debug("=== SYNC VALIDATION DEBUG END ===")
            return validation_result
            
        except Exception as e:
            self.logger.error(f"Error in synchronous market data validation: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Try to continue with unprocessed data if preprocessing failed
            try:
                self.logger.debug("=== FALLBACK VALIDATION DEBUG START ===")
                self.logger.debug("Starting fallback validation with original data")
                fallback_result = await self.data_validator.validate_market_data(market_data)
                self.logger.debug(f"Fallback validation result: {fallback_result}")
                self.logger.debug("=== FALLBACK VALIDATION DEBUG END ===")
                return fallback_result
            except Exception as inner_e:
                self.logger.error(f"Fallback validation also failed: {str(inner_e)}")
                self.logger.debug(traceback.format_exc())
            return False
            
    def _preprocess_market_data_for_validation(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Preprocess market data to ensure numeric values are handled correctly.
        
        This method converts string numeric values to float in common fields that might
        cause comparison issues during validation.
        
        Args:
            market_data: Raw market data dictionary
            
        Returns:
            Dict[str, Any]: Processed market data with numeric conversions
        """
        # Create a deep copy to avoid modifying the original data
        processed = copy.deepcopy(market_data)
        
        try:
            # Add enhanced debugging - log the input structure but only at start
            self.logger.debug("=== PREPROCESSING MARKET DATA DEBUG ===")
            self.logger.debug(f"Market data keys: {list(market_data.keys())}")
            
            # Process orderbook data
            if 'orderbook' in processed and isinstance(processed['orderbook'], dict):
                self.logger.debug(f"Processing orderbook with keys: {list(processed['orderbook'].keys())}")
                
                # Handle nested orderbook structure (Bybit format with result.b and result.a)
                if 'result' in processed['orderbook'] and isinstance(processed['orderbook']['result'], dict):
                    result_data = processed['orderbook']['result']
                    if 'b' in result_data and 'a' in result_data:
                        # Copy bids and asks to standard format
                        processed['orderbook']['bids'] = result_data['b']
                        processed['orderbook']['asks'] = result_data['a']
                        self.logger.debug("Restructured orderbook from result.b/result.a format")
                
                # Log summary of processing, not individual levels
                for side in ['bids', 'asks']:
                    if side in processed['orderbook'] and isinstance(processed['orderbook'][side], list):
                        levels_count = len(processed['orderbook'][side])
                        converted_count = 0
                        
                        for i, level in enumerate(processed['orderbook'][side]):
                            if isinstance(level, (list, tuple)) and len(level) >= 2:
                                # Convert price and size to float
                                try:
                                    if isinstance(level[0], str):
                                        processed['orderbook'][side][i] = [float(level[0]), float(level[1])]
                                        converted_count += 1
                                except (ValueError, TypeError):
                                    # Leave as is if conversion fails
                                    pass
                        
                        # Log summary instead of each conversion
                        if converted_count > 0:
                            self.logger.debug(f"Converted {converted_count}/{levels_count} orderbook {side} levels to float")
            
            # Process trades data with less verbose logging
            if 'trades' in processed:
                self.logger.debug(f"Processing trades data, type: {type(processed['trades'])}")
                original_trades = processed['trades']
                
                # Detect the structure of trades data and extract/normalize the list
                trades_list = None
                trades_structure = 'unknown'
                
                # If trades are in a dict (like API responses with metadata)
                if isinstance(original_trades, dict):
                    # Check for Bybit specific nested structure (result.list)
                    if 'result' in original_trades and isinstance(original_trades['result'], dict) and 'list' in original_trades['result']:
                        trades_list = original_trades['result']['list']
                        trades_structure = 'result.list'
                    # Check if trades are nested in a 'list' key
                    elif 'list' in original_trades:
                        trades_list = original_trades['list']
                        trades_structure = 'list'
                    # Check for 'result' containing a list directly
                    elif 'result' in original_trades and isinstance(original_trades['result'], list):
                        trades_list = original_trades['result']
                        trades_structure = 'result'
                    # Handle other potential structures
                    elif 'data' in original_trades and isinstance(original_trades['data'], list):
                        trades_list = original_trades['data']
                        trades_structure = 'data'
                # If trades is already a list
                elif isinstance(original_trades, list):
                    trades_list = original_trades
                    trades_structure = 'direct_list'
                
                # If we successfully extracted a trades list, process it
                if trades_list is not None:
                    self.logger.debug(f"Extracted trades list from '{trades_structure}' structure with {len(trades_list)} trades")
                    processed['trades_original_format'] = trades_structure
                    
                    # Process the extracted trades list
                    processed_trades = []
                    total_conversions = 0
                    
                    for i, trade in enumerate(trades_list):
                        if isinstance(trade, dict):
                            # Create a processed copy of the trade
                            processed_trade = trade.copy()
                            
                            # Log only the first trade as a sample
                            if i == 0:
                                self.logger.debug(f"Sample trade keys: {list(trade.keys())}")
                                
                            # Convert price and amount/size to float
                            converted_fields = 0
                            for field in trade:
                                if field not in ['symbol', 'side', 'id', 'execId', 'direction', 'tickDirection'] and isinstance(trade[field], str):
                                    try:
                                        processed_trade[field] = float(trade[field])
                                        converted_fields += 1
                                    except (ValueError, TypeError):
                                        pass
                            
                            total_conversions += converted_fields
                            
                            # Handle field mapping but log only first trade
                            if 'price' not in processed_trade and 'execPrice' in processed_trade:
                                processed_trade['price'] = processed_trade['execPrice']
                                if i == 0:
                                    self.logger.debug(f"Mapped execPrice to price field")
                            
                            if 'size' not in processed_trade and 'execQty' in processed_trade:
                                processed_trade['size'] = processed_trade['execQty']
                                if i == 0:
                                    self.logger.debug(f"Mapped execQty to size field")
                            
                            if 'amount' not in processed_trade and 'size' in processed_trade:
                                processed_trade['amount'] = processed_trade['size']
                                if i == 0:
                                    self.logger.debug(f"Added amount field from size")

                            # Map execId to id (needed for orderflow analysis)
                            if "id" not in processed_trade and "execId" in processed_trade:
                                processed_trade["id"] = processed_trade["execId"]
                                if i == 0:
                                    self.logger.debug(f"Mapped execId to id field")
                            
                            processed_trades.append(processed_trade)
                    
                    # Update processed data with the normalized trades list
                    processed['trades'] = processed_trades
                    self.logger.debug(f"Processed {len(processed_trades)} trades with {total_conversions} field conversions")
                else:
                    # Enhanced fallback handling for unknown trade structures
                    self.logger.warning(f"Could not extract trades list from trades data, attempting additional fallbacks")
                    
                    # If it's a dictionary, try to identify any list that might contain trades
                    if isinstance(original_trades, dict):
                        potential_trade_lists = []
                        
                        # Try to find any list with trade-like data in the dict
                        for key, value in original_trades.items():
                            if isinstance(value, list) and len(value) > 0:
                                potential_trade_lists.append((key, value))
                        
                        # If we found potential trade lists, use the one that looks most like trades
                        if potential_trade_lists:
                            # Sort by length - assume longer lists are more likely to be trades
                            potential_trade_lists.sort(key=lambda x: len(x[1]), reverse=True)
                            
                            # Check if items in the list look like trades (has price or amount)
                            for key, value in potential_trade_lists:
                                if value and isinstance(value[0], dict):
                                    # Check if it has trade-like fields
                                    first_item = value[0]
                                    trade_fields = ['price', 'amount', 'size', 'quantity', 'side', 'time', 'timestamp']
                                    
                                    # If it has at least one trade-like field, use this list
                                    if any(field in first_item for field in trade_fields):
                                        trades_list = value
                                        trades_structure = f"extracted_from_{key}"
                                        self.logger.debug(f"Found trade-like list in key '{key}' with {len(trades_list)} items")
                                        
                                        # Process this list similarly to the normal case
                                        processed_trades = []
                                        for trade in trades_list:
                                            if isinstance(trade, dict):
                                                processed_trade = trade.copy()
                                                for field in trade:
                                                    if field not in ['symbol', 'side', 'id'] and isinstance(trade[field], str):
                                                        try:
                                                            processed_trade[field] = float(trade[field])
                                                        except (ValueError, TypeError):
                                                            pass
                                                processed_trades.append(processed_trade)
                                        
                                        processed['trades'] = processed_trades
                                        self.logger.debug(f"Processed {len(processed_trades)} trades from fallback approach")
                                        break
                        
                        # If we still don't have a list, create an empty one
                        if 'trades' not in processed or not isinstance(processed['trades'], list):
                            self.logger.warning("All fallback approaches failed, creating empty trades list")
                            processed['trades'] = []
                    else:
                        # If it's not a dict or list, create an empty list
                        self.logger.warning(f"Trades data type {type(original_trades)} is not supported, creating empty list")
                        processed['trades'] = []
            
            # Process ticker data more efficiently
            if 'ticker' in processed and isinstance(processed['ticker'], dict):
                converted_count = 0
                ticker_fields = list(processed['ticker'].keys())
                
                for field in ticker_fields:
                    value = processed['ticker'][field]
                    if isinstance(value, str) and field not in ['symbol']:
                        try:
                            processed['ticker'][field] = float(value)
                            converted_count += 1
                        except (ValueError, TypeError):
                            pass
                
                # Log summary instead of individual conversions
                if converted_count > 0:
                    self.logger.debug(f"Converted {converted_count}/{len(ticker_fields)} ticker fields to float")
            
            self.logger.debug("=== END PREPROCESSING DEBUG ===")        
            return processed
            
        except Exception as e:
            self.logger.error(f"Error preprocessing market data: {str(e)}")
            self.logger.error(traceback.format_exc())
            # Return original data if preprocessing fails
            return market_data

    async def _update_metrics(self, analysis_results: Optional[List[Dict[str, Any]]]) -> None:
        """Update monitoring metrics."""
        try:
            if not self.metrics_manager:
                return
                
            # Convert all values to float and ensure they are numeric
            metrics = {
                'total_messages': float(self._stats['total_messages']),
                'invalid_messages': float(self._stats['invalid_messages']),
                'delayed_messages': float(self._stats['delayed_messages']),
                'error_count': float(self._stats['error_count']),
                'last_update_time': float(time.time())
            }
            
            # Add market data manager stats
            mdm_stats = self.market_data_manager.get_stats()
            metrics.update({
                'rest_calls': float(mdm_stats.get('rest_calls', 0)),
                'websocket_updates': float(mdm_stats.get('websocket_updates', 0)),
            })
            
            # Update monitoring metrics as system metrics
            await self.metrics_manager.update_system_metrics(metrics)
            
        except Exception as e:
            error_context = ErrorContext(
                component="market_monitor",
                operation="update_metrics"
            )
            if self.error_handler:
                await self.error_handler.handle_error(
                    error=e,
                    context=error_context,
                    severity=ErrorSeverity.LOW
                )
            else:
                logger.error(f"Error updating metrics: {str(e)}")
                
    async def _check_system_health(self) -> Dict[str, Any]:
        """Check overall system health."""
        try:
            health_status = {
                'status': 'healthy',
                'components': {
                    'exchange': await self._check_exchange_health(),
                    'database': await self._check_database_health(),
                    'memory': await self._check_memory_usage(),
                    'cpu': await self._check_cpu_usage(),
                    'market_data_manager': await self._check_market_data_manager_health()
                }
            }
            
            # Check if any component is unhealthy
            for component, status in health_status['components'].items():
                if status.get('status') != 'healthy':
                    health_status['status'] = 'warning'
                    
                    if status.get('status') == 'critical':
                        health_status['status'] = 'critical'
                        break
                    
            return health_status
            
        except Exception as e:
            self.logger.error(f"Error checking system health: {str(e)}")
            return {'status': 'error', 'message': str(e)}

    async def _check_exchange_health(self) -> Dict[str, Any]:
        """Check exchange connectivity and response times."""
        try:
            if not self.exchange_manager:
                return {'status': 'error', 'message': 'Exchange not initialized'}
                
            # Test API connection
            await self.exchange_manager.ping()
            return {'status': 'healthy'}
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_market_data_manager_health(self) -> Dict[str, Any]:
        """Check health of the market data manager."""
        try:
            stats = self.market_data_manager.get_stats()
            websocket_status = stats.get('websocket', {})
            
            # Check WebSocket connection
            if not websocket_status.get('connected', False):
                return {
                    'status': 'warning',
                    'message': 'WebSocket not connected',
                    'details': websocket_status
                }
            
            # Check time since last WebSocket message
            seconds_since_last_message = websocket_status.get('seconds_since_last_message', 0)
            if seconds_since_last_message > 60:  # No message in last minute
                return {
                    'status': 'warning',
                    'message': f'No WebSocket message received in {seconds_since_last_message:.1f}s',
                    'details': websocket_status
                }
            
            # Check data freshness (oldest symbol data)
            if 'data_freshness' in stats:
                max_age = 0
                oldest_symbol = None
                
                for symbol, freshness in stats['data_freshness'].items():
                    age = freshness.get('age_seconds', 0)
                    if age > max_age:
                        max_age = age
                        oldest_symbol = symbol
                
                if max_age > 300:  # Data older than 5 minutes
                    return {
                        'status': 'warning',
                        'message': f'Data for {oldest_symbol} is {max_age:.1f}s old',
                        'details': {
                            'oldest_symbol': oldest_symbol,
                            'age_seconds': max_age
                        }
                    }
            
            # All checks passed
            return {'status': 'healthy'}
            
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_database_health(self) -> Dict[str, Any]:
        """Check database connectivity."""
        try:
            # Add your database health check logic here
            return {'status': 'healthy'}
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_memory_usage(self) -> Dict[str, Any]:
        """Check system memory usage."""
        try:
            import psutil
            memory = psutil.virtual_memory()
            return {
                'status': 'healthy' if memory.percent < 90 else 'warning',
                'usage': memory.percent
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_cpu_usage(self) -> Dict[str, Any]:
        """Check CPU usage."""
        try:
            import psutil
            cpu_percent = psutil.cpu_percent(interval=1)
            return {
                'status': 'healthy' if cpu_percent < 80 else 'warning',
                'usage': cpu_percent
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}

    async def _check_thresholds(self) -> None:
        """Check monitoring thresholds and generate alerts."""
        try:
            # Check invalid message ratio
            if self._stats['total_messages'] > 0:
                invalid_ratio = (
                    self._stats['invalid_messages'] /
                    self._stats['total_messages']
                )
                
                if invalid_ratio > self.config.get('max_invalid_ratio', 0.1):
                    await self._generate_alert(
                        f"High invalid message ratio: {invalid_ratio:.2%}"
                    )
                    
            # Check delayed message ratio
            if self._stats['total_messages'] > 0:
                delayed_ratio = (
                    self._stats['delayed_messages'] /
                    self._stats['total_messages']
                )
                
                if delayed_ratio > self.config.get('max_delayed_ratio', 0.1):
                    await self._generate_alert(
                        f"High delayed message ratio: {delayed_ratio:.2%}"
                    )
                    
            # Check error count
            if self._stats['error_count'] > self.config.get('max_errors', 100):
                await self._generate_alert(
                    f"High error count: {self._stats['error_count']}"
                )
                
        except Exception as e:
            logger.error(f"Error checking thresholds: {str(e)}")
            
    async def _generate_alert(self, message: str) -> None:
        """Generate monitoring alert.
        
        Args:
            message: Alert message
        """
        try:
            if self.alert_manager:
                await self.alert_manager.send_alert(
                    message=message,
                    level="warning",
                    component="monitor"
                )
            else:
                logger.warning(f"Monitor alert: {message}")
                
        except Exception as e:
            logger.error(f"Error generating alert: {str(e)}")
            
    @property
    def stats(self) -> Dict[str, Any]:
        """Get monitoring statistics."""
        # Combine internal stats with market data manager stats
        combined_stats = self._stats.copy()
        
        # Add market data manager stats if available
        if hasattr(self, 'market_data_manager'):
            mdm_stats = self.market_data_manager.get_stats()
            combined_stats['market_data_manager'] = mdm_stats
        
        return combined_stats

    async def _process_analysis_result(self, symbol: str, result: Dict[str, Any]) -> None:
        """Process analysis result and generate signals if appropriate."""
        try:
            # Extract key information
            confluence_score = result.get('score', result.get('confluence_score', 0))
            reliability = result.get('reliability', 0)
            components = result.get('components', {})
            
            # Get thresholds from config.confluence section for consistency
            confluence_config = self.config.get('confluence', {})
            threshold_config = confluence_config.get('thresholds', {})
            buy_threshold = float(threshold_config.get('buy', 60))
            sell_threshold = float(threshold_config.get('sell', 40))
            neutral_buffer = float(threshold_config.get('neutral_buffer', 5))
            
            # Log component scores
            self.logger.debug("\n=== Component Scores ===")
            for component, score in components.items():
                self.logger.debug(f"{component}: {score}")
                
            # Display comprehensive confluence score table with top components and interpretations
            from src.core.formatting import LogFormatter
            formatted_table = LogFormatter.format_enhanced_confluence_score_table(
                symbol=symbol,
                confluence_score=confluence_score,
                components=components,
                results=result.get('results', {}),
                weights=result.get('metadata', {}).get('weights', {}),
                reliability=reliability
            )
            self.logger.info(formatted_table)
            
            # Log detailed market interpretations separately to ensure they appear in the logs
            
            # Generate signal if score meets thresholds
            self.logger.debug(f"=== Generating Signal ===")
            # Store threshold information in result for downstream processing
            result['buy_threshold'] = buy_threshold
            result['sell_threshold'] = sell_threshold
            
            # Format score safely for logging
            score_str = f"{confluence_score:.2f}" if confluence_score is not None else "N/A"
            buy_threshold_str = f"{buy_threshold:.2f}" if buy_threshold is not None else "N/A"
            sell_threshold_str = f"{sell_threshold:.2f}" if sell_threshold is not None else "N/A"
            
            if confluence_score >= buy_threshold:
                await self._generate_signal(symbol, result)
                self.logger.info(f"Generated BUY signal for {symbol} with score {score_str} (threshold: {buy_threshold_str})")
            elif confluence_score <= sell_threshold:
                await self._generate_signal(symbol, result)
                self.logger.info(f"Generated SELL signal for {symbol} with score {score_str} (threshold: {sell_threshold_str})")
            else:
                self.logger.debug(f"No signal generated - score {score_str} in neutral zone (buy: {buy_threshold_str}, sell: {sell_threshold_str})")

            # Update metrics
            if self.metrics_manager:
                await self.metrics_manager.update_analysis_metrics(symbol, result)

        except Exception as e:
            self.logger.error(f"Error processing analysis result: {str(e)}")
            self.logger.debug(traceback.format_exc())

    async def _generate_signal(self, symbol: str, analysis_result: Dict[str, Any]) -> None:
        """Generate trading signal based on analysis results with enhanced validation and tracking."""
        if not hasattr(self, 'signal_generator') or self.signal_generator is None:
            self.logger.error(f"Signal generator not available for {symbol}")
            return

        try:
            # Generate transaction and signal IDs for tracking
            transaction_id = str(uuid.uuid4())
            signal_id = str(uuid.uuid4())[:8]
            
            # Log the start of signal generation with transaction ID
            self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating signal for {symbol}")
            
            # Extract critical information
            if not analysis_result or not isinstance(analysis_result, dict):
                self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Invalid analysis result for {symbol}")
                return
                
            # Extract data from analysis result
            confluence_score = analysis_result.get('confluence_score', 0)
            components = analysis_result.get('components', {})
            results = analysis_result.get('results', {})
            
            # Get reliability score
            reliability = analysis_result.get('reliability', 0.5)
            
            # Get price information
            price = None
            if 'price' in analysis_result:
                price = analysis_result['price']
            elif 'market_data' in analysis_result and 'ticker' in analysis_result['market_data']:
                ticker = analysis_result['market_data']['ticker']
                price = ticker.get('last', ticker.get('close', None))
            
            if price is None and hasattr(self, 'market_data_manager'):
                try:
                    market_data = await self.market_data_manager.get_market_data(symbol)
                    if market_data and 'ticker' in market_data:
                        price = float(market_data['ticker'].get('last', market_data['ticker'].get('close', 0)))
                except Exception as e:
                    self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] Error getting price from market data: {str(e)}")
            
            # Get thresholds from config
            config = self.config.get('confluence', {}).get('thresholds', {})
            buy_threshold = float(config.get('buy', 70))
            sell_threshold = float(config.get('sell', 30))
            
            # Create enhanced signal data
            signal_data = {
                'symbol': symbol,
                'confluence_score': confluence_score,
                'components': components,
                'results': results,
                'weights': analysis_result.get('metadata', {}).get('weights', {}),
                'reliability': reliability,
                'price': price,
                'transaction_id': transaction_id,
                'signal_id': signal_id,
                'buy_threshold': buy_threshold,
                'sell_threshold': sell_threshold
            }
            
            # Add enhanced analysis data if available
            if 'market_interpretations' in analysis_result:
                signal_data['market_interpretations'] = analysis_result['market_interpretations']
            
            if 'actionable_insights' in analysis_result:
                signal_data['actionable_insights'] = analysis_result['actionable_insights']
                
            if 'influential_components' in analysis_result:
                signal_data['influential_components'] = analysis_result['influential_components']
            
            # Determine signal type based on thresholds
            signal_type = "NEUTRAL"
            if confluence_score >= buy_threshold:
                signal_type = "BUY"
            elif confluence_score <= sell_threshold:
                signal_type = "SELL"
            
            signal_data['signal_type'] = signal_type
            
            # Format values for logging, handling None cases
            score_str = f"{confluence_score:.2f}" if confluence_score is not None else 'N/A'
            reliability_str = f"{reliability:.2f}" if reliability is not None else 'N/A'
            price_str = f"${price:.2f}" if price is not None else 'N/A'
            
            # Log signal data before setting trade parameters
            signal_log = (
                f"[TXN:{transaction_id}][SIG:{signal_id}] {symbol} - "
                f"Score: {score_str} ({signal_type}) - "
                f"Reliability: {reliability_str} - "
                f"Price: {price_str}"
            )
            self.logger.info(signal_log)
            
            # Set trade parameters based on config
            try:
                signal_data['trade_params'] = self._calculate_trade_parameters(
                    symbol=symbol,
                    price=price,
                    signal_type=signal_type,
                    score=confluence_score,
                    reliability=reliability
                )
            except Exception as e:
                self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error calculating trade parameters: {str(e)}")
                self.logger.debug(traceback.format_exc())
                # Set default trade parameters on error
                signal_data['trade_params'] = {
                    'entry_price': price,
                    'stop_loss': None,
                    'take_profit': None,
                    'position_size': None,
                    'risk_reward_ratio': None,
                    'risk_percentage': None,
                    'confidence': min(confluence_score / 100, 1.0) if confluence_score is not None else 0.5,
                    'timeframe': 'auto'
                }
            
            # Add timestamp
            signal_data['timestamp'] = int(time.time() * 1000)
            
            # Generate enhanced formatted data if signal_generator is available
            if hasattr(self, 'signal_generator') and self.signal_generator:
                try:
                    self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating enhanced formatted data for {symbol}")
                    enhanced_data = self.signal_generator._generate_enhanced_formatted_data(
                        symbol,
                        confluence_score,
                        components,
                        results,
                        reliability,
                        buy_threshold,
                        sell_threshold
                    )
                    # Add enhanced data to signal_data
                    if enhanced_data:
                        signal_data.update(enhanced_data)
                        self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}] Successfully added enhanced data: market_interpretations={len(enhanced_data.get('market_interpretations', []))}, actionable_insights={len(enhanced_data.get('actionable_insights', []))}")
                except Exception as e:
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error generating enhanced data: {str(e)}")
                    self.logger.debug(traceback.format_exc())
            
            # Generate alert
            if self.alert_manager:
                await self.alert_manager.send_signal_alert(signal_data)
            else:
                self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] Alert manager not available for {symbol}")
            
            # Generate report if enabled
            if self.signal_generator and hasattr(self.signal_generator, 'report_generator'):
                try:
                    self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Generating report for {symbol}")
                    
                    # Get cached OHLCV data for the report
                    ohlcv_data = self.get_ohlcv_for_report(symbol)
                    
                    if ohlcv_data is not None:
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}] Using cached OHLCV data for {symbol} report ({len(ohlcv_data)} records)")
                        
                        # Measure report generation time
                        report_start_time = time.time()
                        
                        # Add debug info about the report generator
                        self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Using report generator: {type(self.signal_generator.report_generator).__name__}")
                        if hasattr(self.signal_generator.report_generator, "pdf_enabled"):
                            pdf_enabled = self.signal_generator.report_generator.pdf_enabled
                            self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF generation is {'enabled' if pdf_enabled else 'disabled'}")
                        
                        # Call generate_report with timing
                        report_result = await self.signal_generator.report_generator.generate_report(
                            signal_data=signal_data,
                            ohlcv_data=ohlcv_data
                        )
                        
                        # Calculate time taken
                        report_generation_time = time.time() - report_start_time
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report generation completed in {report_generation_time:.2f}s")
                        
                        # Check and log report result
                        if report_result:
                            self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report result type: {type(report_result)}")
                            
                            # Check if result is a dict and has a pdf_path
                            if isinstance(report_result, dict) and 'pdf_path' in report_result:
                                pdf_path = report_result['pdf_path']
                                self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF path from result: {pdf_path}")
                                
                                # Verify the PDF file exists and check its size
                                if os.path.exists(pdf_path):
                                    pdf_size = os.path.getsize(pdf_path)
                                    self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF file exists - Size: {pdf_size/1024:.2f} KB")
                                    
                                    # Record success metric
                                    if self.metrics_manager:
                                        self.metrics_manager.record_metric('signal_pdf_generation_success', 1)
                                        self.metrics_manager.record_metric('signal_pdf_size_kb', pdf_size/1024)
                                else:
                                    self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF path was returned but file does not exist: {pdf_path}")
                            else:
                                self.logger.debug(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report result keys: {list(report_result.keys()) if isinstance(report_result, dict) else 'Not a dict'}")
                    else:
                        self.logger.warning(f"[TXN:{transaction_id}][SIG:{signal_id}] No cached OHLCV data for {symbol} report, using simulated data")
                        
                        # Measure report generation time
                        report_start_time = time.time()
                        
                        # Call report generator with simulated data
                        report_result = await self.signal_generator.report_generator.generate_report(
                            signal_data=signal_data
                        )
                        
                        # Calculate time taken
                        report_generation_time = time.time() - report_start_time
                        self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Report generation with simulated data completed in {report_generation_time:.2f}s")
                        
                        # Check report result
                        if report_result and isinstance(report_result, dict) and 'pdf_path' in report_result:
                            pdf_path = report_result['pdf_path']
                            if os.path.exists(pdf_path):
                                pdf_size = os.path.getsize(pdf_path)
                                self.logger.info(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] PDF file with simulated data exists - Size: {pdf_size/1024:.2f} KB")
                except Exception as e:
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}] Error generating report: {str(e)}")
                    self.logger.error(f"[TXN:{transaction_id}][SIG:{signal_id}][PDF_DEBUG] Error type: {type(e).__name__}")
                    self.logger.error(traceback.format_exc())
                    
                    # Record failure metric
                    if self.metrics_manager:
                        self.metrics_manager.record_metric('signal_pdf_generation_failure', 1)
                        error_type = type(e).__name__
                        self.metrics_manager.record_metric(f'signal_pdf_error_{error_type}', 1)
            
            # Return generated signal
            return signal_data
            
        except Exception as e:
            self.logger.error(f"Error generating signal for {symbol}: {str(e)}")
            self.logger.error(traceback.format_exc())
            return None

    def _calculate_trade_parameters(self, symbol: str, price: float, signal_type: str, score: float, reliability: float) -> Dict[str, Any]:
        """
        Calculate trade parameters based on signal data and configuration.
        
        Args:
            symbol: Trading symbol
            price: Current price
            signal_type: Signal type (BUY, SELL, NEUTRAL)
            score: Confluence score (0-100)
            reliability: Signal reliability (0-1)
            
        Returns:
            Dict with calculated trade parameters
        """
        try:
            # Handle None values gracefully
            if price is None:
                self.logger.warning(f"Price is None for {symbol}, using default trade parameters")
                return {
                    'entry_price': None,
                    'stop_loss': None,
                    'take_profit': None,
                    'position_size': None,
                    'risk_reward_ratio': None,
                    'risk_percentage': None,
                    'confidence': min(score / 100, 1.0) if score is not None else None,
                    'timeframe': 'auto'
                }
                
            if score is None:
                score = 50.0  # Default to neutral score
                self.logger.warning(f"Score is None for {symbol}, using default value of {score}")
                
            if reliability is None:
                reliability = 0.5  # Default to medium reliability
                self.logger.warning(f"Reliability is None for {symbol}, using default value of {reliability}")
            
            # Default trade parameters
            trade_params = {
                'entry_price': price,
                'stop_loss': None,
                'take_profit': None,
                'position_size': None,
                'risk_reward_ratio': None,
                'risk_percentage': None,
                'confidence': min(score / 100, 1.0) if score is not None else 0.5,
                'timeframe': 'auto'
            }
            
            # If neutral signal, return default params
            if signal_type == "NEUTRAL":
                self.logger.debug(f"Neutral signal for {symbol}, using default trade parameters")
                return trade_params
                
            # Get trading config
            trading_config = self.config.get('trading', {})
            risk_config = trading_config.get('risk', {})
            
            # Calculate position size based on risk percentage
            account_balance = trading_config.get('account_balance', 10000)
            default_risk = risk_config.get('default_risk_percentage', 1.0)
            
            # Adjust risk based on reliability
            adjusted_risk = default_risk * reliability
            
            # Min and max risk bounds
            min_risk = risk_config.get('min_risk_percentage', 0.5)
            max_risk = risk_config.get('max_risk_percentage', 2.0)
            
            # Ensure risk is within bounds
            risk_percentage = max(min_risk, min(max_risk, adjusted_risk))
            
            # Risk amount in USD
            risk_amount = account_balance * (risk_percentage / 100)
            
            # Default stop percentages
            stop_percentage = 0.0
            if signal_type == "BUY":
                stop_percentage = risk_config.get('long_stop_percentage', 3.0)
            else:
                stop_percentage = risk_config.get('short_stop_percentage', 3.0)
                
            # Calculate stop loss price
            stop_loss = 0.0
            if signal_type == "BUY":
                stop_loss = price * (1 - stop_percentage / 100)
            else:
                stop_loss = price * (1 + stop_percentage / 100)
                
            # Calculate position size
            position_size = 0.0
            if abs(price - stop_loss) > 0:
                position_size = risk_amount / abs(price - stop_loss)
            
            # Calculate take profit based on risk:reward ratio
            risk_reward_ratio = risk_config.get('risk_reward_ratio', 2.0)
            
            # Higher confidence = higher RR potential
            adjusted_rr = risk_reward_ratio * (1 + (score - 50) / 100)
            
            # Calculate take profit price
            take_profit = 0.0
            if signal_type == "BUY":
                take_profit = price + (adjusted_rr * (price - stop_loss))
            else:
                take_profit = price - (adjusted_rr * (stop_loss - price))
                
            # Update trade params
            trade_params.update({
                'stop_loss': round(stop_loss, 8),
                'take_profit': round(take_profit, 8),
                'position_size': round(position_size, 8),
                'risk_reward_ratio': round(adjusted_rr, 2),
                'risk_percentage': round(risk_percentage, 2),
                'risk_amount': round(risk_amount, 2)
            })
            
            self.logger.debug(f"Calculated trade parameters for {symbol}: {trade_params}")
            return trade_params
            
        except Exception as e:
            self.logger.error(f"Error calculating trade parameters for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Return default parameters on error
            return {
                'entry_price': price,
                'stop_loss': None,
                'take_profit': None,
                'position_size': None,
                'risk_reward_ratio': None,
                'risk_percentage': None,
                'confidence': min(score / 100, 1.0),
                'timeframe': 'auto'
            }

    async def _monitor_volume(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor volume indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['volume_change']:
                await self._handle_volume_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring volume: {str(e)}")

    async def _monitor_orderflow(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor orderflow indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['orderflow_change']:
                await self._handle_orderflow_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring orderflow: {str(e)}")

    async def _monitor_orderbook(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor orderbook indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['orderbook_change']:
                await self._handle_orderbook_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring orderbook: {str(e)}")

    async def _monitor_position(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor position indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['position_change']:
                await self._handle_position_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring position: {str(e)}")

    async def _monitor_sentiment(self, symbol: str, current: Dict[str, Any], previous: Optional[float], market_data: Dict[str, Any]) -> None:
        """Monitor sentiment indicators for significant changes."""
        try:
            if previous is None:
                return
                
            change = abs(current['score'] - previous)
            if change > self.monitoring_thresholds['sentiment_change']:
                await self._handle_sentiment_alert(symbol, current, previous)
                
        except Exception as e:
            self.logger.error(f"Error monitoring sentiment: {str(e)}")

    # Add corresponding alert handlers
    async def _handle_momentum_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant momentum changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant momentum change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("MOMENTUM", message, level="INFO")

    async def _handle_volume_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant volume changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant volume change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("VOLUME", message, level="INFO")

    async def _handle_orderflow_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant orderflow changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant orderflow change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("ORDERFLOW", message, level="INFO")

    async def _handle_orderbook_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant orderbook changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant orderbook change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("ORDERBOOK", message, level="INFO")

    async def _handle_position_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant position changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant position change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("POSITION", message, level="INFO")

    async def _handle_sentiment_alert(self, symbol: str, current: Dict[str, Any], previous: float) -> None:
        """Handle significant sentiment changes."""
        prev_str = f"{previous:.2f}" if previous is not None else "N/A"
        current_str = f"{current['score']:.2f}" if current.get('score') is not None else "N/A"
        message = f"Significant sentiment change for {symbol}: {prev_str} -> {current_str}"
        await self.alert_manager.send_alert("SENTIMENT", message, level="INFO")

    async def _process_market_data(self, symbol: str, market_data: Dict[str, Any]) -> None:
        """Process market data for monitoring and analysis.
        
        This consolidated method handles all market data processing functionality,
        replacing multiple redundant implementations.
        
        Args:
            symbol: Trading pair symbol
            market_data: Market data dictionary
        """
        try:
            # Start performance tracking
            operation = self.metrics_manager.start_operation("process_market_data")
            
            # Log the operation
            self.logger.debug(f"\n=== Processing market data for {symbol} ===")
            
            # If symbol is not provided in the arguments, try to get it from the market data
            symbol = symbol or market_data.get('symbol', 'unknown')
            
            # Validate market data (now asynchronous)
            self.logger.debug("Calling validate_market_data asynchronously for {}".format(symbol))
            if not await self.validate_market_data(market_data):
                self.logger.error(f"Invalid market data for {symbol}")
                self.metrics_manager.end_operation(operation, success=False)
                return
            
            # Calculate metrics
            ticker = market_data.get('ticker', {})
            price = float(ticker.get('last', 0))
            change24h = float(ticker.get('change24h', 0))
            volume = float(ticker.get('volume', 0))
            funding_rate = float(ticker.get('fundingRate', 0))

            # Monitor significant price changes
            if abs(change24h) > 5.0:  # 5% threshold
                await self.alert_manager.send_alert(
                    "PRICE_CHANGE",
                    f"Significant price change for {symbol}: {change24h:+.2f}%",
                    level="INFO"
                )

            # Monitor funding rate extremes
            if abs(funding_rate) > 0.001:  # 0.1% threshold
                await self.alert_manager.send_alert(
                    "FUNDING_RATE",
                    f"High funding rate for {symbol}: {funding_rate:.4f}",
                    level="INFO"
                )

            # Monitor orderbook imbalances
            orderbook = market_data.get('orderbook', {})
            if orderbook:
                bids = orderbook.get('bids', [])
                asks = orderbook.get('asks', [])
                
                if bids and asks:
                    # Extract the quantities from the first 5 levels and convert to numpy array for efficient calculations
                    bid_quantities = np.array([float(bid[1]) for bid in bids[:5]])
                    ask_quantities = np.array([float(ask[1]) for ask in asks[:5]])
                    
                    bid_sum = np.sum(bid_quantities)
                    ask_sum = np.sum(ask_quantities)
                    
                    if bid_sum > 0 and ask_sum > 0:
                        imbalance = (bid_sum - ask_sum) / (bid_sum + ask_sum)
                        if abs(imbalance) > 0.2:  # 20% imbalance threshold
                            direction = "buy" if imbalance > 0 else "sell"
                            await self.alert_manager.send_alert(
                                "ORDERBOOK_IMBALANCE",
                                f"Large {direction} imbalance for {symbol}: {abs(imbalance):.1%}",
                                level="INFO"
                            )
            
            # Update indicators
            self._update_indicators(market_data)
            
            # Generate signals
            signals = self._generate_signals(market_data)
            
            # Process alerts
            self._process_alerts(signals)
            
            # Record metrics
            self.metrics_manager.record_metric(f"market_data_processed.{symbol}", 1)
            
            # End performance tracking
            self.metrics_manager.end_operation(operation)
            self.logger.debug(f"Successfully processed market data for {symbol}")
            
        except Exception as e:
            self.logger.error(f"Error processing market data for {symbol}: {str(e)}")
            self.logger.debug(traceback.format_exc())
            
            # Record error metric
            self.metrics_manager.record_metric(f"market_data_errors.{symbol}", 1)
            
            # End operation with failure if it was started
            if 'operation' in locals():
                self.metrics_manager.end_operation(operation, success=False)
    
    async def _generate_market_report(self) -> None:
        """Generate and send a market report for all monitored symbols."""
        try:
            # Create a unique transaction ID for tracking this report generation process
            transaction_id = str(uuid.uuid4())[:8]
            start_time = time.time()
            memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 if psutil else None
            
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Starting market report generation at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            if memory_before:
                self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Memory usage before report: {memory_before:.2f} MB")
            
            # Record steps and timings for process debugging
            steps_timing = {}
            
            # STEP 1: Validate prerequisites
            step_start = time.time()
            
            # Check if we have a market reporter
            if not hasattr(self, 'market_reporter') or self.market_reporter is None:
                self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: No market reporter available")
                return
            
            # Make sure we have an alert manager for sending reports
            if not hasattr(self, 'alert_manager') or self.alert_manager is None:
                self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: No alert manager available")
                return
            
            # Log reporter configuration
            reporter_enabled = getattr(self.market_reporter, 'enabled', True)
            reporter_pdf_enabled = getattr(self.market_reporter, 'pdf_enabled', True)
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Reporter enabled: {reporter_enabled}, PDF enabled: {reporter_pdf_enabled}")
                
            # Get list of symbols to report on
            symbols = self.get_monitored_symbols()
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Generating report for {len(symbols)} symbols")
            
            # Check if we have at least one symbol
            if not symbols:
                self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] No symbols to report on, skipping report")
                return
            
            # Record step timing    
            steps_timing['prerequisites'] = time.time() - step_start
            
            # STEP 2: Data Collection
            step_start = time.time()
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Starting data collection phase")
            
            # Generate the report data
            report_data = {}
            successful_symbols = 0
            failed_symbols = 0
            
            for idx, symbol in enumerate(symbols):
                symbol_start = time.time()
                self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Symbol {idx+1}/{len(symbols)}: Processing {symbol}")
                
                try:
                    # Get latest data for this symbol
                    ohlcv_data = self.get_ohlcv_for_report(symbol)
                    
                    if ohlcv_data is None or (hasattr(ohlcv_data, 'empty') and ohlcv_data.empty):
                        self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] No OHLCV data for {symbol}, skipping")
                        failed_symbols += 1
                        continue
                        
                    # Check data quality
                    if hasattr(ohlcv_data, 'shape'):
                        rows, cols = ohlcv_data.shape
                        self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Symbol {symbol}: OHLCV data shape: {rows} rows, {cols} columns")
                        
                        # Check for sufficient data points (at least 50 candles is usually good)
                        if rows < 50:
                            self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Symbol {symbol}: Only {rows} OHLCV data points, may be insufficient")
                    
                    report_data[symbol] = {
                        'ohlcv': ohlcv_data,
                        'metadata': {
                            'timestamp': datetime.utcnow().isoformat(),
                            'exchange': self.exchange_id,
                            'collection_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'cache_age': (time.time() - self._last_ohlcv_update.get(symbol, 0)) if hasattr(self, '_last_ohlcv_update') else None,
                        }
                    }
                    
                    successful_symbols += 1
                    symbol_time = time.time() - symbol_start
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Symbol {symbol} processed in {symbol_time:.2f}s")
                    
                except Exception as e:
                    self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: Failed to process {symbol} for report: {str(e)}")
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Exception type: {type(e).__name__}")
                    self.logger.debug(traceback.format_exc())
                    failed_symbols += 1
                    continue
                    
            # Record step timing and stats
            steps_timing['data_collection'] = time.time() - step_start
            data_collection_stats = {
                'total_symbols': len(symbols),
                'successful_symbols': successful_symbols,
                'failed_symbols': failed_symbols,
                'success_rate': f"{(successful_symbols / len(symbols) * 100):.1f}%" if len(symbols) > 0 else "N/A"
            }
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Data collection phase complete: {json.dumps(data_collection_stats)}")
                    
            # Check if we have any data
            if not report_data:
                self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] No data collected for any symbol, skipping report")
                return
                
            # STEP 3: Report Generation
            step_start = time.time()
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Starting report generation phase")
            
            # Generate the report
            report = None
            try:
                market_reporter_memory_before = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 if psutil else None
                if market_reporter_memory_before:
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Memory before market_reporter.generate_report: {market_reporter_memory_before:.2f} MB")
                
                generation_start = time.time()
                self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Calling market_reporter.generate_report with {len(report_data)} symbols")
                
                # Log methods available on market_reporter
                if hasattr(self.market_reporter, 'generate_report'):
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] market_reporter.generate_report method found")
                else:
                    self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: market_reporter.generate_report method not found!")
                    available_methods = [method for method in dir(self.market_reporter) if callable(getattr(self.market_reporter, method)) and not method.startswith('_')]
                    self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Available methods: {available_methods}")
                    return
                
                # Call generate_report with timing
                report = await self.market_reporter.generate_report(report_data)
                
                generation_time = time.time() - generation_start
                market_reporter_memory_after = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 if psutil else None
                
                # Log memory usage
                if market_reporter_memory_before and market_reporter_memory_after:
                    memory_diff = market_reporter_memory_after - market_reporter_memory_before
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Memory after report generation: {market_reporter_memory_after:.2f} MB (change: {memory_diff:+.2f} MB)")
                
                self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Report generated successfully in {generation_time:.2f}s")
                
                # Validate report structure
                if not isinstance(report, dict):
                    self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: Generated report is not a dictionary: {type(report)}")
                    return
                
                # Check expected keys in report
                expected_keys = ['timestamp', 'summary', 'market_overview']
                missing_keys = [key for key in expected_keys if key not in report]
                if missing_keys:
                    self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] WARNING: Report missing expected keys: {missing_keys}")
                
                # Log report content summary
                report_keys = list(report.keys())
                self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Report keys: {report_keys}")
                
                # Check for HTML content - critical for PDF generation
                if 'html' not in report:
                    self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] WARNING: Report missing HTML content, PDF generation may fail")
                elif not report['html']:
                    self.logger.warning(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] WARNING: Empty HTML content, PDF generation may fail")
                else:
                    html_length = len(report['html'])
                    self.logger.debug(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] HTML content found: {html_length} characters")
                
            except Exception as e:
                self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: Failed to generate report: {str(e)}")
                self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Exception type: {type(e).__name__}")
                self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Error traceback: {traceback.format_exc()}")
                return
            
            # Record step timing
            steps_timing['report_generation'] = time.time() - step_start

            # STEP 4: PDF Generation
            step_start = time.time()
            pdf_path = None
            
            try:
                # Check if PDF generation is enabled in the market reporter
                if hasattr(self.market_reporter, 'pdf_enabled') and self.market_reporter.pdf_enabled:
                    self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] PDF generation is enabled")
                    
                    # Generate timestamp for filename
                    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
                    pdf_filename = f"market_report_{timestamp_str}.pdf"
                    pdf_output_dir = os.path.join("reports", "pdf")
                    
                    # Ensure output directory exists
                    os.makedirs(pdf_output_dir, exist_ok=True)
                    self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Output directory: {pdf_output_dir}")
                    
                    # Full path for PDF
                    pdf_path = os.path.join(pdf_output_dir, pdf_filename)
                    self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Generated PDF path: {pdf_path}")
                    
                    # Log available HTML content details
                    if 'html' not in report:
                        self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: No 'html' key in report dictionary")
                        self.logger.debug(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Report keys: {list(report.keys())}")
                        pdf_path = None
                    elif not report['html']:
                        self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: Report has empty HTML content")
                        pdf_path = None
                    else:
                        html_content = report['html']
                        html_length = len(html_content)
                        self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] HTML content length: {html_length} characters")
                        
                        # Log HTML content preview for debugging (truncated)
                        max_preview_length = 500  # Show first 500 chars
                        html_preview = html_content[:max_preview_length] + "..." if html_length > max_preview_length else html_content
                        self.logger.debug(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] HTML preview: {html_preview}")
                        
                        # Check if critical HTML elements are present (basic content validation)
                        if "<html" not in html_content.lower() or "<body" not in html_content.lower():
                            self.logger.warning(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] WARNING: HTML content may be malformed (missing html or body tags)")
                        
                        # Generate PDF from HTML
                        if hasattr(self.market_reporter, 'html_to_pdf'):
                            pdf_generation_start = time.time()
                            self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Starting PDF generation")
                            
                            # Check for PDF generation parameters
                            pdf_params = getattr(self.market_reporter, 'pdf_params', None)
                            if pdf_params:
                                self.logger.debug(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Using PDF parameters: {pdf_params}")
                            
                            # Generate the PDF
                            pdf_result = await self.market_reporter.html_to_pdf(
                                html_content=report['html'],
                                output_path=pdf_path
                            )
                            
                            pdf_generation_time = time.time() - pdf_generation_start
                            self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] PDF generation attempt completed in {pdf_generation_time:.2f}s")
                            
                            # Check PDF generation success
                            if pdf_result and os.path.exists(pdf_path):
                                file_size_bytes = os.path.getsize(pdf_path)
                                file_size_kb = file_size_bytes / 1024  
                                self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] PDF generated successfully: {pdf_path} ({file_size_kb:.2f} KB)")
                                
                                # Check if PDF file is suspiciously small (could indicate problems)
                                if file_size_bytes < 10000:  # Less than 10KB
                                    self.logger.warning(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] WARNING: PDF file is suspiciously small: {file_size_bytes} bytes")
                                    
                                    # Try to check if PDF is valid
                                    try:
                                        with open(pdf_path, 'rb') as f:
                                            header = f.read(10)
                                            if not header.startswith(b'%PDF-'):
                                                self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: Generated file is not a valid PDF (missing PDF header)")
                                    except Exception as e:
                                        self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: Could not validate PDF file: {str(e)}")
                            else:
                                self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: PDF generation failed or file not found at {pdf_path}")
                                # Check possible failure causes
                                if pdf_result is False:
                                    self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Method returned False, indicating failure")
                                if not os.path.exists(pdf_path):
                                    # Check if directory exists and is writable
                                    pdf_dir = os.path.dirname(pdf_path)
                                    if not os.path.exists(pdf_dir):
                                        self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Output directory does not exist: {pdf_dir}")
                                    elif not os.access(pdf_dir, os.W_OK):
                                        self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] No write permission to output directory: {pdf_dir}")
                                    self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Output file was not created: {pdf_path}")
                                pdf_path = None
                        else:
                            self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: market_reporter.html_to_pdf method not found")
                            # List methods available on market_reporter
                            available_methods = [method for method in dir(self.market_reporter) if callable(getattr(self.market_reporter, method)) and not method.startswith('_')]
                            self.logger.debug(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Available methods: {available_methods}")
                            pdf_path = None
                else:
                    self.logger.info(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] PDF generation is disabled")
            except Exception as e:
                self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] ERROR: Exception during PDF generation: {str(e)}")
                self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Exception type: {type(e).__name__}")
                self.logger.error(f"[DIAGNOSTICS] [PDF_GENERATION] [TXN:{transaction_id}] Error traceback: {traceback.format_exc()}")
                pdf_path = None
            
            # Record step timing
            steps_timing['pdf_generation'] = time.time() - step_start
                
            # STEP 5: Report Delivery
            step_start = time.time()
            self.logger.info(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Starting report delivery phase")
                
            # Send the report via Discord
            try:
                # Check if alert manager is properly initialized
                if not self.alert_manager:
                    self.logger.error(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] ERROR: Alert manager not available")
                    return
                
                # Check if Discord webhook is available
                if not hasattr(self.alert_manager, 'discord_webhook_url') or not self.alert_manager.discord_webhook_url:
                    self.logger.error(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] ERROR: Discord webhook URL not set in alert manager")
                    return
                
                # Get market state from report for more descriptive message
                market_state = "Unknown"
                if report and isinstance(report, dict) and 'market_overview' in report:
                    market_state = report['market_overview'].get('regime', 'Unknown')
                
                # Prepare message content with market state
                market_emoji = "" if market_state == "BEARISH" else "" if market_state == "BULLISH" else ""
                message = {
                    'content': f"{market_emoji} Market Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Market state: {market_state}",
                    'username': 'Market Reporter'
                }
                
                # Add file attachments if we have a PDF
                files = None
                if pdf_path and os.path.exists(pdf_path):
                    self.logger.info(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Adding PDF attachment: {pdf_path}")
                    files = [{
                        'path': pdf_path,
                        'filename': os.path.basename(pdf_path),
                        'description': 'Market Report PDF'
                    }]
                else:
                    self.logger.warning(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] No valid PDF to attach")
                
                # Send the message with webhook
                delivery_start = time.time()
                self.logger.info(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Sending webhook message with report")
                
                await self.alert_manager.send_discord_webhook_message(message, files=files)
                
                delivery_time = time.time() - delivery_start
                self.logger.info(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Report sent successfully in {delivery_time:.2f}s")
                
            except Exception as e:
                self.logger.error(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] ERROR: Failed to send report: {str(e)}")
                self.logger.error(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Exception type: {type(e).__name__}")
                self.logger.error(f"[DIAGNOSTICS] [REPORT_DELIVERY] [TXN:{transaction_id}] Send error traceback: {traceback.format_exc()}")
            
            # Record step timing
            steps_timing['report_delivery'] = time.time() - step_start
            
            # STEP 6: Cleanup and Summary
            step_start = time.time()
            
            # Get final memory usage
            memory_after = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 if psutil else None
            
            # Calculate total processing time
            total_time = time.time() - start_time
            
            # Log timing and memory summary
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Market report generation completed in {total_time:.2f} seconds")
            
            if memory_before and memory_after:
                memory_diff = memory_after - memory_before
                self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Memory usage: Before {memory_before:.2f} MB, After {memory_after:.2f} MB, Diff: {memory_diff:+.2f} MB")
            
            # Log time breakdown for each step
            timing_summary = []
            for step, duration in steps_timing.items():
                percent = (duration / total_time) * 100
                timing_summary.append(f"{step}: {duration:.2f}s ({percent:.1f}%)")
            
            self.logger.info(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Time breakdown: {', '.join(timing_summary)}")
            
            # Force garbage collection to clean up memory
            gc.collect()
            
            # Record step timing
            steps_timing['cleanup'] = time.time() - step_start
                
        except Exception as e:
            self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] ERROR: Exception in _generate_market_report: {str(e)}")
            self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Exception type: {type(e).__name__}")
            self.logger.error(f"[DIAGNOSTICS] [MARKET_REPORT] [TXN:{transaction_id}] Error traceback: {traceback.format_exc()}")
