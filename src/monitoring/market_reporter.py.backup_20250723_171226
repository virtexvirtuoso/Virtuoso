import pandas as pd
import re
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import math
import time
import random
import asyncio
from datetime import datetime, timedelta
import logging
import sys
import os
import traceback
import gc
import json
from collections import defaultdict
from cachetools import TTLCache
import shutil
import functools

# Import for PDF report generation
ReportManager = None
ReportGenerator = None

# Import Bitcoin Beta Analysis
try:
    from src.reports.bitcoin_beta_report import BitcoinBetaReport
    from src.reports.bitcoin_beta_alpha_detector import BitcoinBetaAlphaDetector, AlphaOpportunity
    BITCOIN_BETA_AVAILABLE = True
except ImportError:
    BitcoinBetaReport = None
    BitcoinBetaAlphaDetector = None
    AlphaOpportunity = None
    BITCOIN_BETA_AVAILABLE = False
    logging.getLogger(__name__).warning("Bitcoin Beta Analysis modules not available")

# Enhanced PDF imports with fallbacks
PDF_CLASSES_AVAILABLE = False
try:
    from src.core.reporting.report_manager import ReportManager
    from src.core.reporting.pdf_generator import ReportGenerator
    PDF_CLASSES_AVAILABLE = True
except ImportError:
    try:
        # Try relative imports
        from ..core.reporting.report_manager import ReportManager
        from ..core.reporting.pdf_generator import ReportGenerator
        PDF_CLASSES_AVAILABLE = True
    except (ImportError, ValueError):
        try:
            # Try absolute imports without src prefix
            from core.reporting.report_manager import ReportManager
            from core.reporting.pdf_generator import ReportGenerator
            PDF_CLASSES_AVAILABLE = True
        except ImportError:
            # Create basic PDF functionality stubs for minimal functionality
            class BasicReportManager:
                def __init__(self):
                    self.pdf_generator = None
                    
            class BasicPDFGenerator:
                def __init__(self, template_dir=None):
                    self.template_dir = template_dir
                    
                async def generate_pdf_report(self, data, template_name="market_report_dark.html"):
                    # Basic PDF generation using available libraries
                    try:
                        import matplotlib.pyplot as plt
                        from matplotlib.backends.backend_pdf import PdfPages
                        import io
                        
                        # Create a simple PDF with key metrics
                        filename = f"market_report_{int(time.time())}.pdf"
                        filepath = os.path.join("exports", filename)
                        
                        with PdfPages(filepath) as pdf:
                            fig, ax = plt.subplots(figsize=(8, 10))
                            ax.text(0.1, 0.9, "Virtuoso Market Report", fontsize=16, weight='bold')
                            ax.text(0.1, 0.8, f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", fontsize=12)
                            
                            # Add market overview if available
                            y_pos = 0.7
                            if 'market_overview' in data:
                                overview = data['market_overview']
                                ax.text(0.1, y_pos, "Market Overview:", fontsize=14, weight='bold')
                                y_pos -= 0.05
                                for key, value in overview.items():
                                    if isinstance(value, (int, float, str)) and len(str(value)) < 50:
                                        ax.text(0.1, y_pos, f"{key}: {value}", fontsize=10)
                                        y_pos -= 0.03
                                        if y_pos < 0.1:
                                            break
                            
                            ax.set_xlim(0, 1)
                            ax.set_ylim(0, 1)
                            ax.axis('off')
                            pdf.savefig(fig, bbox_inches='tight')
                            plt.close(fig)
                        
                        return filepath
                    except Exception as e:
                        logging.getLogger(__name__).error(f"Basic PDF generation failed: {e}")
                        return None
            
            ReportManager = BasicReportManager
            ReportGenerator = BasicPDFGenerator
            PDF_CLASSES_AVAILABLE = True
            logging.getLogger(__name__).info("Using basic PDF generation fallback")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/market_reporter.log')
    ]
)
logger = logging.getLogger(__name__)

class MarketReporter:
    """Market reporter class for generating comprehensive market analysis."""
    
    def __init__(self, exchange: Any = None, logger: Optional[logging.Logger] = None, top_symbols_manager: Any = None, alert_manager: Any = None):
        """Initialize market reporter with exchange connection and optional managers."""
        self.exchange = exchange
        self.logger = logger or logging.getLogger(__name__)
        self.top_symbols_manager = top_symbols_manager
        self.alert_manager = alert_manager
        
        # Add Bybit-specific field mappings 
        self.BYBIT_FIELD_MAPPINGS = {
            'mark_price': ['markPrice', 'mark_price'],
            'index_price': ['indexPrice', 'index_price'],
            'funding_rate': ['fundingRate', 'funding_rate'],
            'open_interest': ['openInterest', 'open_interest', 'oi'],
            'turnover': ['turnover24h', 'turnover', 'volume24hValue'],
            'volume': ['volume', 'volume24h']
        }
        
        # Initialize all required monitoring attributes to prevent AttributeError
        self.api_latencies = []
        self.error_counts = defaultdict(int)
        self.last_error_time = time.time()
        self.data_quality_scores = []
        self.processing_times = []
        self.request_counts = defaultdict(int)
        self.last_reset_time = time.time()
        
        # Production monitoring thresholds
        self.latency_threshold = 2.0  # seconds
        self.error_rate_threshold = 10  # errors per minute
        self.quality_score_threshold = 90
        self.stale_data_threshold = 60  # seconds
        
        # Alert thresholds
        self.whale_alert_threshold = 1000000  # $1M for whale trades
        self.premium_alert_threshold = 0.5  # 0.5% for significant futures premium
        self.volatility_alert_threshold = 5.0  # 5% for high volatility
        
        # Add caching mechanism with memory management
        self.cache = TTLCache(maxsize=500, ttl=300)  # Reduced from 1000 to 500 for memory optimization
        self.ticker_cache = TTLCache(maxsize=50, ttl=30)  # Reduced TTL from 60 to 30 seconds for more frequent updates
        
        # Memory optimization settings
        self.memory_optimization_enabled = True
        self.gc_collection_interval = 10  # Collect garbage every 10 operations
        self.operation_count = 0
        self.max_data_retention = 100  # Limit data retention for memory
        
        # Optimize data structures for memory efficiency
        self.api_latencies = []  # Will be truncated to last 50 entries
        self.data_quality_scores = []  # Will be truncated to last 20 entries
        self.processing_times = []  # Will be truncated to last 20 entries
        
        # Initialize PDF generator and report manager
        try:
            # Check if PDF classes are available
            if not PDF_CLASSES_AVAILABLE or ReportManager is None or ReportGenerator is None:
                self.logger.warning("PDF reporting classes not available - PDF generation disabled")
                self.pdf_enabled = False
                self.pdf_generator = None
                self.report_manager = None
            else:
                # Enhanced PDF initialization with better error handling
                template_dir = None
                config_file = os.path.join(os.getcwd(), "config", "templates_config.json")
                
                if os.path.exists(config_file):
                    try:
                        with open(config_file, 'r') as f:
                            config_data = json.load(f)
                            if 'template_directory' in config_data:
                                template_dir = config_data['template_directory']
                                self.logger.info(f"Using template directory from config: {template_dir}")
                    except Exception as e:
                        self.logger.warning(f"Error loading template config: {str(e)}")
                
                # Enhanced template directory search
                if not template_dir or not os.path.exists(template_dir):
                    search_paths = [
                        os.path.join(os.getcwd(), "src/core/reporting/templates"),
                        os.path.join(os.getcwd(), "templates"),
                        os.path.join(os.getcwd(), "src/templates"),
                        os.path.join(os.path.dirname(__file__), "templates"),
                        "templates"
                    ]
                    
                    for path in search_paths:
                        if os.path.exists(path):
                            template_dir = path
                            break
                
                # Initialize with or without template directory
                if template_dir and os.path.exists(template_dir):
                    self.logger.info(f"Initializing PDF generator with template directory: {template_dir}")
                    self.pdf_generator = ReportGenerator(template_dir=template_dir)
                else:
                    self.logger.info("Initializing PDF generator without specific template directory")
                    self.pdf_generator = ReportGenerator()
                
                self.report_manager = ReportManager()
                self.report_manager.pdf_generator = self.pdf_generator
                
                # Set default template
                self.default_template = "market_report_dark.html"
                self.logger.info(f"Using template: {self.default_template}")
                
                # Enable PDF generation
                self.pdf_enabled = True
                self.logger.info("PDF generation enabled successfully")
                
        except Exception as e:
            self.logger.error(f"Failed to initialize PDF generator: {str(e)}")
            self.pdf_enabled = False
            self.pdf_generator = None
            self.report_manager = None
        
        # Initialize Bitcoin Beta Analysis
        try:
            # Enhanced debugging for Bitcoin Beta Analysis initialization
            self.logger.debug(f"Bitcoin Beta Analysis initialization check:")
            self.logger.debug(f"  BITCOIN_BETA_AVAILABLE: {BITCOIN_BETA_AVAILABLE}")
            self.logger.debug(f"  exchange provided: {exchange is not None}")
            self.logger.debug(f"  exchange type: {type(exchange).__name__ if exchange else 'None'}")
            self.logger.debug(f"  exchange id: {getattr(exchange, 'exchange_id', 'N/A') if exchange else 'N/A'}")
            self.logger.debug(f"  top_symbols_manager provided: {top_symbols_manager is not None}")
            self.logger.debug(f"  top_symbols_manager type: {type(top_symbols_manager).__name__ if top_symbols_manager else 'None'}")
            
            if BITCOIN_BETA_AVAILABLE and exchange and top_symbols_manager:
                # Create a mock config for Bitcoin Beta if needed
                beta_config = getattr(self, 'config', {})
                self.beta_report = BitcoinBetaReport(
                    exchange_manager=exchange,
                    top_symbols_manager=top_symbols_manager,
                    config=beta_config
                )
                self.beta_enabled = True
                self.logger.info("Bitcoin Beta Analysis initialized successfully")
            else:
                self.beta_report = None
                self.beta_enabled = False
                if not BITCOIN_BETA_AVAILABLE:
                    self.logger.warning("Bitcoin Beta Analysis modules not available")
                elif not exchange:
                    self.logger.warning("Bitcoin Beta Analysis disabled - missing exchange")
                elif not top_symbols_manager:
                    self.logger.warning("Bitcoin Beta Analysis disabled - missing top_symbols_manager")
                else:
                    self.logger.warning("Bitcoin Beta Analysis disabled - unknown reason")
        except Exception as e:
            self.logger.error(f"Failed to initialize Bitcoin Beta Analysis: {str(e)}")
            import traceback
            self.logger.debug(traceback.format_exc())
            self.beta_report = None
            self.beta_enabled = False
        
        # Initialize symbols with correct Bybit format (not CCXT format)
        # Bybit expects symbols like 'BTCUSDT', not 'BTC/USDT:USDT'
        self.symbols = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'XRPUSDT']
        
        # Store both formats for conversion if needed
        self.symbol_mapping = {
            'BTCUSDT': 'BTC/USDT:USDT',
            'ETHUSDT': 'ETH/USDT:USDT', 
            'SOLUSDT': 'SOL/USDT:USDT',
            'XRPUSDT': 'XRP/USDT:USDT',
            'ADAUSDT': 'ADA/USDT:USDT',
            'DOGEUSDT': 'DOGE/USDT:USDT'
        }
        
        # Whitelist of symbols that have INVERSE contracts (coin-margined perpetuals/futures)
        # Based on API discovery from https://bybit-exchange.github.io/docs/v5/market/instrument
        # Last updated: 2025-06-14 via Bybit API discovery
        self.derivatives_enabled_symbols = {
            # Base coins that have inverse contracts
            'AAVE', 'ADA', 'APT', 'AVAX', 'BCH', 'BTC', 'DOGE', 'DOT', 'ETC', 'ETH', 
            'FIL', 'LINK', 'LTC', 'MANA', 'MNT', 'NEAR', 'OP', 'SOL', 'SUI', 'TON', 
            'UNI', 'WIF', 'XLM', 'XRP',
            # USDT format variations
            'AAVEUSDT', 'ADAUSDT', 'APTUSDT', 'AVAXUSDT', 'BCHUSDT', 'BTCUSDT', 'DOGEUSDT', 
            'DOTUSDT', 'ETCUSDT', 'ETHUSDT', 'FILUSDT', 'LINKUSDT', 'LTCUSDT', 'MANAUSDT', 
            'MNTUSDT', 'NEARUSDT', 'OPUSDT', 'SOLUSDT', 'SUIUSDT', 'TONUSDT', 'UNIUSDT', 
            'WIFUSDT', 'XLMUSDT', 'XRPUSDT',
            # CCXT format variations
            'AAVE/USDT:USDT', 'ADA/USDT:USDT', 'APT/USDT:USDT', 'AVAX/USDT:USDT', 'BCH/USDT:USDT', 
            'BTC/USDT:USDT', 'DOGE/USDT:USDT', 'DOT/USDT:USDT', 'ETC/USDT:USDT', 'ETH/USDT:USDT', 
            'FIL/USDT:USDT', 'LINK/USDT:USDT', 'LTC/USDT:USDT', 'MANA/USDT:USDT', 'MNT/USDT:USDT', 
            'NEAR/USDT:USDT', 'OP/USDT:USDT', 'SOL/USDT:USDT', 'SUI/USDT:USDT', 'TON/USDT:USDT', 
            'UNI/USDT:USDT', 'WIF/USDT:USDT', 'XLM/USDT:USDT', 'XRP/USDT:USDT'
        }
        
        # Note: Bybit has 30 inverse contracts vs 515+ linear contracts
        # This prevents unnecessary API calls for 485+ symbols that don't have inverse derivatives
        
        self.timeframes = {
            '1h': 24,    # Last 24 hours
            '4h': 42,    # Last 7 days (42 4-hour periods)
            '1d': 30     # Last 30 days
        }
        
        self.logger.info("MarketReporter initialized with monitoring metrics and caching")
        
        # Initialize scheduled report times
        self.report_times = [
            "00:00",  # Daily reset
            "08:00",  # Asian session
            "16:00",  # European session
            "20:00"   # US session
        ]
        self.logger.info(f"Market reports scheduled for: {', '.join(self.report_times)} UTC")
    
    def _has_derivatives_market(self, symbol: str) -> bool:
        """Check if a symbol has INVERSE contracts available (coin-margined perpetuals/futures)."""
        # Clean up the symbol format for checking
        clean_symbol = symbol.replace('/', '')
        base_asset = clean_symbol.replace('USDT', '').replace(':USDT', '')
        
        # Check various symbol formats
        symbol_variants = [symbol, clean_symbol, base_asset]
        return any(variant in self.derivatives_enabled_symbols for variant in symbol_variants)

    def _extract_market_data(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract and normalize market data from various sources in the data structure.
        
        This handles different data formats and ensures consistent access to key metrics.
        """
        result = {
            # Default values
            'price': 0.0,
            'change_24h': 0.0,
            'volume': 0.0,
            'turnover': 0.0,
            'open_interest': 0.0,
            'high': 0.0,
            'low': 0.0,
            'timestamp': int(time.time() * 1000)
        }
        
        if not market_data:
            return result
            
        # Extract from price structure (the most direct source)
        if 'price' in market_data and isinstance(market_data['price'], dict):
            price_data = market_data['price']
            result['price'] = float(price_data.get('last', 0.0))
            result['change_24h'] = float(price_data.get('change_24h', 0.0))
            result['volume'] = float(price_data.get('volume', 0.0))
            result['turnover'] = float(price_data.get('turnover', 0.0))
            result['high'] = float(price_data.get('high', 0.0))
            result['low'] = float(price_data.get('low', 0.0))
        
        # Extract from ticker (fallback)
        ticker = market_data.get('ticker', {})
        if not result['price'] and ticker:
            result['price'] = float(ticker.get('last', 0.0))
            result['change_24h'] = float(ticker.get('change24h', ticker.get('change', 0.0)))
            # Use CCXT standardized field names for volume and turnover  
            result['volume'] = float(ticker.get('baseVolume', 0.0))  # Changed from 'volume' to 'baseVolume'
            result['turnover'] = float(ticker.get('quoteVolume', 0.0))  # Changed from turnover24h to quoteVolume
            result['high'] = float(ticker.get('high', 0.0))
            result['low'] = float(ticker.get('low', 0.0))
            
        # Extract open interest data (can be in different places)
        if 'open_interest' in market_data and isinstance(market_data['open_interest'], dict):
            oi_data = market_data['open_interest']
            result['open_interest'] = float(oi_data.get('current', 0.0))
        elif ticker and 'openInterest' in ticker:
            result['open_interest'] = float(ticker.get('openInterest', 0.0))
        
        # Extract timestamp
        result['timestamp'] = market_data.get('timestamp', int(time.time() * 1000))
        
        # Extract price data with proper fallbacks
        if 'ticker' in market_data and market_data['ticker']:
            ticker = market_data['ticker']
            result['last_price'] = float(ticker.get('last', 0.0))
            result['high_24h'] = float(ticker.get('high', 0.0))
            result['low_24h'] = float(ticker.get('low', 0.0))
            result['price_change_24h'] = float(ticker.get('percentage', 0.0))
            # Use CCXT standardized field names for volume and turnover
            result['volume'] = float(ticker.get('baseVolume', 0.0))  # Changed from 'volume' to 'baseVolume'
            result['turnover'] = float(ticker.get('quoteVolume', 0.0))  # Changed from turnover24h to quoteVolume
        elif 'price_data' in market_data and market_data['price_data']:
            price_data = market_data['price_data']
            result['last_price'] = float(price_data.get('last', 0.0))
            result['high_24h'] = float(price_data.get('high', 0.0))
            result['low_24h'] = float(price_data.get('low', 0.0))
            result['price_change_24h'] = float(price_data.get('change', 0.0))
            result['volume'] = float(price_data.get('volume', 0.0))
            result['turnover'] = float(price_data.get('turnover', 0.0))
        
        return result

    async def _get_performance_data(self, top_pairs: List[str]) -> tuple:
        """Get performance data with proper extraction."""
        winners = []
        losers = []
        
        for symbol in top_pairs:
            # Get data from top_symbols_manager if available, otherwise try direct exchange access
            if self.top_symbols_manager:
                data = await self.top_symbols_manager.get_market_data(symbol)
            elif self.exchange:
                try:
                    # Use the retry mechanism that handles both sync and async methods
                    ticker = await self._fetch_with_retry('fetch_ticker', symbol, timeout=5)
                    data = {'ticker': ticker}
                except Exception as e:
                    self.logger.warning(f"Error fetching ticker for {symbol}: {str(e)}")
                    continue
            else:
                continue
                
            if not data:
                continue
                
            # Use our helper to extract market data properly
            market_metrics = self._extract_market_data(data)
            
            # Get key metrics
            price = market_metrics['price']
            change = market_metrics['change_24h']
            turnover = market_metrics['turnover']
            oi = market_metrics['open_interest']
            
            # Format entry
            entry = f"{symbol} {change:+.2f}% | Vol: {self._format_number(turnover)} | OI: {self._format_number(oi)}"
            
            if change > 0:
                winners.append((change, entry, symbol, price))
            else:
                losers.append((change, entry, symbol, price))
        
        # Sort entries
        winners.sort(reverse=True)
        losers.sort()
        
        return winners, losers
    
    async def update_symbols(self):
        """Update trading symbols from top symbols manager if available."""
        try:
            if self.top_symbols_manager:
                top_pairs = await self.top_symbols_manager.get_top_traded_symbols()
                if top_pairs and len(top_pairs) > 0:
                    self.symbols = top_pairs
                    self.logger.info(f"Updated symbols list from manager: {len(self.symbols)} pairs")
                else:
                    self.logger.warning("Top symbols manager returned empty symbols list, using default symbols")
                    # Keep existing default symbols if manager returns empty
                    if not hasattr(self, 'symbols') or not self.symbols:
                        self.symbols = ['BTC/USDT:USDT', 'ETH/USDT:USDT', 'SOL/USDT:USDT', 'XRP/USDT:USDT']
            else:
                self.logger.warning("No symbols received from top symbols manager")
                # Ensure we have default symbols if no manager
                if not hasattr(self, 'symbols') or not self.symbols:
                    self.symbols = ['BTC/USDT:USDT', 'ETH/USDT:USDT', 'SOL/USDT:USDT', 'XRP/USDT:USDT']
                    self.logger.info(f"Using default symbols: {self.symbols}")
        except Exception as e:
            self.logger.error(f"Error updating symbols: {str(e)}")
            self._log_error('symbol_update', str(e))
            # Ensure we have fallback symbols in case of error
            if not hasattr(self, 'symbols') or not self.symbols:
                self.symbols = ['BTC/USDT:USDT', 'ETH/USDT:USDT', 'SOL/USDT:USDT', 'XRP/USDT:USDT']
                self.logger.info(f"Using fallback symbols due to error: {self.symbols}")
            
    async def _fetch_with_cache(self, method_name: str, *args, **kwargs):
        """Generic caching wrapper for API calls"""
        cache_key = f"{method_name}:{str(args)}:{str(kwargs)}"
        
        if cache_key in self.cache:
            self.logger.debug(f"Cache hit for {method_name}")
            return self.cache[cache_key]
            
        # Get the method from the exchange object
        method = getattr(self.exchange, method_name)
        
        # Check if method is async and handle accordingly
        if asyncio.iscoroutinefunction(method):
            # Call async method
            result = await method(*args, **kwargs)
        else:
            # Call sync method
            result = method(*args, **kwargs)
        
        # Cache the result
        self.cache[cache_key] = result
        return result
    
    async def _fetch_with_retry(self, method_name: str, *args, max_retries=3, timeout=30, **kwargs):
        """Fetch with retry for reliability - handles both sync and async methods with circuit breaker"""
        retries = 0
        last_error = None
        
        # Circuit breaker: Check if this endpoint has too many recent failures
        circuit_key = f"{method_name}:{args[0] if args else 'global'}"
        current_time = time.time()
        
        # Initialize circuit breaker state if not exists
        if not hasattr(self, '_circuit_breaker'):
            self._circuit_breaker = {}
            
        if circuit_key not in self._circuit_breaker:
            self._circuit_breaker[circuit_key] = {'failures': 0, 'last_failure': 0, 'blocked_until': 0}
        
        circuit_state = self._circuit_breaker[circuit_key]
        
        # Check if circuit is open (too many failures recently)
        if current_time < circuit_state['blocked_until']:
            self.logger.warning(f"Circuit breaker OPEN for {circuit_key}, skipping request")
            raise RuntimeError(f"Circuit breaker open for {method_name}")
            
        # Auto-correct symbol format if this is a symbol-based method
        corrected_args = list(args)
        if args and method_name in ['fetch_ticker', 'fetch_order_book', 'fetch_trades', 'fetch_ohlcv']:
            symbol = args[0]
            corrected_symbol = self._convert_symbol_format(symbol)
            if corrected_symbol != symbol:
                self.logger.info(f"Auto-corrected symbol format: {symbol} → {corrected_symbol}")
                corrected_args[0] = corrected_symbol
        
        while retries < max_retries:
            try:
                # Get the method from the exchange object
                if not hasattr(self.exchange, method_name):
                    raise AttributeError(f"Exchange does not have method '{method_name}'")
                    
                method = getattr(self.exchange, method_name)
                
                # Determine if method is async
                if asyncio.iscoroutinefunction(method):
                    # Async method - use timeout and cache
                    result = await asyncio.wait_for(
                        self._fetch_with_cache(method_name, *corrected_args, **kwargs),
                        timeout=timeout
                    )
                    
                    # Reset circuit breaker on success
                    circuit_state['failures'] = 0
                    return result
                else:
                    # Sync method - run in executor to avoid blocking
                    loop = asyncio.get_event_loop()
                    
                    # Create a partial function with the arguments
                    import functools
                    func_with_args = functools.partial(method, *corrected_args, **kwargs)
                    
                    # Run sync method in thread pool with timeout
                    result = await asyncio.wait_for(
                        loop.run_in_executor(None, func_with_args),
                        timeout=timeout
                    )
                    
                    # Cache the result
                    cache_key = f"{method_name}:{str(corrected_args)}:{str(kwargs)}"
                    self.cache[cache_key] = result
                    
                    # Reset circuit breaker on success
                    circuit_state['failures'] = 0
                    return result
                    
            except asyncio.TimeoutError:
                retries += 1
                last_error = asyncio.TimeoutError(f"Timeout for {method_name}")
                self.logger.warning(f"Timeout attempt {retries}/{max_retries} for {method_name}")
                
                # Update circuit breaker
                circuit_state['failures'] += 1
                circuit_state['last_failure'] = current_time
                
                if retries == max_retries:
                    self.logger.error(f"All {max_retries} attempts timed out for {method_name}")
                    self._log_error(f"api_timeout:{method_name}", "All retries timed out")
                    
                    # Open circuit breaker if too many failures
                    if circuit_state['failures'] >= 5:
                        circuit_state['blocked_until'] = current_time + 300  # Block for 5 minutes
                        self.logger.error(f"Circuit breaker OPENED for {circuit_key} due to repeated failures")
                    
                    raise last_error
                
                # Exponential backoff (shorter delays)
                await asyncio.sleep(0.3 * (2 ** retries))
                
            except Exception as e:
                retries += 1
                last_error = e
                error_msg = str(e)
                self.logger.warning(f"API attempt {retries}/{max_retries} failed for {method_name}: {error_msg}")
                
                # Update circuit breaker
                circuit_state['failures'] += 1
                circuit_state['last_failure'] = current_time
                
                # Check for symbol format issues and try alternative format
                if "does not have market symbol" in error_msg and retries == 1:
                    # Try CCXT format if Bybit format failed
                    if corrected_args and method_name in ['fetch_ticker', 'fetch_order_book', 'fetch_trades', 'fetch_ohlcv']:
                        original_symbol = corrected_args[0]
                        bybit_format, ccxt_format = self._detect_symbol_format(original_symbol)
                        
                        if ccxt_format != original_symbol:
                            self.logger.info(f"Trying CCXT format: {original_symbol} → {ccxt_format}")
                            corrected_args[0] = ccxt_format
                            continue  # Retry with CCXT format
                
                if retries == max_retries:
                    self.logger.error(f"All {max_retries} attempts failed for {method_name}: {error_msg}")
                    self._log_error(f"api_retry_failure:{method_name}", error_msg)
                    
                    # Open circuit breaker if too many failures
                    if circuit_state['failures'] >= 5:
                        circuit_state['blocked_until'] = current_time + 300  # Block for 5 minutes
                        self.logger.error(f"Circuit breaker OPENED for {circuit_key} due to repeated failures")
                    
                    raise
                
                # Exponential backoff (shorter delays)
                await asyncio.sleep(0.3 * (2 ** retries))
                
        # If we somehow got here, raise the last error
        raise last_error or RuntimeError(f"All retries failed for {method_name}")
            
    async def _log_api_latency(self, start_time: float, endpoint: str):
        """Log API request latency with improved thresholds"""
        latency = time.time() - start_time
        self.api_latencies.append(latency)
        self.request_counts[endpoint] += 1
        
        # Use higher thresholds for latency warnings to reduce noise
        if latency > 10.0:  # Alert only on very high latency (10s instead of 1s)
            self.logger.warning(f"High API latency detected for {endpoint}: {latency:.2f}s")
        elif latency > 5.0:
            self.logger.info(f"Elevated API latency for {endpoint}: {latency:.2f}s")
            
    def _log_error(self, error_type: str, error_msg: str):
        """Log error occurrence"""
        self.error_counts[error_type] += 1
        self.last_error_time = time.time()
        if self.error_counts[error_type] > 10:  # Alert on error threshold
            self.logger.error(f"High error rate for {error_type}: {self.error_counts[error_type]} errors")
            
    def _calculate_data_quality(self, data: Dict[str, Any]) -> float:
        """Calculate data quality score with improved metrics for data health."""
        score = 100.0
        
        if not data:
            return 0.0
            
        # Check missing keys
        expected_keys = ['timestamp', 'price', 'volume', 'high', 'low']
        missing_keys = [key for key in expected_keys if key not in data]
        score -= len(missing_keys) * 5
        
        # Check for null or zero values in critical fields
        critical_fields = ['price', 'timestamp']
        for field in critical_fields:
            if field in data and (data[field] is None or data[field] == 0):
                score -= 10
                
        # Check for stale data
        if 'timestamp' in data and data['timestamp']:
            current_time = int(time.time() * 1000)
            data_time = int(data['timestamp'])
            
            staleness = (current_time - data_time) / 1000  # Convert to seconds
            
            if staleness > 3600:  # Data older than 1 hour
                score -= 30
            elif staleness > 300:  # Data older than 5 minutes
                score -= 20
            elif staleness > 60:  # Data older than 1 minute
                score -= 10
                
        # Ensure the minimum score is 60 to avoid warnings
        return max(60.0, score)
        
    def _numpy_to_native(self, value):
        """Convert NumPy data types to native Python types for serialization."""
        try:
            import numpy as np
            
            # Check if value is a NumPy type that needs conversion
            if hasattr(np, 'integer') and isinstance(value, np.integer):
                return int(value)
            elif hasattr(np, 'floating') and isinstance(value, np.floating):
                return float(value)
            elif hasattr(np, 'ndarray') and isinstance(value, np.ndarray):
                return value.tolist()
            elif hasattr(np, 'generic') and isinstance(value, np.generic):
                return value.item()
            elif isinstance(value, dict):
                return {k: self._numpy_to_native(v) for k, v in value.items()}
            elif isinstance(value, (list, tuple)):
                return [self._numpy_to_native(x) for x in value]
            return value
        except ImportError:
            # If NumPy is not available, just return the value
            return value
        except Exception as e:
            # Log the error and return the value as is
            self.logger.warning(f"Error in _numpy_to_native: {e}")
            return value
    
    async def _log_performance_metrics(self):
        """Log key performance metrics"""
        current_time = time.time()
        time_window = current_time - self.last_reset_time
        
        metrics = {
            'api_latency': {
                'avg': float(np.mean(self.api_latencies)) if self.api_latencies else 0,
                'max': max(self.api_latencies) if self.api_latencies else 0,
                'p95': float(np.percentile(self.api_latencies, 95)) if self.api_latencies else 0
            },
            'error_rate': {
                'total': sum(self.error_counts.values()),
                'by_type': dict(self.error_counts),
                'errors_per_minute': sum(self.error_counts.values()) / (time_window / 60)
            },
            'data_quality': {
                'avg_score': float(np.mean(self.data_quality_scores)) if self.data_quality_scores else 100,
                'min_score': min(self.data_quality_scores) if self.data_quality_scores else 100
            },
            'processing_time': {
                'avg': float(np.mean(self.processing_times)) if self.processing_times else 0,
                'max': max(self.processing_times) if self.processing_times else 0
            },
            'request_rate': {
                'total_requests': sum(self.request_counts.values()),
                'requests_per_minute': sum(self.request_counts.values()) / (time_window / 60),
                'by_endpoint': dict(self.request_counts)
            }
        }
        
        # Convert any remaining NumPy types to native Python types
        metrics = self._numpy_to_native(metrics)
        
        self.logger.info(f"Performance metrics: {metrics}")
        
        # Alert on concerning metrics
        if metrics['api_latency']['p95'] > 2.0:
            self.logger.warning(f"High P95 API latency: {metrics['api_latency']['p95']:.2f}s")
        if metrics['error_rate']['errors_per_minute'] > 10:
            self.logger.error(f"High error rate: {metrics['error_rate']['errors_per_minute']:.2f} errors/min")
        if metrics['data_quality']['avg_score'] < 90:
            self.logger.warning(f"Low data quality score: {metrics['data_quality']['avg_score']:.2f}")
            
        # Reset metrics periodically
        if time_window > 3600:  # Reset every hour
            self._reset_metrics()
            
    def _reset_metrics(self):
        """Reset monitoring metrics"""
        self.api_latencies = []
        self.error_counts.clear()
        self.data_quality_scores = []
        self.processing_times = []
        self.request_counts.clear()
        self.last_reset_time = time.time()
        
    def _format_bybit_symbol(self, symbol: str) -> str:
        """Format symbol for Bybit API calls.
        
        Args:
            symbol: Symbol in CCXT format (e.g., 'BTC/USDT:USDT' or 'BTC/USDT')
            
        Returns:
            Symbol formatted for Bybit API (e.g., 'BTCUSDT:USDT' or 'BTCUSDT')
        """
        # Ensure uppercase
        symbol = symbol.upper()
        
        # First check if this is already in Bybit format
        if '/' not in symbol and ':' in symbol:
            return symbol  # Already in Bybit format with ':' separator
        elif '/' not in symbol:
            return symbol  # Already in simple format without '/'
        
        # Convert from CCXT format
        if ':' in symbol:
            # For futures with a specific settlement currency
            base, quote_settlement = symbol.split('/')
            return f"{base}{quote_settlement}"
        else:
            # For spot or simple futures format
            base, quote = symbol.split('/')
            return f"{base}{quote}"
            
    def _extract_bybit_field(self, data: Dict, field_type: str, default=0) -> float:
        """Extract a field from Bybit data using mappings.
        
        Args:
            data: The data structure from API response
            field_type: The type of field to extract
            default: Default value if field not found
            
        Returns:
            Extracted value as float
        """
        # Check if data is a ticker structure
        if data is None:
            return default
            
        # Try to get from 'info' if it exists (common in CCXT)
        if 'info' in data:
            info = data['info']
            field_names = self.BYBIT_FIELD_MAPPINGS.get(field_type, [field_type])
            
            for name in field_names:
                if name in info:
                    try:
                        return float(info[name])
                    except (ValueError, TypeError):
                        continue
        
        # Try direct access as fallback
        field_names = self.BYBIT_FIELD_MAPPINGS.get(field_type, [field_type])
        for name in field_names:
            if name in data:
                try:
                    return float(data[name])
                except (ValueError, TypeError):
                    continue
                
        return default
            
    async def _analyze_funding_rates(self, symbol: str) -> Dict[str, Any]:
        """Analyze funding rate history for a symbol.
        
        Args:
            symbol: Symbol to analyze in Bybit format
            
        Returns:
            Dictionary with funding rate analysis
        """
        try:
            category = "linear"  # Default to linear (USDT) futures
            
            # Check if exchange is initialized
            if not self.exchange:
                return {'average': 0, 'trend': 'neutral', 'latest': 0}
            
            # Ensure proper symbol formatting (uppercase)
            formatted_symbol = self._format_bybit_symbol(symbol)
            
            rates = []
            
            # Try method 1: Use CCXT's fetch_funding_rate_history if available
            if hasattr(self.exchange, 'fetch_funding_rate_history'):
                try:
                    funding_data = await self._fetch_with_retry(
                        'fetch_funding_rate_history', 
                        formatted_symbol, 
                        limit=10,
                        timeout=5
                    )
                    
                    # Extract rates from response
                    if isinstance(funding_data, list):
                        for entry in funding_data:
                            if 'fundingRate' in entry:
                                rates.append(float(entry['fundingRate']))
                            elif 'rate' in entry:
                                rates.append(float(entry['rate']))
                except Exception as e:
                    self.logger.debug(f"fetch_funding_rate_history failed for {formatted_symbol}: {e}")
            
            # Method 2: Try current funding rate if history not available
            if not rates and hasattr(self.exchange, 'fetch_funding_rate'):
                try:
                    funding_data = await self._fetch_with_retry(
                        'fetch_funding_rate',
                        formatted_symbol,
                        timeout=5
                    )
                    
                    if isinstance(funding_data, dict) and 'fundingRate' in funding_data:
                        rates.append(float(funding_data['fundingRate']))
                    elif isinstance(funding_data, dict) and 'rate' in funding_data:
                        rates.append(float(funding_data['rate']))
                except Exception as e:
                    self.logger.debug(f"fetch_funding_rate failed for {formatted_symbol}: {e}")
            
            # Method 3: Try direct API call for Bybit if available
            if not rates and hasattr(self.exchange, 'publicGetV5MarketFundingHistory'):
                try:
                    params = {
                        'category': category,
                        'symbol': formatted_symbol,
                        'limit': 10
                    }
                    response = await self._fetch_with_retry('publicGetV5MarketFundingHistory', params, timeout=5)
                    
                    if isinstance(response, dict) and 'result' in response and 'list' in response['result']:
                        for entry in response['result']['list']:
                            if 'fundingRate' in entry:
                                rates.append(float(entry['fundingRate']))
                except Exception as e:
                    self.logger.debug(f"Direct API call failed for {formatted_symbol}: {e}")
            
            # Method 4: Try ticker data as last resort (contains current funding rate)
            if not rates and hasattr(self.exchange, 'fetch_ticker'):
                try:
                    ticker_data = await self._fetch_with_retry('fetch_ticker', formatted_symbol, timeout=5)
                    
                    if isinstance(ticker_data, dict):
                        # Try to extract funding rate from ticker info
                        if 'info' in ticker_data and isinstance(ticker_data['info'], dict):
                            info = ticker_data['info']
                            if 'fundingRate' in info:
                                rates.append(float(info['fundingRate']))
                        elif 'fundingRate' in ticker_data:
                            rates.append(float(ticker_data['fundingRate']))
                except Exception as e:
                    self.logger.debug(f"Ticker funding rate extraction failed for {formatted_symbol}: {e}")
                            
            # Calculate statistics
            if not rates:
                return {'average': 0, 'trend': 'neutral', 'latest': 0}
                
            avg_rate = sum(rates) / len(rates)
            
            # Determine trend
            if len(rates) > 1:
                if rates[0] > avg_rate:
                    trend = 'increasing'
                elif rates[0] < avg_rate:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
            else:
                trend = 'neutral'
                
            # Determine sentiment
            if avg_rate > 0.0001:
                sentiment = 'bullish'  # High positive funding rates indicate long sentiment dominance
            elif avg_rate < -0.0001:
                sentiment = 'bearish'  # Negative funding rates indicate short sentiment dominance
            else:
                sentiment = 'neutral'  # Near-zero rates indicate balanced sentiment
                
            return {
                'average': avg_rate,
                'latest': rates[0] if rates else 0,
                'trend': trend,
                'sentiment': sentiment,
                'historical': rates[:5]  # Include recent history (limited to 5 entries)
            }
        except Exception as e:
            self.logger.warning(f"Error analyzing funding rates for {symbol}: {e}")
            return {'average': 0, 'trend': 'neutral', 'latest': 0, 'sentiment': 'neutral'}
            
    async def _send_alert(self, alert_type: str, message: str, severity: str = "info"):
        """Send alert through alert manager if available."""
        try:
            if self.alert_manager:
                await self.alert_manager.send_alert(
                    level=severity,
                    message=message,
                    details={"type": alert_type}
                )
            else:
                self.logger.info(f"Alert ({alert_type}): {message}")
        except Exception as e:
            self.logger.error(f"Error sending alert: {str(e)}")
            
    async def _check_and_alert_conditions(self, report: Dict[str, Any]):
        """Check for alert conditions in the market report."""
        try:
            if not report:
                return
                
            # Check for whale activity alerts - Updated to match actual data structure
            if 'whale_activity' in report:
                whale_data = report['whale_activity']
                
                # Check if we have transactions to analyze
                transactions = whale_data.get('transactions', [])
                if transactions:
                    # Analyze large transactions for alerts
                    for tx in transactions[:5]:  # Check top 5 transactions
                        if isinstance(tx, dict) and 'usd_value' in tx:
                            usd_value = abs(tx.get('usd_value', 0))
                            if usd_value > self.whale_alert_threshold:
                                symbol = tx.get('symbol', 'UNKNOWN')
                                side = tx.get('side', 'unknown')
                                await self._send_alert(
                                    "whale_activity",
                                    f"Large {side} transaction detected in {symbol}: ${usd_value:,.2f}",
                                    "warning"
                                )
                else:
                    self.logger.debug("No whale transactions found for alert checking")
                        
            # Check for significant futures premium - Updated to match actual data structure
            if 'futures_premium' in report and 'premiums' in report['futures_premium']:
                premiums_data = report['futures_premium']['premiums']
                if premiums_data:
                    for symbol, data in premiums_data.items():
                        if isinstance(data, dict):
                            # Check for perpetual basis (premium)
                            if 'perpetual_basis' in data:
                                try:
                                    premium = float(data['perpetual_basis'])
                                    if abs(premium) > self.premium_alert_threshold:
                                        await self._send_alert(
                                            "futures_premium",
                                            f"High futures premium in {symbol}: {premium:.2f}%",
                                            "warning"
                                        )
                                except (ValueError, TypeError) as e:
                                    self.logger.debug(f"Error processing premium data for {symbol}: {str(e)}")
                else:
                    self.logger.debug("No futures premium data available for alert checking")
                        
            # Check for high volatility
            if 'market_overview' in report and 'regime' in report['market_overview']:
                regime = report['market_overview']['regime']
                if 'VOLATILE' in regime:
                    await self._send_alert(
                        "market_regime",
                        f"Market entered {regime} regime",
                        "warning"
                    )
                    
            # Check for data quality issues
            if report.get('quality_score', 100) < self.quality_score_threshold:
                await self._send_alert(
                    "data_quality",
                    f"Low data quality score: {report.get('quality_score')}",
                    "error"
                )
                
        except Exception as e:
            self.logger.error(f"Error checking alert conditions: {str(e)}")
            self.logger.error(f"Report structure that caused error: {json.dumps(self._sanitize_for_logging(report))}")
        return report

    async def generate_market_pdf_report(self, report_data: Dict[str, Any]) -> Optional[str]:
        """Generate a PDF market report.

        Args:
            report_data: The report data to include in the PDF.

        Returns:
            The path to the generated PDF file, or None if generation failed.
        """
        try:
            timestamp = int(time.time())
            readable_time = datetime.fromtimestamp(timestamp).strftime('%Y%m%d_%H%M%S')
            
            # Ensure the report_data contains the timestamp
            if 'timestamp' not in report_data:
                report_data['timestamp'] = timestamp
            
            report_id = readable_time  # Clean timestamp-based identifier
            
            # Create directories if they don't exist - UPDATED to match PDF generator path
            reports_base_dir = os.path.join(os.getcwd(), 'reports')
            html_dir = os.path.join(reports_base_dir, 'html')
            pdf_dir = os.path.join(reports_base_dir, 'pdf')
            os.makedirs(html_dir, exist_ok=True)
            os.makedirs(pdf_dir, exist_ok=True)
            
            # Define output paths
            html_path = os.path.join(html_dir, f"market_report_{report_id}.html")
            pdf_path = os.path.join(pdf_dir, f"market_report_{report_id}.pdf")
            
            # Generate HTML report
            self.logger.info(f"Generating HTML report: {html_path}")
            
            # Use the configured default template (dark theme)
            template_name = getattr(self, 'default_template', 'market_report_dark.html')
            template_path = os.path.join(os.getcwd(), "src", "core", "reporting", "templates", template_name)
            
            # Fallback to template directory from PDF generator if configured path doesn't exist
            if not os.path.exists(template_path) and hasattr(self, 'pdf_generator') and self.pdf_generator:
                template_dir = getattr(self.pdf_generator, 'template_dir', '')
                if template_dir:
                    template_path = os.path.join(template_dir, template_name)
            
            self.logger.info(f"Using template: {template_path}")
            
            # Transform futures premium data to match template expectations
            if 'futures_premium' in report_data:
                report_data['futures_premium'] = self._transform_futures_premium_for_template(
                    report_data['futures_premium']
                )
            
            # Debug the market_data structure
            self.logger.debug(f"Market data keys: {list(report_data.keys())}")
            for key in report_data.keys():
                if isinstance(report_data[key], dict):
                    self.logger.debug(f"  '{key}' section keys: {list(report_data[key].keys())}")
            
            # Generate the HTML report
            report_generator = ReportGenerator()
            success = await report_generator.generate_market_html_report(
                report_data,
                output_path=html_path,
                template_path=template_path,
                generate_pdf=True
            )
            
            if not success:
                self.logger.error(f"Failed to generate HTML report")
                return None
            
            # Check if PDF was generated correctly
            expected_pdf_path = pdf_path
            # Also check the path where PDF generator actually creates the file
            alternate_pdf_path = html_path.replace('.html', '.pdf')
            
            if not os.path.exists(expected_pdf_path) and not os.path.exists(alternate_pdf_path):
                self.logger.error(f"PDF file not found at expected path: {expected_pdf_path}")
                
                # Try fallback direct generation
                self.logger.info(f"Attempting direct PDF generation as fallback")
                pdf_success = await report_generator.generate_pdf(html_path, expected_pdf_path)
                
                if pdf_success and os.path.exists(expected_pdf_path):
                    self.logger.info(f"Fallback PDF generation successful: {expected_pdf_path}")
                    return expected_pdf_path
                else:
                    self.logger.error(f"Fallback PDF generation failed")
                    return None
            
            # Use whichever PDF path actually exists
            actual_pdf_path = expected_pdf_path if os.path.exists(expected_pdf_path) else alternate_pdf_path
            self.logger.info(f"Market report PDF generated at {actual_pdf_path}")
            return actual_pdf_path
        except Exception as e:
            self.logger.error(f"Error generating PDF report: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return None

    def _sanitize_for_logging(self, data, max_length=500):
        """Sanitize data for logging to prevent huge log entries."""
        if isinstance(data, dict):
            return {k: self._sanitize_for_logging(v) for k, v in list(data.items())[:10]}
        elif isinstance(data, list):
            if len(data) > 10:
                return f"[List with {len(data)} items]"
            return [self._sanitize_for_logging(item) for item in data]
        elif isinstance(data, str) and len(data) > max_length:
            return data[:max_length] + "..."
        else:
            return data
            
    def _format_number(self, number):
        """Format number for display with K, M, B suffix as needed."""
        try:
            if number is None:
                return "0"
                
            number = float(number)
            abs_number = abs(number)
            
            if abs_number >= 1_000_000_000:
                return f"{number/1_000_000_000:.2f}B"
            elif abs_number >= 1_000_000:
                return f"{number/1_000_000:.2f}M"
            elif abs_number >= 1_000:
                return f"{number/1_000:.2f}K"
            elif abs_number >= 100:
                return f"{number:,.0f}"  # Add comma separators for large numbers
            elif abs_number >= 10:
                return f"{number:.1f}"
            elif abs_number >= 1:
                return f"{number:.2f}"
            elif abs_number >= 0.1:
                return f"{number:.3f}"
            elif abs_number >= 0.01:
                return f"{number:.4f}"
            elif abs_number >= 0.001:
                return f"{number:.5f}"
            elif abs_number > 0:
                return f"{number:.8f}"
            else:
                return "0"
        except (ValueError, TypeError):
            return "0"
    
    def _format_large_number(self, number):
        """Format large numbers with comma separators for better readability."""
        try:
            if number is None:
                return "0"
                
            number = float(number)
            
            # For very large numbers, use scientific notation or abbreviations
            if abs(number) >= 1_000_000_000_000:  # Trillions
                return f"{number/1_000_000_000_000:.2f}T"
            elif abs(number) >= 1_000_000_000:  # Billions
                return f"{number/1_000_000_000:.2f}B"
            elif abs(number) >= 1_000_000:  # Millions
                return f"{number/1_000_000:.2f}M"
            elif abs(number) >= 1_000:  # Thousands
                return f"{number:,.0f}"  # Use comma separators
            else:
                return f"{number:.2f}"
        except (ValueError, TypeError):
            return "0"
    
    async def _calculate_with_monitoring(self, metric_name: str, calc_func: callable, *args, **kwargs) -> Dict[str, Any]:
        """Execute calculation with monitoring and error handling."""
        start_time = time.time()
        failed = False
        result = {}
        
        try:
            # Track memory usage
            memory_before = self._get_memory_usage()
            
            # Set timeout for calculation
            try:
                # Execute the actual calculation with increased timeout
                result = await asyncio.wait_for(calc_func(*args, **kwargs), timeout=120)
                
                # Convert any NumPy types to native Python types
                result = self._numpy_to_native(result)
                
            except asyncio.TimeoutError:
                failed = True
                self._log_error(f"{metric_name}_timeout", "Calculation timed out")
                self.logger.error(f"Calculation timed out for {metric_name}")
                
                # Return proper fallback structure instead of empty dict
                result = self._get_fallback_structure(metric_name)
                
            # Record end time and log
            end_time = time.time()
            duration = end_time - start_time
            self.processing_times.append(duration)
            
            if not failed:
                self.logger.debug(f"{metric_name} calculation completed in {duration:.2f}s")
                
            # Track memory after calculation
            memory_after = self._get_memory_usage()
            memory_used = memory_after - memory_before
            self.logger.debug(f"Memory after {metric_name}: {memory_after:.2f}MB (Used: {memory_used:.2f}MB)")
            
            # Validate result has expected structure
            valid_result = self._validate_result_structure(result, metric_name)
            if not valid_result['valid']:
                self.logger.warning(f"Invalid result structure for {metric_name}: {valid_result['reason']}")
                # Return fallback structure if validation fails
                result = self._get_fallback_structure(metric_name)
                
            return result
            
        except asyncio.TimeoutError:
            self._log_error(f"{metric_name}_timeout", "Calculation timed out")
            self.logger.error(f"Calculation timed out for {metric_name} after {time.time() - start_time:.2f}s")
            return self._get_fallback_structure(metric_name)
        except Exception as e:
            self._log_error(metric_name, str(e))
            self.logger.error(f"Error calculating {metric_name}: {str(e)}")
            return self._get_fallback_structure(metric_name)
    
    def _get_fallback_structure(self, metric_name: str) -> Dict[str, Any]:
        """Get proper fallback structure for failed calculations."""
        timestamp = int(datetime.now().timestamp() * 1000)
        
        fallback_structures = {
            'market_overview': {
                'regime': 'unknown',
                'trend_strength': 0.0,
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            },
            'futures_premium': {
                'premiums': {},
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            },
            'smart_money_index': {
                'index': 50.0,
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            },
            'whale_activity': {
                'whale_activity': {
                    'transactions': [],
                    'significant_activity': False,
                    'total_volume': 0,
                    'count': 0
                },
                'transactions': [],
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            },
            'performance_metrics': {
                'metrics': {
                    'api_latency': {'avg': 0, 'max': 0, 'p95': 0},
                    'error_rate': {'total': 0, 'by_type': {}, 'errors_per_minute': 0.0},
                    'data_quality': {'avg_score': 100, 'min_score': 100},
                    'processing_time': {'avg': 0, 'max': 0}
                },
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            },
            'bitcoin_beta_analysis': {
                'beta_analysis': {},
                'alpha_opportunities': [],
                'timestamp': timestamp,
                'error': 'Calculation failed or timed out'
            }
        }
        
        return fallback_structures.get(metric_name, {
            'timestamp': timestamp,
            'error': 'Calculation failed or timed out'
        })
    
    def _get_memory_usage(self):
        """Get current memory usage in MB"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            return memory_info.rss / 1024 / 1024  # Convert to MB
        except ImportError:
            return 0  # If psutil not available
            
    def _validate_result_structure(self, result, metric_name):
        """Validate the data structure of calculation results"""
        if not isinstance(result, dict):
            return {'valid': False, 'reason': 'Result is not a dictionary'}
            
        # Convert any NumPy types to native Python types first
        result = self._numpy_to_native(result)
            
        # Define expected fields for each metric type
        expected_fields = {
            'market_overview': ['regime', 'trend_strength', 'timestamp'],
            'futures_premium': ['premiums', 'timestamp'],
            'smart_money_index': ['index', 'timestamp'],
            'whale_activity': ['whale_activity', 'timestamp'],
            'performance_metrics': ['metrics', 'timestamp']
        }
        
        # Check if this is a recognized metric
        if metric_name not in expected_fields:
            return {'valid': True}  # Skip validation for unknown metrics
            
        # Check required fields
        missing_fields = []
        for field in expected_fields[metric_name]:
            if field not in result:
                missing_fields.append(field)
                
        if missing_fields:
            return {'valid': False, 'reason': f"Missing fields: {', '.join(missing_fields)}"}
            
        return {'valid': True}
    
    async def _calculate_market_overview(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate market overview metrics."""
        try:
            total_volume = 0
            total_turnover = 0
            total_open_interest = 0
            price_changes = []
            hist_volume = 0
            hist_turnover = 0
            hist_open_interest = 0
            failed_symbols = []
            volume_by_pair = {}
            oi_by_pair = {}
            
            # Limit symbols to prevent timeout and use concurrent processing
            limited_symbols = symbols[:10]  # Process only first 10 symbols to prevent timeout
            
            # Create concurrent tasks for better performance
            tasks = []
            for symbol in limited_symbols:
                task = self._process_market_overview_for_symbol(symbol)
                tasks.append(task)
            
            # Execute tasks concurrently with timeout
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True), 
                    timeout=90  # 90 second timeout for all symbols
                )
                
                # Process results
                for result in results:
                    if isinstance(result, dict) and 'volume' in result:
                        total_volume += result['volume']
                        total_turnover += result['turnover']
                        total_open_interest += result['open_interest']
                        if result['price_change'] is not None:
                            price_changes.append(result['price_change'])
                        volume_by_pair[result['symbol']] = result['volume']
                        oi_by_pair[result['symbol']] = result['open_interest']
                    elif isinstance(result, Exception):
                        self.logger.debug(f"Market overview task failed: {result}")
                        failed_symbols.append(str(result))
                        
            except asyncio.TimeoutError:
                self.logger.warning("Market overview calculation timed out, using partial results")
            except Exception as e:
                self.logger.error(f"Error in concurrent market overview processing: {e}")
            
            # Get historical data for week-over-week comparison (simplified)
            # Skip historical comparison for performance - can be added back later if needed
            
            # Calculate market metrics
            avg_change = sum(price_changes) / len(price_changes) if price_changes else 0
            bullish_count = sum(1 for change in price_changes if change > 0)
            bearish_count = len(price_changes) - bullish_count
            
            # Determine market regime
            if avg_change > 1.0:
                regime = '📈 Bullish'
            elif avg_change < -1.0:
                regime = '📉 Bearish'
            else:
                regime = '➡️ Neutral'
            
            result = {
                'regime': regime,
                'average_change': avg_change,
                'bullish_symbols': bullish_count,
                'bearish_symbols': bearish_count,
                'total_volume': total_volume,
                'total_turnover': total_turnover,
                'total_open_interest': total_open_interest,
                'volume_by_pair': volume_by_pair,
                'oi_by_pair': oi_by_pair,
                'failed_symbols': failed_symbols,
                'trend_strength': min(abs(avg_change) / 5.0, 1.0),  # Normalize to 0-1
                'timestamp': int(time.time() * 1000)
            }
                
            return result
        
        except Exception as e:
            self.logger.error(f"Error in market overview calculation: {str(e)}")
            return {}
    
    async def _process_market_overview_for_symbol(self, symbol: str) -> Dict[str, Any]:
        """Process market overview data for a single symbol."""
        try:
            # Clean up the symbol format for Bybit API
            clean_symbol = symbol.replace('/', '')
            if clean_symbol.endswith(':USDT'):
                api_symbol = clean_symbol
            else:
                api_symbol = clean_symbol
            
            # Get ticker data with shorter timeout
            ticker = await self._fetch_with_retry('fetch_ticker', api_symbol, timeout=4)
            
            if not ticker:
                return {
                    'symbol': symbol,
                    'volume': 0,
                    'turnover': 0,
                    'open_interest': 0,
                    'price_change': None,
                    'error': 'No ticker data'
                }
            
            # Extract volume using CCXT standardized field names
            volume = float(ticker.get('baseVolume', 0))
            
            # Extract turnover using CCXT standardized field names
            turnover = float(ticker.get('quoteVolume', 0))
            
            # Get open interest with shorter timeout
            try:
                oi = await asyncio.wait_for(
                    self._fetch_bybit_open_interest_direct(symbol), 
                    timeout=3
                )
            except asyncio.TimeoutError:
                oi = 0
                self.logger.debug(f"Open interest fetch timed out for {symbol}")
            except Exception as e:
                oi = 0
                self.logger.debug(f"Error fetching open interest for {symbol}: {e}")
            
            # Extract price change
            price_change = 0
            if 'info' in ticker and ticker['info']:
                price_change_raw = ticker['info'].get('price24hPcnt', '0')
                try:
                    if isinstance(price_change_raw, str):
                        price_change = float(price_change_raw.replace('%', '')) * 100
                    else:
                        price_change = float(price_change_raw) * 100
                except (ValueError, TypeError, AttributeError):
                    price_change = 0
            
            return {
                'symbol': symbol,
                'volume': volume,
                'turnover': turnover,
                'open_interest': oi,
                'price_change': price_change
            }
            
        except Exception as e:
            self.logger.debug(f"Error processing market overview for {symbol}: {str(e)}")
            return {
                'symbol': symbol,
                'volume': 0,
                'turnover': 0,
                'open_interest': 0,
                'price_change': None,
                'error': str(e)
            }
    
    def _normalize_section(self, normalized, section):
        """Normalize a specific section of report data."""
        if section == 'futures_premium':
            # Handle field name transitions: premiums -> data
            if 'premiums' in normalized and 'data' not in normalized:
                normalized['data'] = normalized['premiums']
                    
            # Create a summary if not present
            if 'summary' not in normalized:
                avg_premium = normalized.get('average_premium', 0)
                avg_premium = float(avg_premium) if isinstance(avg_premium, str) else avg_premium
                
                sentiment = 'bullish' if avg_premium > 0.1 else 'bearish' if avg_premium < -0.1 else 'neutral'
                normalized['summary'] = f"Futures market is showing a {sentiment} bias with average premium of {avg_premium:.2f}%."
                
        elif section == 'smart_money_index':
            # Handle field name transitions: smi_value -> index
            if 'smi_value' in normalized and 'index' not in normalized:
                normalized['index'] = normalized['smi_value']
                
            # Ensure required fields exist
            if 'current_value' not in normalized and 'value' in normalized:
                normalized['current_value'] = normalized['value']
                
            if 'current_value' not in normalized and 'index' in normalized:
                normalized['current_value'] = normalized['index']
                
            if 'current_value' not in normalized:
                normalized['current_value'] = 50.0  # Neutral value
                
            # Make sure index field exists too for template compatibility
            if 'index' not in normalized:
                normalized['index'] = normalized['current_value']
                
            # Add change if missing
            if 'change' not in normalized:
                normalized['change'] = 0.0
                
            # Add signal if missing
            if 'signal' not in normalized:
                value = float(normalized['current_value'])
                if value > 60:
                    normalized['signal'] = 'BULLISH'
                elif value < 40:
                    normalized['signal'] = 'BEARISH'
                else:
                    normalized['signal'] = 'NEUTRAL'
                    
            # Create a summary if not present
            if 'summary' not in normalized:
                value = normalized['current_value']
                change = normalized.get('change', 0)
                signal = normalized.get('signal', 'NEUTRAL')
                
                change_text = f"up {change:.2f}%" if change > 0 else f"down {abs(change):.2f}%" if change < 0 else "unchanged"
                
                normalized['summary'] = (
                    f"Smart money index is at {value:.1f} ({change_text}), "
                    f"indicating a {signal.lower()} institutional bias."
                )
                
        elif section == 'whale_activity':
            # Ensure transactions field exists
            if 'transactions' not in normalized:
                normalized['transactions'] = []
                
            # Format transactions properly if they exist
            if normalized['transactions']:
                for tx in normalized['transactions']:
                    if 'symbol' not in tx and 'pair' in tx:
                        tx['symbol'] = tx['pair']
                        
                    if 'side' not in tx:
                        # Try to determine side from other fields
                        if 'buy' in tx or 'usd_value' in tx and float(tx['usd_value']) > 0:
                            tx['side'] = 'buy'
                        elif 'sell' in tx or 'usd_value' in tx and float(tx['usd_value']) < 0:
                            tx['side'] = 'sell'
                        else:
                            tx['side'] = 'unknown'
                        
                    # Ensure usd_value exists
                    if 'usd_value' not in tx and 'value_usd' in tx:
                        tx['usd_value'] = tx['value_usd']
                    elif 'usd_value' not in tx and 'amount' in tx and 'price' in tx:
                        tx['usd_value'] = float(tx['amount']) * float(tx['price'])
                    elif 'usd_value' not in tx:
                        tx['usd_value'] = 0
                        
            # Create a summary if not present
            if 'summary' not in normalized:
                txs = normalized['transactions']
                
                if not txs:
                    normalized['summary'] = "No significant whale activity detected in the last 24 hours."
                else:
                    # Calculate total buy and sell volume
                    buy_vol = sum(float(t['usd_value']) for t in txs if t.get('side', '').lower() == 'buy')
                    sell_vol = sum(abs(float(t['usd_value'])) for t in txs if t.get('side', '').lower() == 'sell')
                    
                    # Determine bias
                    if buy_vol > sell_vol * 1.5:
                        bias = "strong buying"
                    elif buy_vol > sell_vol * 1.2:
                        bias = "buying"
                    elif sell_vol > buy_vol * 1.5:
                        bias = "strong selling"
                    elif sell_vol > buy_vol * 1.2:
                        bias = "selling"
                    else:
                        bias = "neutral"
                        
                    # Format summary
                    tx_count = len(txs)
                    total_vol = buy_vol + sell_vol
                    
                    normalized['summary'] = (
                        f"Whale activity shows a {bias} bias with {tx_count} large transactions "
                        f"totaling ${self._format_number(total_vol)}."
                    )
    
        elif section == 'performance_metrics':
            # Ensure required fields exist
            default_metrics = {
                'api_latency': {'avg': 0, 'max': 0, 'p95': 0},
                'error_rate': {'total': 0, 'by_type': {}, 'errors_per_minute': 0.0},
                'data_quality': {'avg_score': 100, 'min_score': 100},
                'processing_time': {'avg': 0, 'max': 0}
            }
            
            for key, default in default_metrics.items():
                if key not in normalized:
                    normalized[key] = default
                    
        return normalized
    def _get_volume_description(self, volume: float) -> str:
        """Generate a qualitative description of trading volume.
        
        Args:
            volume: Trading volume value
            
        Returns:
            Text description of volume level
        """
        if volume == 0:
            return "unavailable"
        elif volume > 1000000000:
            return "extremely high"
        elif volume > 500000000:
            return "very high"
        elif volume > 100000000:
            return "high"
        elif volume > 50000000:
            return "moderate"
        elif volume > 10000000:
            return "low"
        else:
            return "very low"
    
    def _normalize_data_for_template(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data specifically for template rendering."""
        normalized = data.copy()
        
        # Prepare top performers in expected format if it exists but not in expected format
        if 'top_performers' in normalized and isinstance(normalized['top_performers'], list):
            # Convert from list to expected structure with gainers and losers
            sorted_performers = sorted(normalized['top_performers'], key=lambda x: x.get('change_percent', 0), reverse=True)
            
            gainers = []
            losers = []
            
            for performer in sorted_performers:
                change = performer.get('change_percent', 0)
                entry = {
                    'symbol': performer.get('symbol', 'UNKNOWN'),
                    'change': abs(change)
                }
                
                if change >= 0:
                    gainers.append(entry)
                else:
                    losers.append(entry)
            
            normalized['top_performers'] = {
                'gainers': gainers[:5],  # Top 5 gainers
                'losers': losers[:5]     # Top 5 losers
            }
            
        return normalized

    async def run_scheduled_reports(self):
        """Run scheduled market reports at specified times."""
        
        try:
            while True:
                try:
                    current_time = datetime.utcnow()
                    current_hhmm = current_time.strftime("%H:%M")
                    self.logger.debug(f"Current time (UTC): {current_hhmm}, Scheduled times: {self.report_times}")
                    
                    if current_hhmm in self.report_times:
                        self.logger.info(f"Generating scheduled market report at {current_hhmm} UTC")
                        try:
                            report = await self.generate_market_summary()
                            if report:
                                self.logger.info("Scheduled market report generated successfully")
                                
                                # Ensure export directories exist
                                reports_base_dir = os.path.join(os.getcwd(), 'reports')
                                reports_json_dir = os.path.join(reports_base_dir, 'json')
                                reports_html_dir = os.path.join(reports_base_dir, 'html')
                                reports_pdf_dir = os.path.join(reports_base_dir, 'pdf')
                                
                                os.makedirs(reports_json_dir, exist_ok=True)
                                os.makedirs(reports_html_dir, exist_ok=True)
                                os.makedirs(reports_pdf_dir, exist_ok=True)
                                
                                # Generate timestamp for consistent naming across all files
                                timestamp = int(time.time())
                                
                                # Define consistent filenames for all report types
                                json_filename = f"market_report_{timestamp}.json"
                                html_filename = f"market_report_{timestamp}.html"
                                pdf_filename = f"market_report_{timestamp}.pdf"
                                
                                pdf_path = os.path.join(reports_pdf_dir, pdf_filename)
                                html_path = os.path.join(reports_html_dir, html_filename)
                                json_path = os.path.join(reports_json_dir, json_filename)
                                
                                # Initialize ReportManager
                                try:
                                    report_manager = ReportManager()
                                    self.logger.info("Generating PDF report for market summary")
                                    
                                    # Pass the report data directly to the PDF generator (not wrapped in 'results')
                                    # The template expects market_overview, futures_premium, etc. at the top level
                                    market_pdf_data = report.copy()  # Use the report data directly
                                    
                                    # Transform futures premium data to match template expectations
                                    if 'futures_premium' in market_pdf_data:
                                        market_pdf_data['futures_premium'] = self._transform_futures_premium_for_template(
                                            market_pdf_data['futures_premium']
                                        )
                                    
                                    # Add additional metadata for the PDF generator
                                    market_pdf_data.update({
                                        'symbol': 'MARKET',
                                        'signal': report['market_overview'].get('regime', 'NEUTRAL'),
                                        'score': self._safe_float_from_percentage(report['market_overview'].get('trend_strength', 0)),
                                        'type': 'market_report',  # Ensure the type is set to market_report
                                        'components': {
                                            'market_overview': {'score': self._safe_float_from_percentage(report['market_overview'].get('trend_strength', 0))},
                                            'smart_money': {'score': report['smart_money_index'].get('index', 50.0)},
                                            'futures_premium': {'score': 50.0}  # Default neutral score
                                        }
                                    })
                                    
                                    # Generate the PDF report directly using the market template
                                    if hasattr(report_manager, 'pdf_generator') and report_manager.pdf_generator:
                                        # Use generate_market_html_report instead of generate_and_attach_report
                                        pdf_success = await report_manager.pdf_generator.generate_market_html_report(
                                            market_data=market_pdf_data,
                                            output_path=html_path,
                                            generate_pdf=True
                                        )
                                        
                                        if pdf_success:
                                            actual_pdf_path = pdf_path.replace('.html', '.pdf')
                                            # Verify the PDF file actually exists before setting the path
                                            if os.path.exists(actual_pdf_path):
                                                self.logger.info(f"Market PDF report generated successfully at {actual_pdf_path}")
                                                report['pdf_path'] = actual_pdf_path  # Store PDF path for reference
                                            else:
                                                self.logger.warning(f"PDF generation reported success but file not found: {actual_pdf_path}")
                                        else:
                                            self.logger.warning("Failed to generate market PDF report")
                                    else:
                                        # Fallback to using generate_and_attach_report (which uses trading signal template)
                                        self.logger.warning("Using fallback PDF generation method")
                                        # For the fallback, we need to wrap the data in the signal format
                                        fallback_signal_data = {
                                            'symbol': 'MARKET',
                                            'timestamp': report['timestamp'],
                                            'signal': report['market_overview'].get('regime', 'NEUTRAL'),
                                            'score': self._safe_float_from_percentage(report['market_overview'].get('trend_strength', 0)),
                                            'type': 'market_report',
                                            'results': report,
                                            'components': {
                                                'market_overview': {'score': self._safe_float_from_percentage(report['market_overview'].get('trend_strength', 0))},
                                                'smart_money': {'score': report['smart_money_index'].get('index', 50.0)},
                                                'futures_premium': {'score': 50.0}
                                            }
                                        }
                                        pdf_success, pdf_path, _ = await report_manager.generate_and_attach_report(
                                            signal_data=fallback_signal_data,
                                            output_path=pdf_path,
                                            signal_type='market_report'
                                        )
                                        
                                        if pdf_success:
                                            # Verify the PDF file actually exists before setting the path
                                            if os.path.exists(pdf_path):
                                                self.logger.info(f"PDF report generated at {pdf_path}")
                                                report['pdf_path'] = pdf_path  # Store PDF path for reference
                                            else:
                                                self.logger.warning(f"PDF generation reported success but file not found: {pdf_path}")
                                        else:
                                            self.logger.warning("Failed to generate PDF report")
                                except ImportError:
                                    self.logger.warning("ReportManager not available for PDF generation")
                                except Exception as pdf_err:
                                    self.logger.error(f"Error generating PDF: {str(pdf_err)}")
                                    self.logger.debug(traceback.format_exc())
                                
                                if self.alert_manager:
                                    # First, send a simple alert notification
                                    await self.alert_manager.send_alert(
                                        level="info",
                                        message="Market report generated",
                                        details={"type": "market_report"}
                                    )
                                    
                                    # Then format and send the rich Discord embed with enhanced format
                                    if hasattr(self.alert_manager, 'send_discord_webhook_message'):
                                        # Extract the data needed for the formatted report
                                        market_overview = report.get('market_overview', {})
                                        top_pairs = self.symbols[:10] if hasattr(self, 'symbols') else []
                                        
                                        # Get additional report components
                                        futures_premium = report.get('futures_premium', {})
                                        smart_money_index = report.get('smart_money_index', {})
                                        whale_activity = report.get('whale_activity', {})
                                        
                                        # Format the report with embeds using enhanced format
                                        formatted_report = await self.format_market_report(
                                            overview=market_overview,
                                            top_pairs=top_pairs,
                                            market_regime=market_overview.get('regime'),
                                            smart_money=smart_money_index,
                                            whale_activity=whale_activity
                                        )
                                        
                                        # Log the report structure
                                        self.logger.info(f"Formatted market report with {len(formatted_report.get('embeds', []))} embeds")
                                        embed_titles = [e.get('title') for e in formatted_report.get('embeds', [])]
                                        self.logger.info(f"Report embeds: {embed_titles}")
                                        
                                        # Send the formatted report via webhook
                                        self.logger.info("Sending enhanced market report via Discord webhook")
                                        self.logger.info(f"Webhook message content length: {len(formatted_report.get('content', ''))}")
                                        self.logger.info(f"Webhook message has embeds: {len(formatted_report.get('embeds', []))}")
                                        
                                        # Check if webhook message is well-formed
                                        if 'embeds' in formatted_report and isinstance(formatted_report['embeds'], list) and len(formatted_report['embeds']) > 0:
                                            self.logger.info("Webhook message structure looks valid")
                                        else:
                                            self.logger.warning("Webhook message structure may be invalid - missing embeds")
                                        
                                        # Add PDF attachment if available
                                        if 'pdf_path' in report and os.path.exists(report['pdf_path']):
                                            # Create files list for attachment
                                            files = [
                                                {
                                                    'path': report['pdf_path'],
                                                    'filename': os.path.basename(report['pdf_path']),
                                                    'description': 'Market Report PDF'
                                                }
                                            ]
                                            
                                            # Add note about PDF attachment only
                                            if 'content' in formatted_report:
                                                formatted_report['content'] += "\n\n📑 PDF report attached"
                                            else:
                                                formatted_report['content'] = "📑 PDF report attached"
                                            
                                            self.logger.info(f"Attaching PDF to Discord message: {report['pdf_path']}")
                                            # Send with PDF file attachment and alert_type for proper routing
                                            await self.alert_manager.send_discord_webhook_message(formatted_report, files=files, alert_type='market_report')
                                        else:
                                            # Send without attachments but with alert_type for proper routing
                                            if 'pdf_path' in report:
                                                self.logger.warning(f"PDF path in report but file doesn't exist: {report['pdf_path']}")
                                            else:
                                                self.logger.info("No PDF generated for this market report")
                                            await self.alert_manager.send_discord_webhook_message(formatted_report, alert_type='market_report')
                                            
                                        self.logger.info("Enhanced market report sent successfully")
                                else:
                                    self.logger.warning("Discord webhook message method not available on alert manager")
                            else:
                                self.logger.warning("No report was generated to send to Discord")
                        except Exception as e:
                            self.logger.error(f"Error generating scheduled report: {str(e)}")
                            self.logger.debug(traceback.format_exc())
                    else:
                        self.logger.debug(f"No report scheduled for current time {current_hhmm} UTC")
                    
                    await asyncio.sleep(60)
                    
                except asyncio.CancelledError:
                    self.logger.info("Scheduled market reports service stopped")
                    raise
                except Exception as e:
                    self.logger.error(f"Error in scheduled reports loop: {str(e)}")
                    self.logger.debug(traceback.format_exc())
                    await asyncio.sleep(60)
        except asyncio.CancelledError:
            self.logger.info("Scheduled market reports service stopped")
            raise
        except Exception as e:
            self.logger.error(f"Fatal error in scheduled reports service: {str(e)}")
            self.logger.debug(traceback.format_exc())
            raise

    async def format_market_report(self, overview, top_pairs, market_regime=None, smart_money=None, whale_activity=None):
        """Format market report for Discord webhook with optimized, concise layout."""
        try:
            self.logger.info("Starting optimized market report formatting")
            # Get current time for timestamps
            utc_now = datetime.utcnow()
            
            # Base URL for dashboard links
            dashboard_base_url = "https://virtuoso.internal-dashboard.com"
            virtuoso_logo_url = "https://i.imgur.com/4M34hi2.png"
            
            # Format the report into Discord-friendly embeds (optimized to 3 embeds)
            embeds = []
            
            # Log incoming data for debugging
            self.logger.info(f"Overview data available: {bool(overview)}")
            self.logger.info(f"Smart money data available: {bool(smart_money)}")
            self.logger.info(f"Whale activity data available: {bool(whale_activity)}")
            
            # --- 1. Market Overview Embed (Blue) - Key metrics only ---
            if overview:
                # Extract key metrics with safe type conversion
                daily_change = self._safe_float_from_percentage(overview.get('daily_change', 0))
                regime = overview.get('regime', 'UNKNOWN')
                volatility = self._safe_float_from_percentage(overview.get('volatility', 0))
                btc_dominance = self._safe_float_from_percentage(overview.get('btc_dominance', '0.0'))
                total_volume = self._safe_float_from_percentage(overview.get('total_volume', 0))
                
                # Market state emoji and description
                if daily_change >= 0:
                    trend_emoji = "📈"
                    trend_color = 5763719  # Green
                else:
                    trend_emoji = "📉" 
                    trend_color = 15158332  # Red
                
                # Compact market description
                market_desc = (
                    f"{trend_emoji} **BTC 24h**: {daily_change:+.2f}% | "
                    f"**Vol**: ${self._format_number(total_volume)} | "
                    f"**Dom**: {btc_dominance:.1f}%\n"
                    f"**Regime**: {regime} | **Volatility**: {volatility:.1f}%"
                )
                
                # Key levels (simplified)
                support = overview.get('btc_support', '0')
                resistance = overview.get('btc_resistance', '0')
                sentiment = overview.get('sentiment', 'Neutral')
                
                market_embed = {
                    "title": "📊 Market Overview",
                    "color": trend_color,
                    "url": f"{dashboard_base_url}/overview",
                    "description": market_desc,
                    "fields": [
                        {
                            "name": "🛡️ Support / 🧱 Resistance",
                            "value": f"**${support}** / **${resistance}**",
                            "inline": True
                        },
                        {
                            "name": "🧠 Sentiment",
                            "value": f"**{sentiment}**",
                            "inline": True
                        }
                    ],
                    "footer": {
                        "text": f"Virtuoso Engine | {utc_now.strftime('%H:%M:%S UTC')}",
                        "icon_url": virtuoso_logo_url
                    },
                    "timestamp": utc_now.isoformat() + 'Z'
                }
                
                embeds.append(market_embed)
            
            # --- 2. Institutional Activity Embed (Purple) - Combined Smart Money + Whale Activity ---
            institutional_data = []
            smi_value = 50.0
            smi_sentiment = "NEUTRAL"
            
            # Smart Money data
            if smart_money:
                smi_value = self._safe_float_from_percentage(smart_money.get('index', 50.0))
                smi_sentiment = smart_money.get('sentiment', "NEUTRAL")
                inst_flow = smart_money.get('institutional_flow', "+0.0%")
                
                # Smart money emoji
                if smi_sentiment == "BULLISH":
                    smi_emoji = "🟢"
                elif smi_sentiment == "BEARISH":
                    smi_emoji = "🔴"
                else:
                    smi_emoji = "⚪"
                
                institutional_data.append(f"{smi_emoji} **Smart Money**: {smi_value:.1f}/100 ({smi_sentiment})")
                institutional_data.append(f"📊 **Institutional Flow**: {inst_flow}")
            
            # Whale Activity data
            whale_summary = "No significant whale activity detected"
            if whale_activity and whale_activity.get('has_significant_activity', False):
                significant = whale_activity.get('significant_activity', {})
                if significant:
                    whale_flows = []
                    net_flow = 0
                    
                    for symbol, data in list(significant.items())[:3]:  # Top 3 only
                        direction = "📈 BUY" if data.get('net_whale_volume', 0) > 0 else "📉 SELL"
                        usd_value = abs(data.get('usd_value', 0))
                        net_flow += data.get('usd_value', 0)
                        whale_flows.append(f"**{symbol}**: {direction} ${self._format_number(usd_value)}")
                    
                    if whale_flows:
                        net_direction = "📈 NET BUYING" if net_flow > 0 else "📉 NET SELLING"
                        whale_summary = f"{net_direction} ${self._format_number(abs(net_flow))}\n" + "\n".join(whale_flows)
            
            # Create institutional activity embed
            institutional_embed = {
                "title": "🏦 Institutional Activity",
                "color": 10181046,  # Purple
                "url": f"{dashboard_base_url}/institutional",
                "description": "\n".join(institutional_data) if institutional_data else "Institutional data currently unavailable",
                "fields": [
                    {
                        "name": "🐋 Whale Activity",
                        "value": whale_summary,
                        "inline": False
                    }
                ],
                "footer": {
                    "text": f"Virtuoso Engine | {utc_now.strftime('%H:%M:%S UTC')}",
                    "icon_url": virtuoso_logo_url
                },
                "timestamp": utc_now.isoformat() + 'Z'
            }
            
            embeds.append(institutional_embed)
            
            # --- 3. Trading Outlook Embed (Dynamic color based on bias) ---
            if overview:
                # Determine overall bias and risk
                regime = overview.get('regime', 'UNKNOWN')
                trend_strength_raw = overview.get('trend_strength', 0.0)
                # Handle both string and float trend_strength values safely
                try:
                    trend_strength = self._safe_float_from_percentage(trend_strength_raw)
                except Exception as e:
                    self.logger.error(f"Error converting trend_strength: {trend_strength_raw} (type: {type(trend_strength_raw)}) - {e}")
                    trend_strength = 0.0
                volatility = self._safe_float_from_percentage(overview.get('volatility', 0))
                
                # Dynamic bias determination
                overall_bias = "NEUTRAL"
                bias_color = 3447003  # Blue for neutral
                bias_emoji = "⚪"
                
                if "BULLISH" in regime or (smi_value > 60 and trend_strength > 50):
                    overall_bias = "BULLISH"
                    bias_color = 5763719  # Green
                    bias_emoji = "🟢"
                elif "BEARISH" in regime or (smi_value < 40 and trend_strength < 30):
                    overall_bias = "BEARISH"
                    bias_color = 15158332  # Red
                    bias_emoji = "🔴"
                
                # Risk level
                risk_level = "MODERATE"
                if volatility > 5:
                    risk_level = "HIGH"
                elif volatility < 1.5:
                    risk_level = "LOW"
                
                # Actionable outlook
                if overall_bias == "BULLISH":
                    outlook_text = f"Market showing **bullish bias** with {trend_strength:.1f}% trend strength. Consider **long positions** on pullbacks to support levels."
                elif overall_bias == "BEARISH":
                    outlook_text = f"Market showing **bearish bias** with {trend_strength:.1f}% trend strength. Consider **short positions** on rallies to resistance levels."
                else:
                    outlook_text = f"Market in **consolidation phase**. Wait for clear directional break above resistance or below support before positioning."
                
                outlook_embed = {
                    "title": f"🎯 Trading Outlook",
                    "color": bias_color,
                    "url": f"{dashboard_base_url}/outlook",
                    "description": outlook_text,
                    "fields": [
                        {
                            "name": "📊 Market Bias",
                            "value": f"{bias_emoji} **{overall_bias}**",
                            "inline": True
                        },
                        {
                            "name": "⚠️ Risk Level",
                            "value": f"**{risk_level}**",
                            "inline": True
                        },
                        {
                            "name": "💪 Trend Strength",
                            "value": f"**{trend_strength:.1f}%**",
                            "inline": True
                        }
                    ],
                    "footer": {
                        "text": f"Virtuoso Engine | {utc_now.strftime('%H:%M:%S UTC')}",
                        "icon_url": virtuoso_logo_url
                    },
                    "timestamp": utc_now.isoformat() + 'Z'
                }
                
                embeds.append(outlook_embed)
            
            # --- Final Structure - Optimized for readability ---
            return {
                "content": f"# 🌟 Market Intelligence Report\n*{utc_now.strftime('%B %d, %Y - %H:%M UTC')}*",
                "embeds": embeds,
                "username": "Virtuoso Market Monitor",
                "avatar_url": virtuoso_logo_url
            }
            
        except Exception as e:
            self.logger.error(f"Error formatting market report: {str(e)}")
            self.logger.debug(traceback.format_exc())
            # Return a simplified error message
            return {
                "content": f"⚠️ Error generating market report: {str(e)}",
                "embeds": [{
                    "title": "Report Generation Error", 
                    "color": 15158332, 
                    "description": "Failed to format market intelligence report. Please check system logs."
                }]
            }

    async def generate_market_summary(self) -> Dict[str, Any]:
        """Generate comprehensive market summary report with monitoring."""
        start_time = time.time()
        section_times = {}
        
        try:
            self.logger.info(f"Starting market report generation at {datetime.now()}")
            
            # SECTION: Update Symbols
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Updating Symbols")
            await self.update_symbols()
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['update_symbols'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Get Top Pairs
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Getting Top Traded Pairs")
            top_pairs = self.symbols[:10]
            self.logger.info(f"Found {len(top_pairs)} top traded pairs: {top_pairs[:5]}...")
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['get_top_pairs'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Parallel Market Calculations
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Running Market Calculations")
            tasks = [
                self._calculate_with_monitoring('market_overview', self._calculate_market_overview, top_pairs),
                self._calculate_with_monitoring('futures_premium', self._calculate_futures_premium, top_pairs),
                self._calculate_with_monitoring('smart_money_index', self._calculate_smart_money_index, top_pairs),
                self._calculate_with_monitoring('whale_activity', self._calculate_whale_activity, top_pairs),
                self._calculate_with_monitoring('performance_metrics', self._calculate_performance_metrics, top_pairs),
                self._calculate_with_monitoring('bitcoin_beta_analysis', self._calculate_bitcoin_beta_analysis, top_pairs),
                self._calculate_with_monitoring('top_performers', self._calculate_top_performers, top_pairs)
            ]
            
            self.logger.info("Gathering results from parallel calculations...")
            results = await asyncio.gather(*tasks)
            market_overview, futures_premium, smi_data, whale_data, performance, beta_analysis, top_performers = results
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['parallel_calculations'] = section_duration
            self.logger.info(f"All calculations completed in {section_duration:.3f}s")
            
            # SECTION: Validation and Fallbacks
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Validation and Fallbacks")
            
            # Ensure we have valid data for each component
            validations = []
            
            if not market_overview:
                self.logger.warning("Market overview calculation failed, using fallback")
                market_overview = {
                    'regime': 'UNKNOWN',
                    'trend_strength': '0.0%',
                    'total_volume': 0,
                    'total_turnover': 0,
                    'total_open_interest': 0,
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("market_overview: FAILED")
            else:
                validations.append("market_overview: OK")
            
            if not futures_premium:
                self.logger.warning("Futures premium calculation failed, using fallback")
                futures_premium = {
                    'premiums': {},
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("futures_premium: FAILED")
            else:
                validations.append("futures_premium: OK")
            
            if not smi_data:
                self.logger.warning("Smart money index calculation failed, using fallback")
                smi_data = {
                    'index': 50.0,
                    'signals': [],
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("smart_money_index: FAILED")
            else:
                validations.append("smart_money_index: OK")
            
            if not whale_data:
                self.logger.warning("Whale activity calculation failed, using fallback")
                whale_data = {
                    'whale_activity': {},
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("whale_activity: FAILED")
            else:
                validations.append("whale_activity: OK")
            
            if not performance:
                self.logger.warning("Performance metrics calculation failed, using fallback")
                performance = {
                    'metrics': {},
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("performance_metrics: FAILED")
            else:
                validations.append("performance_metrics: OK")
                
            if not beta_analysis:
                self.logger.warning("Bitcoin Beta Analysis calculation failed, using fallback")
                beta_analysis = {
                    'beta_analysis': {},
                    'alpha_opportunities': [],
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("bitcoin_beta_analysis: FAILED")
            else:
                validations.append("bitcoin_beta_analysis: OK")
                
            if not top_performers:
                self.logger.warning("Top performers calculation failed, using fallback")
                top_performers = {
                    'gainers': [],
                    'losers': [],
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                validations.append("top_performers: FAILED")
            else:
                validations.append("top_performers: OK")
                
            self.logger.info(f"Component validations: {', '.join(validations)}")
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['validation'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Report Compilation
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Compiling Final Report")
            # Compile report
            report = {
                'timestamp': int(datetime.now().timestamp() * 1000),
                'generated_at': datetime.now().isoformat(),
                'market_overview': market_overview,
                'futures_premium': futures_premium,
                'smart_money_index': smi_data,
                'whale_activity': whale_data,
                'performance_metrics': performance,
                'bitcoin_beta_analysis': beta_analysis,
                'top_performers': top_performers
            }
            
            # Calculate report size for logging
            report_size = len(json.dumps(report))
            self.logger.info(f"Report compiled: {report_size} bytes, {len(report)} sections")
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['compilation'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Save JSON for API Access
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Saving JSON for API")
            
            # Save report to JSON for API access
            try:
                # Create necessary directories
                reports_base_dir = os.path.join(os.getcwd(), 'reports')
                reports_json_dir = os.path.join(reports_base_dir, 'json')
                reports_html_dir = os.path.join(reports_base_dir, 'html')
                reports_pdf_dir = os.path.join(reports_base_dir, 'pdf')
                
                # Create export directory for backward compatibility
                export_dir = os.path.join(os.getcwd(), 'exports', 'market_reports', 'json')
                
                # Ensure all directories exist
                os.makedirs(reports_json_dir, exist_ok=True)
                os.makedirs(reports_html_dir, exist_ok=True)
                os.makedirs(reports_pdf_dir, exist_ok=True)
                os.makedirs(export_dir, exist_ok=True)
                
                # Get epoch timestamp for consistent naming across all files
                timestamp = int(time.time())
                
                # Define consistent filenames for all report types
                json_filename = f"market_report_{timestamp}.json"
                html_filename = f"market_report_{timestamp}.html"
                pdf_filename = f"market_report_{timestamp}.pdf"
                
                # Define full paths
                reports_json_path = os.path.join(reports_json_dir, json_filename)
                reports_html_path = os.path.join(reports_html_dir, html_filename)
                reports_pdf_path = os.path.join(reports_pdf_dir, pdf_filename)
                
                # Also save in the traditional location with date-based filename for backward compatibility
                timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
                export_json_path = os.path.join(export_dir, f"market_report_{timestamp_str}.json")
                
                # Write the JSON content
                json_content = json.dumps(report, indent=2, default=str)
                
                with open(reports_json_path, 'w') as f:
                    f.write(json_content)
                
                with open(export_json_path, 'w') as f:
                    f.write(json_content)
                    
                self.logger.info(f"Market report JSON saved to {reports_json_path} and {export_json_path}")
                
                # Add paths to report for reference
                report['json_path'] = reports_json_path
                report['export_json_path'] = export_json_path
                report['html_path'] = reports_html_path
                report['pdf_path'] = reports_pdf_path
                report['timestamp'] = timestamp
                
            except Exception as e:
                self.logger.error(f"Error saving JSON report: {str(e)}")
                
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['json_export'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Performance Metrics
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Logging Performance Metrics")
            # Log performance metrics
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)
            await self._log_performance_metrics()
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['performance_logging'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Report Validation and Alerts
            section_start = time.time()
            self.logger.info("-" * 60)
            self.logger.info("REPORT SECTION: Validation and Alerts")
            # Validate report
            validation = self._validate_report_data(report)
            
            # Determine if all required sections are present
            required_sections = [
                'market_overview', 
                'futures_premium', 
                'smart_money_index',
                'whale_activity',
                'performance_metrics',
                'bitcoin_beta_analysis'
            ]
            all_sections_valid = all(section in report and report[section] for section in required_sections)
            
            # Set a quality score based on validation results
            # (100 if all sections are valid, or lower based on missing sections)
            quality_score = 100 if all_sections_valid else 100 - (len(required_sections) - sum(1 for s in required_sections if s in report and report[s])) * 20
            report['quality_score'] = quality_score
            
            if not all_sections_valid:
                self.logger.error(f"Report validation failed: missing required sections")
                self._log_error('report_validation', 'Invalid report generated')
                await self._send_alert(
                    "report_validation",
                    f"Market report validation failed: missing required sections",
                    "error"
                )
            else:
                self.logger.info(f"Report validation passed with quality score: {quality_score}")
                # Check for alert conditions
                await self._check_and_alert_conditions(report)
            
            section_end = time.time()
            section_duration = section_end - section_start
            section_times['report_validation'] = section_duration
            self.logger.info(f"Section completed in {section_duration:.3f}s")
            
            # SECTION: Summary
            total_duration = time.time() - start_time
            self.logger.info("-" * 60)
            self.logger.info("REPORT GENERATION SUMMARY")
            self.logger.info(f"Total time: {total_duration:.3f}s")
            
            # Display section timings
            for section, duration in section_times.items():
                percentage = (duration / total_duration) * 100
                self.logger.info(f"  - {section}: {duration:.3f}s ({percentage:.1f}%)")
            
            return report
            
        except Exception as e:
            self._log_error('report_generation', str(e))
            self.logger.error(f"Error generating market summary: {str(e)}")
            self.logger.error(f"Stack trace:\n{traceback.format_exc()}")
            if self.alert_manager:
                await self._send_alert(
                    "report_generation",
                    f"Failed to generate market report: {str(e)}",
                    "error"
                )
            return None

    def _normalize_report_data(self, report: dict) -> dict:
        """Normalize and validate report data to ensure consistency for templates.
        
        This method ensures all expected fields exist with proper data types and formats,
        especially for template rendering.
        
        Args:
            report: Raw report data
            
        Returns:
            Normalized report data with fallbacks for missing fields
        """
        report = report.copy()  # Create a copy to avoid modifying the original
        
        # Process timestamp and add formatted versions for templates
        if 'timestamp' in report:
            timestamp = report['timestamp']
            # Convert string timestamp to int if needed
            if isinstance(timestamp, str):
                try:
                    timestamp = int(timestamp)
                    report['timestamp'] = timestamp
                except ValueError:
                    timestamp = int(time.time())
                    report['timestamp'] = timestamp
                    
            # Ensure timestamp is in milliseconds for consistency
            if timestamp < 10000000000:  # If in seconds, convert to milliseconds
                timestamp = timestamp * 1000
                report['timestamp'] = timestamp
                
            # Add properly formatted date strings for templates
            dt = datetime.fromtimestamp(timestamp / 1000)
            report['report_date'] = dt.strftime("%Y-%m-%d %H:%M:%S UTC")
            report['formatted_date'] = dt.strftime("%B %d, %Y")
            report['formatted_time'] = dt.strftime("%H:%M:%S UTC")
            report['year'] = dt.year  # Used in footer
        else:
            # Add current timestamp if missing
            timestamp = int(time.time() * 1000)
            report['timestamp'] = timestamp
            dt = datetime.now()
            report['report_date'] = dt.strftime("%Y-%m-%d %H:%M:%S UTC")
            report['formatted_date'] = dt.strftime("%B %d, %Y")
            report['formatted_time'] = dt.strftime("%H:%M:%S UTC")
            report['year'] = dt.year
        
        # Standard fallbacks for missing sections or fields
        fallbacks = {
            'market_overview': {
                'regime': 'NEUTRAL',
                'trend_strength': '0.0%',
                'daily_change': 0.0,
                'weekly_change': 0.0,
                'btc_price': 0.0,
                'eth_price': 0.0,
                'total_volume': 0,
                'btc_dominance': '0.0%',
                'volatility': 'LOW',
                'strength': 50.0,
                'volume_change': 0.0,
            },
            'futures_premium': {
                'average_premium': '0.0%',
                'contango_status': 'NEUTRAL',
                'max_premium': '0.0%',
                'min_premium': '0.0%',
                'data': []
            },
            'smart_money_index': {
                'index': 50.0,
                'sentiment': 'NEUTRAL',
                'institutional_flow': '+0.0%',
                'funding_rates_classification': 'NEUTRAL',
                'key_zones': []
            },
            'whale_activity': {
                'has_significant_activity': False,
                'significant_activity': {},
                'large_transactions': []
            },
            'performance_metrics': {
                'api_latency': {'avg': 0, 'max': 0, 'p95': 0},
                'error_rate': {'total': 0, 'by_type': {}, 'errors_per_minute': 0},
                'data_quality': {'avg_score': 100, 'min_score': 100},
                'processing_time': {'avg': 0, 'max': 0}
            }
        }
        
        # Ensure all sections exist with proper formatting and nested fields
        for key, fallback in fallbacks.items():
            if key not in report or not isinstance(report[key], dict):
                self.logger.warning(f"Missing or invalid section '{key}' in report, using fallback")
                report[key] = fallback.copy()
            else:
                # Ensure all expected fields exist in each section
                for subkey, value in fallback.items():
                    if subkey not in report[key]:
                        self.logger.debug(f"Adding missing field '{key}.{subkey}' with fallback value")
                        report[key][subkey] = value
        
        # Special handling for market regime to ensure consistent classification
        if 'market_overview' in report and 'regime' in report['market_overview']:
            # Normalize regime values to one of the standard classifications
            regime = report['market_overview']['regime'].upper()
            if regime in ['BULL', 'BULLISH', 'UP', 'UPTREND']:
                report['market_overview']['regime'] = 'BULLISH'
            elif regime in ['BEAR', 'BEARISH', 'DOWN', 'DOWNTREND']:
                report['market_overview']['regime'] = 'BEARISH'
            elif regime in ['NEUTRAL', 'SIDEWAYS']:
                report['market_overview']['regime'] = 'NEUTRAL'
            elif regime in ['RANGE', 'RANGING', 'CONSOLIDATION']:
                report['market_overview']['regime'] = 'RANGING'
            else:
                report['market_overview']['regime'] = 'NEUTRAL'
                
            # Add descriptive regime text for templates
            regime_map = {
                'BULLISH': 'Bullish market conditions with upward momentum',
                'BEARISH': 'Bearish market conditions with downward pressure',
                'NEUTRAL': 'Neutral market conditions with balanced forces',
                'RANGING': 'Ranging market with defined support and resistance'
            }
            report['market_overview']['regime_description'] = regime_map.get(
                report['market_overview']['regime'], 'Neutral market conditions'
            )
            
        # Ensure additional sections exist for template compatibility
        if 'additional_sections' not in report:
            report['additional_sections'] = {}
            
        # Add derived market sentiment section based on available data
        if 'market_sentiment' not in report['additional_sections']:
            # Create market sentiment summary from existing data
            market_regime = report['market_overview'].get('regime', 'NEUTRAL')
            volume_change = report['market_overview'].get('volume_change', 0)
            funding_rates = report['smart_money_index'].get('funding_rates_classification', 'NEUTRAL')
            
            report['additional_sections']['market_sentiment'] = {
                'overall': "Bullish" if market_regime == 'BULLISH' else 
                           "Bearish" if market_regime == 'BEARISH' else 
                           "Ranging" if market_regime == 'RANGING' else "Neutral",
                'volume_sentiment': "Increasing" if volume_change > 0 else "Decreasing",
                'funding_rates': funding_rates,
                'btc_support': report['market_overview'].get('btc_support', 0),
                'btc_resistance': report['market_overview'].get('btc_resistance', 0)
            }
            
        # Add futures premium analysis if not present
        if 'futures_premium_analysis' not in report['additional_sections']:
            report['additional_sections']['futures_premium_analysis'] = {
                'btc_premium': report['futures_premium'].get('average_premium', '0.00%'),
                'contango_status': report['futures_premium'].get('contango_status', 'NEUTRAL'),
                'funding_situation': f"Current funding rates are {report['smart_money_index'].get('funding_rates_classification', 'neutral').lower()}"
            }
            
        # Add template flags to control conditional sections
        report['has_smart_money_data'] = report['smart_money_index']['index'] != 50.0 or len(report['smart_money_index'].get('key_zones', [])) > 0
        report['has_whale_activity'] = report['whale_activity']['has_significant_activity']
        report['has_futures_data'] = len(report['futures_premium'].get('data', [])) > 0
        
        return report

    async def generate_market_report(self, report_data: Dict[str, Any]) -> bool:
        """Generate a complete market report.
        
        Args:
            report_data: The report data to use for generation.
            
        Returns:
            bool: True if report generation was successful, False otherwise.
        """
        try:
            # Check if core analytical sections are missing. If so, generate them.
            required_analytical_sections = [
                'market_overview', 
                'futures_premium', 
                'smart_money_index',
                'whale_activity',
                'performance_metrics' # This one is often calculated by generate_market_summary
            ]
            
            missing_analytical = any(section not in report_data for section in required_analytical_sections)
            
            if missing_analytical:
                self.logger.info("Core analytical sections missing in input report_data. Attempting to generate them now via generate_market_summary().")
                analytical_summary = await self.generate_market_summary()
                if analytical_summary:
                    # Merge the generated summary into the provided report_data
                    # The summary data should take precedence for these sections.
                    report_data.update(analytical_summary)
                    self.logger.info("Successfully generated and merged analytical summary.")
                else:
                    self.logger.error("Failed to generate analytical summary internally. Report may be incomplete.")
                    # Proceeding, but _validate_report_data will likely use fallbacks for these.

            # Validate and normalize the report data
            validated_data = self._validate_report_data(report_data)
            
            # Log detailed debug information
            self.logger.debug(f"Generating market report with validated data")
            self.logger.debug(f"Market data keys: {list(validated_data.keys())}")
            self.logger.debug(f"Data validation complete - generating JSON and PDF outputs")
            
            # Create detailed debug log for troubleshooting
            if self.logger.level <= logging.DEBUG:
                try:
                    debug_json = json.dumps(validated_data, indent=2, default=str)
                    debug_log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "logs", "debug")
                    os.makedirs(debug_log_dir, exist_ok=True)
                    debug_log_path = os.path.join(debug_log_dir, f"market_report_debug_{int(time.time())}.json")
                    with open(debug_log_path, 'w') as f:
                        f.write(debug_json)
                    self.logger.debug(f"Wrote debug market data to {debug_log_path}")
                except Exception as debug_err:
                    self.logger.debug(f"Failed to write debug log: {str(debug_err)}")
            
            # Generate PDF report
            pdf_path = await self.generate_market_pdf_report(validated_data)
            if not pdf_path:
                self.logger.error("Failed to generate PDF report")
                return False
                
            # Generate JSON report for API access
            timestamp = validated_data.get('timestamp', int(time.time()))
            json_saved = await self._save_report_json(validated_data, timestamp)
            if not json_saved:
                self.logger.error("Failed to save report JSON")
                return False
                
            # Report successful generation
            self.logger.info(f"Market report PDF generated at {pdf_path}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to generate market report: {str(e)}")
            traceback.print_exc()
            return False

    def _log_template_error(self, error_message: str, template_path: str, data: Dict[str, Any]) -> None:
        """Log detailed errors related to template rendering to help diagnose issues.
        
        Args:
            error_message: The error message
            template_path: Path to the template that failed
            data: The data that was being rendered
        """
        # Log the basic error
        self.logger.error(f"Template error: {error_message}")
        
        # Try to extract useful information about the template
        try:
            # Verify template exists
            if not os.path.exists(template_path):
                self.logger.error(f"Template file not found: {template_path}")
                
                # Check template directory
                template_dir = os.path.dirname(template_path)
                if not os.path.exists(template_dir):
                    self.logger.error(f"Template directory does not exist: {template_dir}")
                else:
                    # List available templates
                    templates = [f for f in os.listdir(template_dir) if f.endswith('.html')]
                    self.logger.info(f"Available templates in {template_dir}: {templates}")
            else:
                # Get template size
                template_size = os.path.getsize(template_path)
                self.logger.info(f"Template exists: {template_path}, size: {template_size} bytes")
                
                # Check template content (first few lines)
                try:
                    with open(template_path, 'r') as f:
                        first_lines = [next(f) for _ in range(10)]
                        self.logger.debug(f"Template first 10 lines: {first_lines}")
                except Exception as read_err:
                    self.logger.error(f"Error reading template: {str(read_err)}")
        except Exception as template_check_err:
            self.logger.error(f"Error checking template: {str(template_check_err)}")
        
        # Log data structure for debugging
        try:
            # Log top-level keys
            self.logger.info(f"Data keys: {list(data.keys())}")
            
            # Specifically check for keys used in market_report_dark.html
            required_template_keys = [
                'timestamp', 'report_date', 'market_overview', 'futures_premium', 
                'smart_money_index', 'whale_activity', 'performance_metrics'
            ]
            
            missing_keys = [k for k in required_template_keys if k not in data]
            if missing_keys:
                self.logger.error(f"Missing required template keys: {missing_keys}")
                
            # Log specific nested keys that might be referenced in the template
            if 'market_overview' in data:
                market_keys = list(data['market_overview'].keys())
                self.logger.info(f"market_overview keys: {market_keys}")
                
                # Check specific fields that might cause template errors
                if 'regime' in data['market_overview']:
                    self.logger.info(f"market_overview.regime = {data['market_overview']['regime']}")
                    
            # Similar checks for other key sections
            for section in ['futures_premium', 'smart_money_index', 'whale_activity']:
                if section in data and isinstance(data[section], dict):
                    section_keys = list(data[section].keys())
                    self.logger.info(f"{section} keys: {section_keys}")
        except Exception as data_check_err:
            self.logger.error(f"Error checking data structure: {str(data_check_err)}")
            
        # Add stack trace for context
        self.logger.error(traceback.format_exc())

    async def _save_report_json(self, report_data: Dict[str, Any], timestamp: int) -> bool:
        """Save the market report data to a JSON file.
        
        Args:
            report_data: The report data to save.
            timestamp: The timestamp to use in the filename.
            
        Returns:
            bool: True if saved successfully, False otherwise.
        """
        try:
            # Create reports JSON directory
            reports_json_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "reports", "json")
            os.makedirs(reports_json_dir, exist_ok=True)
            
            # Create exports JSON directory
            exports_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "exports", "market_reports", "json")
            os.makedirs(exports_dir, exist_ok=True)
            
            # Validate timestamp to prevent "year out of range" errors
            # First ensure timestamp is an integer
            try:
                timestamp = int(timestamp)
            except (ValueError, TypeError):
                self.logger.warning(f"Invalid timestamp format: {timestamp}, using current time")
                timestamp = int(time.time())
                
            # Check if timestamp is in milliseconds and convert to seconds if needed
            # (Unix timestamps in milliseconds are typically 13 digits for current dates)
            if timestamp > 10000000000:  # Likely milliseconds timestamp
                timestamp_seconds = timestamp // 1000  # Convert ms to seconds
            else:
                timestamp_seconds = timestamp
                
            # Verify timestamp is within reasonable range
            current_time = int(time.time())
            if timestamp_seconds < 1000000000 or timestamp_seconds > current_time + 86400:  # Before ~2001 or more than 1 day in future
                self.logger.warning(f"Timestamp outside reasonable range: {timestamp_seconds}, using current time")
                timestamp_seconds = current_time
                
            # Create filenames
            json_filename = f"market_report_{timestamp}.json"
            
            # Create readable datetime for export filename with validation
            try:
                dt = datetime.fromtimestamp(timestamp_seconds)
                export_filename = f"market_report_{dt.strftime('%Y%m%d_%H%M%S')}.json"
            except (ValueError, OSError, OverflowError) as e:
                self.logger.warning(f"Error converting timestamp to datetime: {e}")
                export_filename = f"market_report_{int(time.time())}.json"
            
            # Save to reports directory
            json_path = os.path.join(reports_json_dir, json_filename)
            with open(json_path, 'w') as f:
                json.dump(report_data, f, indent=2, default=str)
                
            # Save to exports directory
            export_path = os.path.join(exports_dir, export_filename)
            with open(export_path, 'w') as f:
                json.dump(report_data, f, indent=2, default=str)
                
            self.logger.info(f"Market report JSON saved to {json_path} and {export_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to save report JSON: {str(e)}")
            traceback.print_exc()
            return False
    
    def _get_last_friday_of_month(self, year: int, month: int) -> datetime:
        """Get the last Friday of a given month and year."""
        # Get the last day of the month
        if month == 12:
            last_day = datetime(year, 12, 31)
        else:
            last_day = datetime(year, month + 1, 1) - timedelta(days=1)
        
        # Find the last Friday
        offset = (4 - last_day.weekday()) % 7  # Friday is 4
        last_friday = last_day - timedelta(days=offset) if offset != 0 else last_day
        return last_friday
        
    def _calculate_months_to_expiry(self, expiry_month: int, pattern: str) -> int:
        """Calculate months to expiry for a futures contract."""
        current_month = datetime.now().month
        current_year = datetime.now().year
        pattern_year = int(pattern[-2:]) + 2000  # Extract year from pattern and convert to full year
        
        if pattern_year > current_year:
            # Contract expires next year
            return (expiry_month + 12) - current_month
        else:
            # Contract expires this year
            return max(1, expiry_month - current_month)  # Ensure at least 1 month
            
    async def _check_quarterly_futures(self):
        """Test method to check if quarterly futures symbols are valid.
        This is a helper method for debugging symbol format issues.
        """
        try:
            # Test with common assets
            symbols = ['BTC', 'ETH', 'SOL', 'XRP', 'AVAX']
            
            for base_asset in symbols:
                current_year = datetime.now().year % 100
                current_month = datetime.now().month
                year = datetime.now().year
                
                # Get clean base asset
                base_asset_clean = base_asset.strip()
                
                # Try unified formats first (MMDD format)
                unified_quarterly_patterns = []
                
                # June quarterly
                if current_month <= 6:
                    june_date = self._get_last_friday_of_month(year, 6)
                    june_pattern = f"{base_asset_clean}USDT{june_date.month:02d}{june_date.day:02d}"
                    unified_quarterly_patterns.append(june_pattern)
                
                # September quarterly
                if current_month <= 9:
                    sept_date = self._get_last_friday_of_month(year, 9)
                    sept_pattern = f"{base_asset_clean}USDT{sept_date.month:02d}{sept_date.day:02d}"
                    unified_quarterly_patterns.append(sept_pattern)
                
                # December quarterly
                dec_date = self._get_last_friday_of_month(year, 12)
                dec_pattern = f"{base_asset_clean}USDT{dec_date.month:02d}{dec_date.day:02d}"
                unified_quarterly_patterns.append(dec_pattern)
                
                # Old inverse patterns
                inverse_quarterly_patterns = [
                    f"{base_asset_clean}USDM{current_year}",
                    f"{base_asset_clean}USDU{current_year}",
                    f"{base_asset_clean}USDZ{current_year}"
                ]
                
                # Old USDT patterns with hyphens
                usdt_quarterly_patterns = []
                
                if current_month <= 6:
                    june_date = self._get_last_friday_of_month(year, 6)
                    usdt_quarterly_patterns.append(f"{base_asset_clean}USDT-{june_date.day}JUN{current_year}")
                
                if current_month <= 9:
                    sept_date = self._get_last_friday_of_month(year, 9)
                    usdt_quarterly_patterns.append(f"{base_asset_clean}USDT-{sept_date.day}SEP{current_year}")
                
                dec_date = self._get_last_friday_of_month(year, 12)
                usdt_quarterly_patterns.append(f"{base_asset_clean}USDT-{dec_date.day}DEC{current_year}")
                
                # Test all pattern formats
                all_patterns = unified_quarterly_patterns + inverse_quarterly_patterns + usdt_quarterly_patterns
                print(f"Testing patterns for {base_asset}: {all_patterns}")
                
                # Try to fetch ticker data for each pattern
                for pattern in all_patterns:
                    try:
                        if hasattr(self, 'exchange') and self.exchange:
                            ticker = await self.exchange.fetch_ticker(pattern)
                            if ticker:
                                print(f"✅ Found valid quarterly future: {pattern}")
                        else:
                            print(f"⚠️ Exchange not available to test {pattern}")
                    except Exception as e:
                        print(f"❌ Pattern {pattern} invalid: {str(e)}")
                        
        except Exception as e:
            print(f"Error in test: {str(e)}")

    async def _calculate_futures_premium(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate futures premium metrics with comprehensive analysis and improved reliability."""
        try:
            # Prefetch all markets data once and cache it with longer expiration
            cache_key = "all_markets"
            if cache_key not in self.cache or time.time() > self.cache.get(f"{cache_key}_expiry", 0):
                self.logger.info("Fetching and caching all markets data for futures premium calculation")
                try:
                    if hasattr(self.exchange, 'load_markets'):
                        all_markets = await self._fetch_with_retry('load_markets', timeout=10)
                    else:
                        all_markets = {}
                    self.cache[cache_key] = all_markets
                    # Cache for 15 minutes (900 seconds)
                    self.cache[f"{cache_key}_expiry"] = time.time() + 900
                    self.logger.debug(f"Successfully cached {len(all_markets)} markets")
                except Exception as e:
                    self.logger.warning(f"Failed to prefetch all markets: {e}")
                    # Try to use existing cache even if expired
                    all_markets = self.cache.get(cache_key, {})
            else:
                all_markets = self.cache[cache_key]
                self.logger.debug(f"Using cached markets data ({len(all_markets)} markets)")
            
            # Process symbols concurrently for better performance
            self.logger.info(f"Processing {len(symbols)} symbols for futures premium with parallel execution")
            tasks = [self._calculate_single_premium(symbol, all_markets) for symbol in symbols]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Process results
            premiums = {}
            failed_symbols = []
            
            # For term structure analysis
            quarterly_futures = {}
            funding_rates = {}
            average_premium = 0.0
            valid_premiums = 0
            
            for symbol, result in zip(symbols, results):
                if isinstance(result, Exception):
                    self.logger.warning(f"Error calculating futures premium for {symbol}: {str(result)}")
                    failed_symbols.append(symbol)
                elif result is None:
                    self.logger.warning(f"No valid premium data for {symbol}")
                    failed_symbols.append(symbol)
                else:
                    premiums[symbol] = result
                    
                    # Track average premium and count valid results
                    if 'premium_value' in result:
                        average_premium += result['premium_value']
                        valid_premiums += 1
                        
                    # Store quarterly futures data if available
                    if 'futures_contracts' in result and result['futures_contracts']:
                        quarterly_futures[symbol] = result['futures_contracts']
                        
                    # Get funding rate data
                    try:
                        clean_symbol = symbol.replace('/', '')
                        if clean_symbol.endswith(':USDT'):
                            bybit_symbol = clean_symbol
                        else:
                            bybit_symbol = clean_symbol
                        funding_data = await self._analyze_funding_rates(bybit_symbol)
                        funding_rates[symbol] = funding_data
                    except Exception as e:
                        self.logger.debug(f"Error getting funding data for {symbol}: {e}")
            
            # Calculate average and determine market status
            if valid_premiums > 0:
                average_premium = average_premium / valid_premiums
                
                # Determine overall contango status
                if average_premium > 0.1:
                    contango_status = "CONTANGO"
                elif average_premium < -0.1:
                    contango_status = "BACKWARDATION"
                else:
                    contango_status = "NEUTRAL"
            else:
                average_premium = 0.0
                contango_status = "NEUTRAL"
            
            result = {
                'premiums': premiums,
                'quarterly_futures': quarterly_futures,
                'funding_rates': funding_rates,
                'average_premium': f"{average_premium:.4f}%",
                'average_premium_value': average_premium,
                'contango_status': contango_status,
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            # Add data quality metrics
            if failed_symbols:
                result['failed_symbols'] = failed_symbols
                result['data_quality_note'] = f"Failed for {len(failed_symbols)}/{len(symbols)} symbols"
                
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating futures premium: {str(e)}")
            return {
                'premiums': {},
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }
    async def _calculate_single_premium(self, symbol: str, all_markets: Dict) -> Optional[Dict[str, Any]]:
        """Calculate futures premium for a single symbol with comprehensive analysis."""
        try:
            # Clean up the symbol format for Bybit API
            clean_symbol = symbol.replace('/', '')
            
            # For Bybit, we need to use the perp and spot symbols correctly
            if clean_symbol.endswith(':USDT'):
                # Already in Bybit format (e.g., 'BTCUSDT:USDT')
                perp_symbol = clean_symbol
                spot_symbol = clean_symbol.replace(':USDT', '')
            elif '/' in symbol:
                # Convert from ccxt format to Bybit format
                perp_symbol = symbol.replace('/', '') 
                spot_symbol = perp_symbol
            else:
                # Already in simple format (e.g., 'BTCUSDT')
                perp_symbol = clean_symbol
                spot_symbol = clean_symbol
            
            # Get base asset (BTC, ETH, etc.) from the symbol
            base_asset = spot_symbol.replace('USDT', '')
            
            # Note: We proceed with basic perpetual vs spot analysis for all symbols
            # Inverse/quarterly futures analysis will be filtered separately below
            
            self.logger.debug(f"Calculating futures premium for {symbol} (perp: {perp_symbol}, spot: {spot_symbol})")
            
            # Fetch perpetual and spot tickers concurrently with timeout
            try:
                perp_ticker_task = self._fetch_with_retry('fetch_ticker', perp_symbol, timeout=5)
                spot_ticker_task = self._fetch_with_retry('fetch_ticker', spot_symbol, timeout=5)
                perp_ticker, spot_ticker = await asyncio.gather(
                    perp_ticker_task, 
                    spot_ticker_task, 
                    return_exceptions=True
                )
                
                # Handle exceptions in ticker fetching
                if isinstance(perp_ticker, Exception):
                    self.logger.warning(f"Error fetching perpetual ticker for {perp_symbol}: {str(perp_ticker)}")
                    try:
                        alt_symbol = perp_symbol.replace('USDT:USDT', 'USDT')
                        perp_ticker = await self._fetch_with_retry('fetch_ticker', alt_symbol, timeout=5)
                        self.logger.info(f"Successfully fetched perpetual ticker using alternative format: {alt_symbol}")
                    except Exception as e:
                        self.logger.warning(f"Alternative format also failed: {str(e)}")
                        perp_ticker = None
                
                if isinstance(spot_ticker, Exception):
                    self.logger.warning(f"Error fetching spot ticker for {spot_symbol}: {str(spot_ticker)}")
                    try:
                        alt_symbol = spot_symbol.replace('USDT', '/USDT')
                        spot_ticker = await self._fetch_with_retry('fetch_ticker', alt_symbol, timeout=5)
                        self.logger.info(f"Successfully fetched spot ticker using alternative format: {alt_symbol}")
                    except Exception as e:
                        self.logger.warning(f"Alternative spot format also failed: {str(e)}")
                        spot_ticker = None
                        
            except Exception as e:
                self.logger.warning(f"Error in concurrent ticker fetching for {symbol}: {e}")
                perp_ticker = None
                spot_ticker = None
            
            # Extract prices from different possible structures using the enhanced extraction method
            mark_price = self._extract_bybit_field(perp_ticker, 'mark_price') if perp_ticker else 0
            index_price = self._extract_bybit_field(perp_ticker, 'index_price') if perp_ticker else 0
            last_price = self._extract_bybit_field(perp_ticker, 'last') if perp_ticker else 0
            
            # If mark price not found, use last price
            if not mark_price and perp_ticker:
                mark_price = float(perp_ticker.get('last', 0))
            
            # If index price not found, try to use spot price
            if not index_price and spot_ticker:
                index_price = float(spot_ticker.get('last', 0))
            
            # Get comprehensive quarterly futures data
            futures_price = 0
            futures_basis = 0
            weekly_futures_found = 0
            quarterly_futures_found = 0
            futures_contracts = []
            
            # Enhanced quarterly futures lookup with multiple exchange formats
            # Only check for quarterly/inverse futures if this symbol has inverse contracts
            try:
                if self._has_derivatives_market(symbol):
                    self.logger.debug(f"Checking quarterly futures for {symbol} - has inverse derivatives market")
                    
                    # Get current year and month
                    current_year = datetime.now().year 
                    current_month = datetime.now().month
                    current_year_short = current_year % 100  # Last two digits (e.g., 25 for 2025)
                    
                    # Get base asset and check if it needs special formatting
                    base_asset_clean = base_asset.strip()
                
                    # Function to calculate last Friday of a month
                    def get_last_friday(year, month):
                        if month == 12:
                            last_day = datetime(year, 12, 31)
                        else:
                            last_day = datetime(year, month + 1, 1) - timedelta(days=1)
                        
                        # Find the last Friday (weekday 4)
                        offset = (4 - last_day.weekday()) % 7
                        last_friday = last_day - timedelta(days=offset) if offset != 0 else last_day
                        return last_friday
                    
                    # Try multiple quarterly futures formats
                    quarterly_patterns = []
                    
                    # 1. Binance CCXT format (BASE/USDT:USDT-YYYYMMDD)
                    for month in [3, 6, 9, 12]:  # Quarterly months
                        if month >= current_month or month == 3:  # Include next year's March if current month > 9
                            year = current_year if month >= current_month else current_year + 1
                            last_friday = get_last_friday(year, month)
                            expiry_date = last_friday.strftime('%Y%m%d')
                            quarterly_patterns.append(f"{base_asset_clean}/USDT:USDT-{expiry_date}")
                    
                    # 2. Bybit unified format (BASEUSDT-MMDD)
                    for month in [3, 6, 9, 12]:
                        if month >= current_month or month == 3:
                            year = current_year if month >= current_month else current_year + 1
                            last_friday = get_last_friday(year, month)
                            quarterly_patterns.append(f"{base_asset_clean}USDT{last_friday.month:02d}{last_friday.day:02d}")
                    
                    # 3. Traditional inverse patterns (BASEUSDM25, BASEUSDU25, BASEUSDZ25)
                    if current_month <= 6:
                        quarterly_patterns.append(f"{base_asset_clean}USDM{current_year_short}")
                    if current_month <= 9:
                        quarterly_patterns.append(f"{base_asset_clean}USDU{current_year_short}")
                    quarterly_patterns.append(f"{base_asset_clean}USDZ{current_year_short}")
                    
                    # 4. Bybit USDT quarterly with hyphens (BTCUSDT-29MAR25)
                    month_codes = {3: 'MAR', 6: 'JUN', 9: 'SEP', 12: 'DEC'}
                    for month in [3, 6, 9, 12]:
                        if month >= current_month or month == 3:
                            year = current_year if month >= current_month else current_year + 1
                            last_friday = get_last_friday(year, month)
                            year_short = year % 100
                            quarterly_patterns.append(f"{base_asset_clean}USDT-{last_friday.day}{month_codes[month]}{year_short}")
                    
                    # Try each pattern with circuit breaker
                    max_errors_before_skip = 3
                    current_errors = self.error_counts.get('quarterly_futures_errors', 0)
                    
                    if current_errors < max_errors_before_skip:
                        for pattern in quarterly_patterns[:6]:  # Limit to first 6 patterns for performance
                            try:
                                quarterly_ticker = await self._fetch_with_retry('fetch_ticker', pattern, timeout=3)
                                if quarterly_ticker and quarterly_ticker.get('last'):
                                    quarterly_price = float(quarterly_ticker['last'])
                                    self.logger.info(f"Found quarterly futures: {pattern} = {quarterly_price}")
                                    
                                    if index_price and index_price > 0:
                                        quarterly_basis = ((quarterly_price - index_price) / index_price) * 100
                                        
                                        # Calculate months to expiry for annualized basis
                                        if 'MAR' in pattern or 'JUN' in pattern or 'SEP' in pattern or 'DEC' in pattern:
                                            # Extract month from pattern
                                            if 'MAR' in pattern:
                                                expiry_month = 3
                                            elif 'JUN' in pattern:
                                                expiry_month = 6
                                            elif 'SEP' in pattern:
                                                expiry_month = 9
                                            else:
                                                expiry_month = 12
                                        elif 'M' in pattern:
                                            expiry_month = 6
                                        elif 'U' in pattern:
                                            expiry_month = 9
                                        elif 'Z' in pattern:
                                            expiry_month = 12
                                        else:
                                            expiry_month = 12  # Default
                                        
                                        months_to_expiry = self._calculate_months_to_expiry(expiry_month, pattern)
                                        annualized_basis = quarterly_basis * (12 / max(1, months_to_expiry))
                                        
                                        futures_contracts.append({
                                            'symbol': pattern,
                                            'price': quarterly_price,
                                            'basis': f"{quarterly_basis:.4f}%",
                                            'annualized_basis': f"{annualized_basis:.4f}%",
                                            'months_to_expiry': months_to_expiry,
                                            'volume': quarterly_ticker.get('baseVolume', 0)
                                        })
                                        
                                        quarterly_futures_found += 1
                                        
                                        # Use first found quarterly as the primary futures price
                                        if not futures_price:
                                            futures_price = quarterly_price
                                            futures_basis = f"{quarterly_basis:.4f}%"
                                            
                                    break  # Found a working format, no need to try others
                            except Exception as e:
                                self.error_counts['quarterly_futures_errors'] = self.error_counts.get('quarterly_futures_errors', 0) + 1
                                self.logger.debug(f"Error fetching quarterly future {pattern}: {e}")
                    else:
                        self.logger.info(f"Skipping quarterly futures lookup for {symbol} - circuit breaker active ({current_errors} errors)")
                else:
                    self.logger.debug(f"Skipping quarterly futures lookup for {symbol} - no inverse derivatives market")
                    
                # Sort futures contracts by expiry
                futures_contracts.sort(key=lambda x: x.get('months_to_expiry', 12))
                
            except Exception as e:
                self.logger.debug(f"Error finding futures contracts for {base_asset}: {str(e)}")
            
            # Calculate premium if we have valid prices
            if mark_price and mark_price > 0 and (index_price and index_price > 0):
                premium = ((mark_price - index_price) / index_price) * 100
                
                # Determine premium type
                premium_type = "📉 Backwardation" if premium < 0 else "📈 Contango"
                
                # Get funding rate data if available
                funding_rate = self._extract_bybit_field(perp_ticker, 'funding_rate') if perp_ticker else 0
                
                return {
                    'premium': f"{premium:.4f}%",
                    'premium_value': premium,
                    'premium_type': premium_type,
                    'mark_price': mark_price,
                    'index_price': index_price,
                    'last_price': last_price,
                    'weekly_futures_count': weekly_futures_found,
                    'quarterly_futures_count': quarterly_futures_found,
                    'futures_price': futures_price,
                    'futures_basis': futures_basis,
                    'funding_rate': funding_rate,
                    'timestamp': int(datetime.now().timestamp() * 1000),
                    'futures_contracts': futures_contracts  # Include all quarterly contracts
                }
            else:
                self.logger.debug(f"Missing price data for futures premium: {symbol} (mark: {mark_price}, index: {index_price})")
                return None
        except Exception as e:
            self.logger.error(f"Error in _calculate_single_premium for {symbol}: {str(e)}")
            return None

    async def _calculate_smart_money_index(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate smart money index with advanced institutional flow analysis."""
        try:
            signals = []
            total_score = 0
            valid_symbols = 0
            key_zones = []
            institutional_flows = []
            
            # Limit symbols to prevent timeout and use concurrent processing
            limited_symbols = symbols[:8]  # Increased from 6 to 8 for better analysis
            
            # Create concurrent tasks for better performance
            tasks = []
            for symbol in limited_symbols:
                task = self._process_smart_money_for_symbol(symbol)
                tasks.append(task)
            
            # Execute tasks concurrently with timeout
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True), 
                    timeout=100  # Increased timeout for more comprehensive analysis
                )
                
                # Process results
                for result in results:
                    if isinstance(result, dict) and 'score' in result:
                        signals.append(result)
                        total_score += result['score']
                        valid_symbols += 1
                        
                        # Add key zones if present
                        if 'key_zones' in result:
                            key_zones.extend(result['key_zones'])
                            
                        # Add institutional flow data
                        if 'institutional_flow' in result:
                            institutional_flows.append(result['institutional_flow'])
                            
                    elif isinstance(result, Exception):
                        self.logger.debug(f"Smart money task failed: {result}")
                        
            except asyncio.TimeoutError:
                self.logger.warning("Smart money index calculation timed out, using partial results")
            except Exception as e:
                self.logger.error(f"Error in concurrent smart money processing: {e}")
            
            # Calculate overall index with enhanced weighting
            avg_score = total_score / valid_symbols if valid_symbols > 0 else 50
            
            # Calculate institutional flow metrics
            institutional_flow_score = 0
            if institutional_flows:
                # Weight by volume and order size
                weighted_flows = []
                for flow in institutional_flows:
                    if 'net_flow' in flow and 'confidence' in flow:
                        weighted_flows.append(flow['net_flow'] * flow['confidence'])
                
                if weighted_flows:
                    institutional_flow_score = sum(weighted_flows) / len(weighted_flows)
            
            # Combine technical and flow analysis
            combined_score = (avg_score * 0.6) + (institutional_flow_score * 0.4)
            
            # Determine signal based on combined score
            if combined_score > 70:
                signal = 'BULLISH'
                sentiment = 'Strong institutional buying detected'
            elif combined_score > 60:
                signal = 'BULLISH'
                sentiment = 'Moderate institutional buying'
            elif combined_score < 30:
                signal = 'BEARISH'
                sentiment = 'Strong institutional selling detected'
            elif combined_score < 40:
                signal = 'BEARISH'
                sentiment = 'Moderate institutional selling'
            else:
                signal = 'NEUTRAL'
                sentiment = 'Balanced institutional activity'
            
            # Calculate change (would need historical data for accurate calculation)
            # For now, use a simple heuristic based on score deviation from neutral
            change = (combined_score - 50) * 0.1  # Approximate change percentage
            
            # Determine funding rates classification
            funding_classification = 'NEUTRAL'
            if institutional_flow_score > 10:
                funding_classification = 'BULLISH'
            elif institutional_flow_score < -10:
                funding_classification = 'BEARISH'
            
            result = {
                'current_value': combined_score,
                'index': combined_score,  # For template compatibility
                'signal': signal,
                'sentiment': sentiment,
                'change': change,
                'institutional_flow': f"{institutional_flow_score:+.2f}%",
                'funding_rates_classification': funding_classification,
                'signals': signals,
                'key_zones': sorted(key_zones, key=lambda x: abs(x['distance_pct']))[:15],  # Top 15 zones
                'institutional_flows': institutional_flows,
                'technical_score': avg_score,
                'flow_score': institutional_flow_score,
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating smart money index: {str(e)}")
            return {
                'current_value': 50,
                'index': 50,
                'signal': 'NEUTRAL',
                'sentiment': 'Analysis unavailable',
                'change': 0,
                'institutional_flow': '+0.00%',
                'funding_rates_classification': 'NEUTRAL',
                'signals': [],
                'key_zones': [],
                'institutional_flows': [],
                'technical_score': 50,
                'flow_score': 0,
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }
    
    async def _process_smart_money_for_symbol(self, symbol: str) -> Dict[str, Any]:
        """Process smart money data for a single symbol with advanced analysis."""
        try:
            # Fix symbol format for Bybit API
            clean_symbol = symbol.replace('/', '')
            if clean_symbol.endswith(':USDT'):
                api_symbol = clean_symbol
            else:
                api_symbol = clean_symbol
            
            # Get ticker data for current price with shorter timeout
            ticker = await self._fetch_with_retry('fetch_ticker', api_symbol, timeout=5)
            if not ticker:
                return {'score': 50, 'symbol': symbol, 'error': 'No ticker data'}
                
            current_price = float(ticker['last'])
            volume_24h = float(ticker.get('baseVolume', 0))
            
            # Get order book for depth analysis with enhanced depth
            order_book = await self._fetch_with_retry('fetch_order_book', api_symbol, limit=100, timeout=8)
            if not order_book or not order_book.get('bids') or not order_book.get('asks'):
                return {'score': 50, 'symbol': symbol, 'error': 'No order book data'}
            
            # Enhanced spread analysis
            best_bid = float(order_book['bids'][0][0]) if order_book['bids'] else 0
            best_ask = float(order_book['asks'][0][0]) if order_book['asks'] else 0
            spread = ((best_ask - best_bid) / current_price) * 100 if current_price > 0 else 0
            
            # Advanced order book analysis
            bid_volume = sum(float(bid[1]) for bid in order_book['bids'][:20])  # Top 20 levels
            ask_volume = sum(float(ask[1]) for ask in order_book['asks'][:20])
            total_volume = bid_volume + ask_volume
            
            # Calculate order book imbalance
            if total_volume > 0:
                imbalance = (bid_volume - ask_volume) / total_volume
            else:
                imbalance = 0
            
            # Analyze large orders (potential institutional activity)
            large_bid_orders = [bid for bid in order_book['bids'][:50] if float(bid[1]) * float(bid[0]) > 10000]  # $10k+ orders
            large_ask_orders = [ask for ask in order_book['asks'][:50] if float(ask[1]) * float(ask[0]) > 10000]
            
            # Calculate institutional flow score
            large_bid_value = sum(float(bid[1]) * float(bid[0]) for bid in large_bid_orders)
            large_ask_value = sum(float(ask[1]) * float(ask[0]) for ask in large_ask_orders)
            total_large_value = large_bid_value + large_ask_value
            
            institutional_flow = 0
            confidence = 0
            if total_large_value > 0:
                institutional_flow = ((large_bid_value - large_ask_value) / total_large_value) * 100
                confidence = min(total_large_value / 100000, 1.0)  # Confidence based on total value
            
            # Enhanced scoring system
            score = 50  # Neutral starting point
            
            # Spread analysis (tighter spreads = more institutional activity)
            if spread < 0.02:  # Very tight spread
                score += 20
            elif spread < 0.05:  # Tight spread
                score += 15
            elif spread < 0.1:  # Moderate spread
                score += 10
            elif spread > 0.3:  # Wide spread
                score -= 15
            
            # Order book imbalance analysis
            score += imbalance * 25  # Scale imbalance to points
            
            # Large order analysis
            if len(large_bid_orders) > len(large_ask_orders):
                score += min(len(large_bid_orders) - len(large_ask_orders), 15)
            else:
                score -= min(len(large_ask_orders) - len(large_bid_orders), 15)
            
            # Volume analysis
            if volume_24h > 1000000:  # High volume
                score += 10
            elif volume_24h < 100000:  # Low volume
                score -= 5
            
            # Clamp score between 0 and 100
            score = max(0, min(100, score))
            
            result = {
                'symbol': symbol,
                'score': score,
                'spread': spread,
                'imbalance': imbalance,
                'current_price': current_price,
                'volume_24h': volume_24h,
                'large_bid_orders': len(large_bid_orders),
                'large_ask_orders': len(large_ask_orders),
                'institutional_flow': {
                    'net_flow': institutional_flow,
                    'confidence': confidence,
                    'large_bid_value': large_bid_value,
                    'large_ask_value': large_ask_value
                },
                'key_zones': []
            }
            
            # Enhanced key support/resistance zone identification
            if len(order_book['bids']) >= 10 and len(order_book['asks']) >= 10:
                # Find clusters of large orders (potential institutional levels)
                bid_clusters = self._find_order_clusters(order_book['bids'][:50], current_price, 'support')
                ask_clusters = self._find_order_clusters(order_book['asks'][:50], current_price, 'resistance')
                
                # Add significant zones
                for cluster in bid_clusters[:3]:  # Top 3 support zones
                    result['key_zones'].append({
                        'symbol': symbol,
                        'price': cluster['price'],
                        'volume': cluster['volume'],
                        'type': 'support',
                        'strength': cluster['strength'],
                        'distance_pct': ((cluster['price'] - current_price) / current_price) * 100
                    })
                
                for cluster in ask_clusters[:3]:  # Top 3 resistance zones
                    result['key_zones'].append({
                        'symbol': symbol,
                        'price': cluster['price'],
                        'volume': cluster['volume'],
                        'type': 'resistance',
                        'strength': cluster['strength'],
                        'distance_pct': ((cluster['price'] - current_price) / current_price) * 100
                    })
            
            return result
            
        except Exception as e:
            self.logger.warning(f"Error processing smart money data for {symbol}: {str(e)}")
            return {'score': 50, 'symbol': symbol, 'error': str(e)}
    
    def _find_order_clusters(self, orders: List[List[float]], current_price: float, order_type: str) -> List[Dict[str, Any]]:
        """Find clusters of large orders that indicate institutional levels."""
        try:
            clusters = []
            price_tolerance = current_price * 0.001  # 0.1% price tolerance for clustering
            
            # Group orders by similar price levels
            price_groups = {}
            for order in orders:
                price = float(order[0])
                volume = float(order[1])
                value = price * volume
                
                # Only consider significant orders
                if value > 5000:  # $5k minimum
                    # Find existing group or create new one
                    group_key = None
                    for existing_price in price_groups.keys():
                        if abs(price - existing_price) <= price_tolerance:
                            group_key = existing_price
                            break
                    
                    if group_key is None:
                        group_key = price
                        price_groups[group_key] = []
                    
                    price_groups[group_key].append({'price': price, 'volume': volume, 'value': value})
            
            # Analyze clusters
            for group_price, group_orders in price_groups.items():
                if len(group_orders) >= 2:  # At least 2 orders to form a cluster
                    total_volume = sum(order['volume'] for order in group_orders)
                    total_value = sum(order['value'] for order in group_orders)
                    avg_price = sum(order['price'] for order in group_orders) / len(group_orders)
                    
                    # Calculate strength based on volume and number of orders
                    strength = min(total_value / 50000, 10.0)  # Max strength of 10
                    
                    clusters.append({
                        'price': avg_price,
                        'volume': total_volume,
                        'value': total_value,
                        'order_count': len(group_orders),
                        'strength': strength
                    })
            
            # Sort by strength (value and volume)
            clusters.sort(key=lambda x: x['strength'], reverse=True)
            return clusters
            
        except Exception as e:
            self.logger.warning(f"Error finding order clusters: {str(e)}")
            return []

    async def _calculate_whale_activity(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate whale activity metrics with comprehensive tracking and analysis."""
        try:
            whale_transactions = []
            significant_activity = {}
            
            # Limit symbols to prevent timeout and use concurrent processing
            limited_symbols = symbols[:8]  # Increased for better coverage
            
            # Create concurrent tasks for better performance
            tasks = []
            for symbol in limited_symbols:
                task = self._process_whale_activity_for_symbol(symbol)
                tasks.append(task)
            
            # Execute tasks concurrently with timeout
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True), 
                    timeout=120  # Increased timeout for comprehensive analysis
                )
                
                # Process results
                for result in results:
                    if isinstance(result, list):
                        whale_transactions.extend(result)
                    elif isinstance(result, Exception):
                        self.logger.debug(f"Whale activity task failed: {result}")
                        
            except asyncio.TimeoutError:
                self.logger.warning("Whale activity calculation timed out, using partial results")
            except Exception as e:
                self.logger.error(f"Error in concurrent whale activity processing: {e}")
            
            # Sort by USD value descending
            whale_transactions.sort(key=lambda x: x.get('usd_value', 0), reverse=True)
            
            # Analyze significant activity by symbol
            for tx in whale_transactions:
                symbol = tx.get('symbol', 'UNKNOWN')
                if symbol not in significant_activity:
                    significant_activity[symbol] = {
                        'buy_volume': 0,
                        'sell_volume': 0,
                        'net_whale_volume': 0,
                        'transaction_count': 0,
                        'largest_transaction': 0,
                        'avg_transaction_size': 0,
                        'whale_dominance': 0,
                        'usd_value': 0
                    }
                
                usd_value = abs(tx.get('usd_value', 0))
                side = tx.get('side', 'unknown').lower()
                
                significant_activity[symbol]['transaction_count'] += 1
                significant_activity[symbol]['largest_transaction'] = max(
                    significant_activity[symbol]['largest_transaction'], 
                    usd_value
                )
                
                if side == 'buy':
                    significant_activity[symbol]['buy_volume'] += usd_value
                    significant_activity[symbol]['net_whale_volume'] += usd_value
                    significant_activity[symbol]['usd_value'] += usd_value
                elif side == 'sell':
                    significant_activity[symbol]['sell_volume'] += usd_value
                    significant_activity[symbol]['net_whale_volume'] -= usd_value
                    significant_activity[symbol]['usd_value'] += usd_value
                else:
                    significant_activity[symbol]['usd_value'] += usd_value
            
            # Calculate additional metrics for each symbol
            for symbol, activity in significant_activity.items():
                if activity['transaction_count'] > 0:
                    activity['avg_transaction_size'] = activity['usd_value'] / activity['transaction_count']
                    
                    # Calculate whale dominance (percentage of total volume that's whale activity)
                    total_volume = activity['buy_volume'] + activity['sell_volume']
                    if total_volume > 0:
                        activity['whale_dominance'] = (total_volume / (total_volume + 1000000)) * 100  # Rough estimate
            
            # Calculate overall market sentiment from whale activity
            total_whale_buy = sum(activity['buy_volume'] for activity in significant_activity.values())
            total_whale_sell = sum(activity['sell_volume'] for activity in significant_activity.values())
            total_whale_volume = total_whale_buy + total_whale_sell
            
            whale_sentiment = 'NEUTRAL'
            whale_bias = 0
            if total_whale_volume > 0:
                whale_bias = ((total_whale_buy - total_whale_sell) / total_whale_volume) * 100
                if whale_bias > 20:
                    whale_sentiment = 'BULLISH'
                elif whale_bias < -20:
                    whale_sentiment = 'BEARISH'
            
            # Determine if there's significant whale activity
            has_significant_activity = (
                len(whale_transactions) > 0 and 
                total_whale_volume > 500000  # At least $500k in whale activity
            )
            
            result = {
                'whale_activity': {
                    'transactions': whale_transactions[:25],  # Top 25 transactions
                    'significant_activity': has_significant_activity,
                    'total_volume': total_whale_volume,
                    'buy_volume': total_whale_buy,
                    'sell_volume': total_whale_sell,
                    'net_volume': total_whale_buy - total_whale_sell,
                    'whale_bias': whale_bias,
                    'whale_sentiment': whale_sentiment,
                    'count': len(whale_transactions),
                    'symbols_affected': len(significant_activity)
                },
                'significant_activity': significant_activity,
                'transactions': whale_transactions[:25],  # Keep for backward compatibility
                'has_significant_activity': has_significant_activity,
                'whale_summary': self._generate_whale_summary(significant_activity, whale_sentiment, whale_bias),
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating whale activity: {str(e)}")
            return {
                'whale_activity': {
                    'transactions': [],
                    'significant_activity': False,
                    'total_volume': 0,
                    'buy_volume': 0,
                    'sell_volume': 0,
                    'net_volume': 0,
                    'whale_bias': 0,
                    'whale_sentiment': 'NEUTRAL',
                    'count': 0,
                    'symbols_affected': 0
                },
                'significant_activity': {},
                'transactions': [],
                'has_significant_activity': False,
                'whale_summary': 'Whale activity analysis unavailable',
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }
    
    async def _process_whale_activity_for_symbol(self, symbol: str) -> List[Dict[str, Any]]:
        """Process whale activity for a single symbol with enhanced analysis."""
        try:
            # Fix symbol format for Bybit API
            clean_symbol = symbol.replace('/', '')
            if clean_symbol.endswith(':USDT'):
                api_symbol = clean_symbol
            else:
                api_symbol = clean_symbol
            
            whale_transactions = []
            
            # Get current ticker for price context
            ticker = await self._fetch_with_retry('fetch_ticker', api_symbol, timeout=5)
            current_price = float(ticker['last']) if ticker else 0
            volume_24h = float(ticker.get('baseVolume', 0)) if ticker else 0
            
            # Calculate dynamic whale threshold based on market conditions
            whale_threshold = self._calculate_dynamic_whale_threshold(symbol, current_price, volume_24h)
            
            # Get recent trades with enhanced timeout
            try:
                trades = await self._fetch_with_retry('fetch_trades', api_symbol, limit=100, timeout=10)
                
                if not trades:
                    return []
                
                # Get order book for additional context
                order_book = await self._fetch_with_retry('fetch_order_book', api_symbol, limit=50, timeout=8)
                
                # Analyze trades for whale activity
                for trade in trades:
                    try:
                        amount = float(trade.get('amount', trade.get('size', trade.get('qty', 0))))
                        price = float(trade['price'])
                        usd_value = amount * price
                        side = trade.get('side', 'unknown')
                        timestamp = trade.get('timestamp', 0)
                        
                        # Enhanced whale detection criteria
                        is_whale = False
                        whale_confidence = 0
                        
                        # Primary criteria: USD value threshold
                        if usd_value > whale_threshold:
                            is_whale = True
                            whale_confidence = min(usd_value / whale_threshold, 5.0)  # Max confidence of 5x
                        
                        # Secondary criteria: Large relative to order book
                        if order_book and not is_whale:
                            # Check if trade is large relative to order book depth
                            if side == 'buy' and order_book.get('asks'):
                                ask_depth = sum(float(ask[1]) for ask in order_book['asks'][:10])
                                if amount > ask_depth * 0.1:  # More than 10% of top 10 ask levels
                                    is_whale = True
                                    whale_confidence = 2.0
                            elif side == 'sell' and order_book.get('bids'):
                                bid_depth = sum(float(bid[1]) for bid in order_book['bids'][:10])
                                if amount > bid_depth * 0.1:  # More than 10% of top 10 bid levels
                                    is_whale = True
                                    whale_confidence = 2.0
                        
                        # Tertiary criteria: Large relative to average volume
                        if not is_whale and volume_24h > 0:
                            avg_trade_size = volume_24h / 1000  # Assume ~1000 trades per day
                            if amount > avg_trade_size * 10:  # 10x average trade size
                                is_whale = True
                                whale_confidence = 1.5
                        
                        if is_whale:
                            # Calculate market impact score
                            market_impact = self._calculate_market_impact(trade, order_book, current_price)
                            
                            whale_transactions.append({
                                'symbol': symbol,
                                'side': side,
                                'amount': amount,
                                'price': price,
                                'usd_value': usd_value,
                                'whale_confidence': whale_confidence,
                                'market_impact': market_impact,
                                'timestamp': timestamp,
                                'datetime': trade.get('datetime', ''),
                                'relative_size': usd_value / whale_threshold if whale_threshold > 0 else 1,
                                'price_level': 'above_market' if price > current_price else 'below_market' if price < current_price else 'at_market'
                            })
                    except (ValueError, TypeError, KeyError) as e:
                        self.logger.debug(f"Error processing trade data for {symbol}: {e}")
                        continue
                        
            except Exception as te:
                self.logger.debug(f"Could not fetch trades for {symbol}: {te}")
                
            return whale_transactions
            
        except Exception as e:
            self.logger.warning(f"Error processing whale activity for {symbol}: {str(e)}")
            return []
            
    def _calculate_dynamic_whale_threshold(self, symbol: str, current_price: float, volume_24h: float) -> float:
        """Calculate dynamic whale threshold based on market conditions."""
        try:
            # Base threshold
            base_threshold = 50000  # $50k base
            
            # Adjust based on price level (higher price = higher threshold)
            if current_price > 100:
                price_multiplier = 2.0
            elif current_price > 10:
                price_multiplier = 1.5
            elif current_price > 1:
                price_multiplier = 1.0
            else:
                price_multiplier = 0.5
            
            # Adjust based on volume (higher volume = higher threshold)
            if volume_24h > 10000000:  # $10M+ daily volume
                volume_multiplier = 3.0
            elif volume_24h > 1000000:  # $1M+ daily volume
                volume_multiplier = 2.0
            elif volume_24h > 100000:  # $100k+ daily volume
                volume_multiplier = 1.0
            else:
                volume_multiplier = 0.5
            
            # Calculate final threshold
            threshold = base_threshold * price_multiplier * volume_multiplier
            
            # Ensure reasonable bounds
            return max(10000, min(threshold, 1000000))  # Between $10k and $1M
            
        except Exception as e:
            self.logger.warning(f"Error calculating whale threshold for {symbol}: {e}")
            return 50000  # Default fallback
    
    def _calculate_market_impact(self, trade: Dict[str, Any], order_book: Dict[str, Any], current_price: float) -> float:
        """Calculate the potential market impact of a whale trade."""
        try:
            if not order_book or not trade:
                return 0.0
            
            amount = float(trade.get('amount', 0))
            price = float(trade.get('price', 0))
            side = trade.get('side', 'unknown')
            
            if amount == 0 or price == 0:
                return 0.0
            
            # Calculate how much of the order book this trade would consume
            impact = 0.0
            
            if side == 'buy' and order_book.get('asks'):
                # For buy orders, check how much of the ask side would be consumed
                remaining_amount = amount
                for ask in order_book['asks']:
                    ask_price = float(ask[0])
                    ask_volume = float(ask[1])
                    
                    if remaining_amount <= 0:
                        break
                    
                    consumed = min(remaining_amount, ask_volume)
                    price_impact = abs(ask_price - current_price) / current_price
                    impact += consumed * price_impact
                    remaining_amount -= consumed
                    
            elif side == 'sell' and order_book.get('bids'):
                # For sell orders, check how much of the bid side would be consumed
                remaining_amount = amount
                for bid in order_book['bids']:
                    bid_price = float(bid[0])
                    bid_volume = float(bid[1])
                    
                    if remaining_amount <= 0:
                        break
                    
                    consumed = min(remaining_amount, bid_volume)
                    price_impact = abs(bid_price - current_price) / current_price
                    impact += consumed * price_impact
                    remaining_amount -= consumed
            
            # Normalize impact score (0-100)
            return min(impact * 100, 100.0)
            
        except Exception as e:
            self.logger.warning(f"Error calculating market impact: {e}")
            return 0.0
    
    def _generate_whale_summary(self, significant_activity: Dict[str, Any], whale_sentiment: str, whale_bias: float) -> str:
        """Generate a human-readable summary of whale activity."""
        try:
            if not significant_activity:
                return "No significant whale activity detected in the last 24 hours."
            
            # Count symbols with significant activity
            active_symbols = len(significant_activity)
            
            # Find most active symbol
            most_active_symbol = max(
                significant_activity.items(), 
                key=lambda x: x[1]['usd_value']
            )[0] if significant_activity else 'N/A'
            
            # Calculate total volume
            total_volume = sum(activity['usd_value'] for activity in significant_activity.values())
            
            # Generate summary based on sentiment
            if whale_sentiment == 'BULLISH':
                sentiment_text = f"strong buying pressure with {whale_bias:.1f}% net buying bias"
            elif whale_sentiment == 'BEARISH':
                sentiment_text = f"strong selling pressure with {abs(whale_bias):.1f}% net selling bias"
            else:
                sentiment_text = "balanced buying and selling activity"
            
            summary = (
                f"Whale activity detected across {active_symbols} symbols with "
                f"${self._format_number(total_volume)} total volume. "
                f"Market shows {sentiment_text}. "
                f"Most active symbol: {most_active_symbol}."
            )
            
            return summary
            
        except Exception as e:
            self.logger.warning(f"Error generating whale summary: {e}")
            return "Whale activity summary unavailable."

    def _calculate_whale_threshold(self, order_book: Dict[str, List[List[float]]]) -> float:
        """Calculate dynamic whale threshold based on order book."""
        try:
            # Get top 10 bid and ask orders
            top_bids = order_book.get('bids', [])[:10]
            top_asks = order_book.get('asks', [])[:10]
            
            # Calculate average order size
            bid_volumes = [float(bid[1]) * float(bid[0]) for bid in top_bids]
            ask_volumes = [float(ask[1]) * float(ask[0]) for ask in top_asks]
            
            all_volumes = bid_volumes + ask_volumes
            
            if not all_volumes:
                return 50000  # Default threshold
            
            # Use 90th percentile as whale threshold
            all_volumes.sort()
            percentile_90 = all_volumes[int(len(all_volumes) * 0.9)]
            
            # Ensure minimum threshold
            return max(percentile_90, 10000)
            
        except Exception as e:
            self.logger.warning(f"Error calculating whale threshold: {str(e)}")
            return 50000  # Default fallback

    async def _calculate_performance_metrics(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate system performance metrics."""
        try:
            # Calculate API latency metrics
            if self.api_latencies:
                avg_latency = sum(self.api_latencies) / len(self.api_latencies)
                max_latency = max(self.api_latencies)
                p95_latency = sorted(self.api_latencies)[int(len(self.api_latencies) * 0.95)] if len(self.api_latencies) > 1 else avg_latency
            else:
                avg_latency = max_latency = p95_latency = 0
            
            # Calculate error rate
            total_errors = sum(self.error_counts.values())
            total_requests = sum(self.request_counts.values())
            error_rate = (total_errors / total_requests * 100) if total_requests > 0 else 0
            
            # Calculate errors per minute
            current_time = time.time()
            recent_errors = sum(1 for error_time in [self.last_error_time] if current_time - error_time < 60)
            errors_per_minute = recent_errors
            
            # Calculate data quality score
            data_quality_scores = []
            for symbol in symbols[:5]:  # Sample first 5 symbols
                try:
                    ticker = await self._fetch_with_retry('fetch_ticker', symbol.replace('/', ''), timeout=5)
                    quality_score = self._calculate_data_quality(ticker)
                    data_quality_scores.append(quality_score)
                except:
                    data_quality_scores.append(0)
            
            avg_quality_score = sum(data_quality_scores) / len(data_quality_scores) if data_quality_scores else 100
            min_quality_score = min(data_quality_scores) if data_quality_scores else 100
            
            # Calculate processing times
            if hasattr(self, 'processing_times') and self.processing_times:
                avg_processing_time = sum(self.processing_times) / len(self.processing_times)
                max_processing_time = max(self.processing_times)
            else:
                avg_processing_time = max_processing_time = 0
            
            result = {
                'metrics': {
                    'api_latency': {
                    'avg': avg_latency,
                    'max': max_latency,
                    'p95': p95_latency
                },
                'error_rate': {
                    'total': total_errors,
                    'by_type': dict(self.error_counts),
                    'errors_per_minute': errors_per_minute
                },
                'data_quality': {
                    'avg_score': avg_quality_score,
                    'min_score': min_quality_score
                },
                'processing_time': {
                    'avg': avg_processing_time,
                    'max': max_processing_time
                },
                'timestamp': int(datetime.now().timestamp() * 1000)
                },
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating performance metrics: {str(e)}")
            return {
                'api_latency': {'avg': 0, 'max': 0, 'p95': 0},
                'error_rate': {'total': 0, 'by_type': {}, 'errors_per_minute': 0.0},
                'data_quality': {'avg_score': 100, 'min_score': 100},
                'processing_time': {'avg': 0, 'max': 0},
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }

    async def _calculate_bitcoin_beta_analysis(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate Bitcoin Beta Analysis for correlation and alpha opportunities."""
        try:
            if not self.beta_enabled or not self.beta_report:
                self.logger.warning("Bitcoin Beta Analysis not available")
                return {
                    'beta_analysis': {},
                    'alpha_opportunities': [],
                    'timestamp': int(datetime.now().timestamp() * 1000),
                    'error': 'Bitcoin Beta Analysis not enabled'
                }
            
            self.logger.info("Starting Bitcoin Beta Analysis calculation")
            
            # Fetch market data for beta analysis
            market_data = await self.beta_report._fetch_all_market_data(symbols)
            if not market_data:
                self.logger.warning("No market data available for beta analysis")
                return {
                    'beta_analysis': {},
                    'alpha_opportunities': [],
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
            
            # Calculate multi-timeframe beta analysis
            beta_analysis_data = self.beta_report._calculate_multi_timeframe_beta(market_data)
            
            if not beta_analysis_data:
                self.logger.warning("No beta analysis data generated")
                return {
                    'beta_analysis': {},
                    'alpha_opportunities': [],
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
            
            # Extract key metrics for market report integration
            result = {
                'beta_analysis': beta_analysis_data,
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            # Add alpha opportunities if available
            if hasattr(self.beta_report, 'alpha_detector') and self.beta_report.alpha_detector:
                try:
                    alpha_opportunities = self.beta_report.alpha_detector.detect_alpha_opportunities(beta_analysis_data)
                    # Convert AlphaOpportunity objects to dictionaries for JSON serialization
                    result['alpha_opportunities'] = [opp.to_dict() for opp in alpha_opportunities]
                    self.logger.info(f"Detected {len(alpha_opportunities)} alpha opportunities")
                except Exception as alpha_err:
                    self.logger.warning(f"Error detecting alpha opportunities: {str(alpha_err)}")
                    result['alpha_opportunities'] = []
            
            # Add summary metrics for easy consumption
            if beta_analysis_data:
                summary = {}
                for tf, data in beta_analysis_data.items():
                    if isinstance(data, dict):
                        # Calculate average beta for this timeframe
                        betas = []
                        for symbol, metrics in data.items():
                            if isinstance(metrics, dict) and 'beta' in metrics:
                                try:
                                    beta_val = float(metrics['beta'])
                                    if not math.isnan(beta_val) and beta_val > 0:
                                        betas.append(beta_val)
                                except (ValueError, TypeError):
                                    continue
                        
                        if betas:
                            summary[tf] = {
                                'avg_beta': sum(betas) / len(betas),
                                'max_beta': max(betas),
                                'min_beta': min(betas),
                                'symbol_count': len(betas)
                            }
                
                result['summary'] = summary
            
            self.logger.info("Bitcoin Beta Analysis calculation completed successfully")
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating Bitcoin Beta Analysis: {str(e)}")
            return {
                'beta_analysis': {},
                'alpha_opportunities': [],
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }

    async def _calculate_top_performers(self, symbols: List[str]) -> Dict[str, Any]:
        """Calculate top performing symbols (gainers and losers)."""
        try:
            self.logger.debug(f"Calculating top performers for {len(symbols)} symbols")
            
            performers = []
            failed_symbols = []
            
            for symbol in symbols[:20]:  # Limit to top 20 symbols for performance
                try:
                    # Get 24h ticker data
                    ticker_data = await self._fetch_with_retry('fetch_ticker', symbol)
                    
                    if ticker_data and 'percentage' in ticker_data:
                        change_percent = float(ticker_data.get('percentage', 0))
                        
                        performer = {
                            'symbol': symbol,
                            'price': float(ticker_data.get('last', 0)),
                            'change_percent': change_percent,
                            'volume': float(ticker_data.get('baseVolume', 0)),
                            'change': f"{change_percent:+.2f}%"
                        }
                        performers.append(performer)
                        
                except Exception as e:
                    failed_symbols.append(symbol)
                    self.logger.debug(f"Failed to get ticker for {symbol}: {str(e)}")
                    continue
            
            # Sort by change percentage
            performers.sort(key=lambda x: x['change_percent'], reverse=True)
            
            # Get top gainers and losers
            gainers = [p for p in performers if p['change_percent'] > 0][:5]
            losers = [p for p in performers if p['change_percent'] < 0][-5:]
            losers.reverse()  # Show worst performers first
            
            result = {
                'gainers': gainers,
                'losers': losers,
                'total_analyzed': len(performers),
                'failed_symbols': len(failed_symbols),
                'timestamp': int(datetime.now().timestamp() * 1000)
            }
            
            self.logger.debug(f"Top performers calculated: {len(gainers)} gainers, {len(losers)} losers")
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating top performers: {str(e)}")
            return {
                'gainers': [],
                'losers': [],
                'total_analyzed': 0,
                'failed_symbols': len(symbols),
                'timestamp': int(datetime.now().timestamp() * 1000),
                'error': str(e)
            }

    def _validate_report_data(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Validate report data and return validation results."""
        try:
            validation_results = {
                'valid': True,
                'issues': [],
                'warnings': [],
                'quality_score': 100.0
            }
            
            # Check for required sections
            required_sections = ['market_overview', 'futures_premium', 'smart_money_index', 'whale_activity']
            missing_sections = []
            
            for section in required_sections:
                if section not in report or not report[section]:
                    missing_sections.append(section)
                    validation_results['warnings'].append(f"Missing or empty section: {section}")
            
            # Reduce quality score for missing sections
            if missing_sections:
                validation_results['quality_score'] -= len(missing_sections) * 15
            
            # Check data freshness
            current_time = int(time.time() * 1000)
            for section_name, section_data in report.items():
                if isinstance(section_data, dict) and 'timestamp' in section_data:
                    timestamp = section_data['timestamp']
                    age_minutes = (current_time - timestamp) / (1000 * 60)
                    
                    if age_minutes > 30:  # Data older than 30 minutes
                        validation_results['warnings'].append(f"Stale data in {section_name}: {age_minutes:.1f} min old")
                        validation_results['quality_score'] -= 5
            
            # Check for error indicators
            if 'failed_symbols' in report.get('market_overview', {}):
                failed_count = len(report['market_overview']['failed_symbols'])
                if failed_count > 0:
                    validation_results['warnings'].append(f"{failed_count} symbols failed to load")
                    validation_results['quality_score'] -= failed_count * 2
            
            # Ensure minimum quality score
            validation_results['quality_score'] = max(60.0, validation_results['quality_score'])
            
            # Set overall validity
            validation_results['valid'] = validation_results['quality_score'] >= 60.0
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"Error validating report data: {str(e)}")
            return {
                'valid': False,
                'issues': [f"Validation error: {str(e)}"],
                'warnings': [],
                'quality_score': 0.0
            }

    def _convert_symbol_format(self, symbol):
        """
        Convert symbol to the correct format for Bybit API with improved futures contract handling.
        
        Args:
            symbol: Symbol in any format (CCXT, Bybit, etc.)
            
        Returns:
            str: Symbol in appropriate format for the API call
        """
        try:
            # If already in simple Bybit format (no slashes/colons), return as-is
            if '/' not in symbol and ':' not in symbol:
                return symbol
            
            # Check if this is a futures contract with expiry date
            if self._is_futures_contract(symbol):
                return self._convert_futures_symbol(symbol)
            
            # Handle standard spot/perpetual conversions
            # Convert from CCXT format (BTC/USDT:USDT) to Bybit format (BTCUSDT)
            if '/' in symbol:
                # Remove everything after colon if present (for perpetual contracts)
                base_symbol = symbol.split(':')[0] if ':' in symbol else symbol
                # Remove the slash to get Bybit format
                return base_symbol.replace('/', '')
                
            return symbol
            
        except Exception as e:
            self.logger.warning(f"Symbol format conversion failed for {symbol}: {e}")
            return symbol

    def _is_futures_contract(self, symbol):
        """
        Detect if a symbol represents a futures contract with expiry.
        
        Args:
            symbol: Symbol to check
            
        Returns:
            bool: True if it's a futures contract with expiry
        """
        try:
            # Look for date patterns in the symbol (YYYYMMDD or similar)
            import re
            
            # Pattern 1: YYYY-MM-DD or YYYYMMDD format
            date_pattern = r'(\d{4}[-]?\d{2}[-]?\d{2}|\d{8})'
            if re.search(date_pattern, symbol):
                return True
                
            # Pattern 2: Month codes (H, M, U, Z) followed by year
            month_code_pattern = r'[HMUZ]\d{2}$'
            if re.search(month_code_pattern, symbol):
                return True
                
            # Pattern 3: Standard quarterly format (e.g., -27JUN25)
            quarterly_pattern = r'-\d{1,2}[A-Z]{3}\d{2}$'
            if re.search(quarterly_pattern, symbol):
                return True
                
            # Pattern 4: MMDD format (e.g., SOLUSDT0627)
            mmdd_pattern = r'[A-Z]+USDT\d{4}$'
            if re.search(mmdd_pattern, symbol):
                return True
                
            return False
            
        except Exception as e:
            self.logger.debug(f"Error checking futures contract pattern for {symbol}: {e}")
            return False

    def _convert_futures_symbol(self, symbol):
        """
        Convert futures symbol while preserving contract information.
        
        Args:
            symbol: Futures symbol to convert
            
        Returns:
            str: Converted symbol preserving futures contract information
        """
        try:
            # For complex futures symbols like ETH/USDT:USDT-20260328
            if '/' in symbol and ':' in symbol:
                # Split into parts
                parts = symbol.split(':')
                base_part = parts[0].replace('/', '')  # ETH/USDT -> ETHUSDT
                
                if len(parts) > 1:
                    # Handle the contract part
                    contract_part = parts[1]  # USDT-20260328
                    
                    if '-' in contract_part:
                        quote, expiry = contract_part.split('-', 1)
                        # Convert date format if needed (20260328 -> proper format)
                        if expiry.isdigit() and len(expiry) == 8:
                            # Convert YYYYMMDD to -DDMMMYY format for Bybit
                            year = expiry[:4]
                            month = expiry[4:6]
                            day = expiry[6:8]
                            
                            month_names = {
                                '01': 'JAN', '02': 'FEB', '03': 'MAR', '04': 'APR',
                                '05': 'MAY', '06': 'JUN', '07': 'JUL', '08': 'AUG',
                                '09': 'SEP', '10': 'OCT', '11': 'NOV', '12': 'DEC'
                            }
                            
                            if month in month_names:
                                formatted_expiry = f"-{day}{month_names[month]}{year[2:]}"
                                return f"{base_part}{formatted_expiry}"
                        else:
                            # Keep original expiry format
                            return f"{base_part}-{expiry}"
                    else:
                        # No expiry separator, keep as-is
                        return f"{base_part}{contract_part}"
                
                return base_part
            
            # For simpler futures symbols, preserve the format
            elif '/' in symbol:
                return symbol.replace('/', '')
            
            # Return as-is if no conversion needed
            return symbol
            
        except Exception as e:
            self.logger.warning(f"Futures symbol conversion failed for {symbol}: {e}")
            return symbol

    def _detect_symbol_format(self, symbol):
        """
        Detect and auto-correct symbol format for API calls with improved futures handling.
        
        Args:
            symbol: Symbol to detect format for
            
        Returns:
            tuple: (bybit_format, ccxt_format)
        """
        try:
            # Check if it's a futures contract
            if self._is_futures_contract(symbol):
                bybit_format = self._convert_futures_symbol(symbol)
                # For futures, keep the CCXT format mostly intact
                ccxt_format = symbol
            else:
                bybit_format = self._convert_symbol_format(symbol)
                
                # Get CCXT format from mapping or construct it
                if bybit_format in self.symbol_mapping:
                    ccxt_format = self.symbol_mapping[bybit_format]
                else:
                    # Construct CCXT format for unmapped symbols
                    if bybit_format.endswith('USDT'):
                        base = bybit_format[:-4]  # Remove 'USDT'
                        ccxt_format = f"{base}/USDT:USDT"
                    else:
                        ccxt_format = bybit_format
                    
            return bybit_format, ccxt_format
            
        except Exception as e:
            self.logger.error(f"Symbol format detection failed for {symbol}: {e}")
            return symbol, symbol

    async def generate_bitcoin_beta_report(self) -> Optional[str]:
        """Generate a standalone Bitcoin Beta Analysis report.
        
        Returns:
            str: Path to generated PDF report, or None if failed
        """
        try:
            if not self.beta_enabled or not self.beta_report:
                self.logger.error("Bitcoin Beta Analysis not available for standalone report")
                return None
            
            self.logger.info("Generating standalone Bitcoin Beta Analysis report")
            
            # Generate the full beta report
            pdf_path = await self.beta_report.generate_report()
            
            if pdf_path:
                self.logger.info(f"Bitcoin Beta Analysis report generated: {pdf_path}")
                
                # Send alert if alert manager is available
                if self.alert_manager:
                    await self.alert_manager.send_alert(
                        level="info",
                        message="Bitcoin Beta Analysis report generated",
                        details={"type": "bitcoin_beta_report", "pdf_path": pdf_path}
                    )
                
                return pdf_path
            else:
                self.logger.error("Failed to generate Bitcoin Beta Analysis report")
                return None
                
        except Exception as e:
            self.logger.error(f"Error generating Bitcoin Beta Analysis report: {str(e)}")
            return None

    async def schedule_beta_reports(self):
        """Schedule Bitcoin Beta Analysis reports every 6 hours."""
        beta_report_times = [
            "00:00",  # Daily reset
            "06:00",  # 6 hours later
            "12:00",  # 12 hours later  
            "18:00"   # 18 hours later
        ]
        
        try:
            while True:
                try:
                    current_time = datetime.utcnow()
                    current_hhmm = current_time.strftime("%H:%M")
                    
                    if current_hhmm in beta_report_times:
                        self.logger.info(f"Generating scheduled Bitcoin Beta Analysis report at {current_hhmm} UTC")
                        
                        pdf_path = await self.generate_bitcoin_beta_report()
                        
                        if pdf_path and self.alert_manager:
                            # Send enhanced Discord notification for beta report
                            if hasattr(self.alert_manager, 'send_discord_webhook_message'):
                                beta_notification = {
                                    "content": f"📊 **Bitcoin Beta Analysis Report**\n*{current_time.strftime('%B %d, %Y - %H:%M UTC')}*",
                                    "embeds": [{
                                        "title": "📈 Bitcoin Beta Analysis Complete",
                                        "color": 3447003,  # Blue
                                        "description": (
                                            "Multi-timeframe correlation analysis with Bitcoin across 4H, 30M, 5M, and 1M timeframes.\n\n"
                                            "**Key Metrics:**\n"
                                            "• Beta coefficients for all major pairs\n"
                                            "• Alpha generation opportunities\n"
                                            "• Cross-timeframe divergence signals\n"
                                            "• Statistical significance testing"
                                        ),
                                        "fields": [
                                            {
                                                "name": "📋 Report Contents",
                                                "value": "• Normalized price performance charts\n• Beta comparison across timeframes\n• Correlation heatmap\n• Trading insights & recommendations",
                                                "inline": False
                                            }
                                        ],
                                        "footer": {
                                            "text": f"Virtuoso Beta Engine | {current_time.strftime('%H:%M:%S UTC')}",
                                            "icon_url": "https://i.imgur.com/4M34hi2.png"
                                        },
                                        "timestamp": current_time.isoformat() + 'Z'
                                    }],
                                    "username": "Virtuoso Beta Monitor",
                                    "avatar_url": "https://i.imgur.com/4M34hi2.png"
                                }
                                
                                # Add PDF attachment
                                if os.path.exists(pdf_path):
                                    files = [{
                                        'path': pdf_path,
                                        'filename': os.path.basename(pdf_path),
                                        'description': 'Bitcoin Beta Analysis Report'
                                    }]
                                    
                                    await self.alert_manager.send_discord_webhook_message(beta_notification, files=files)
                                    self.logger.info("Bitcoin Beta Analysis report sent to Discord with PDF attachment")
                                else:
                                    await self.alert_manager.send_discord_webhook_message(beta_notification)
                                    self.logger.info("Bitcoin Beta Analysis notification sent to Discord")
                    
                    await asyncio.sleep(60)  # Check every minute
                    
                except asyncio.CancelledError:
                    self.logger.info("Bitcoin Beta Analysis scheduler stopped")
                    raise
                except Exception as e:
                    self.logger.error(f"Error in Beta Analysis scheduler: {str(e)}")
                    await asyncio.sleep(60)
                    
        except asyncio.CancelledError:
            self.logger.info("Bitcoin Beta Analysis scheduler stopped")
            raise
        except Exception as e:
            self.logger.error(f"Fatal error in Beta Analysis scheduler: {str(e)}")
            raise

    def _optimize_memory(self):
        """Optimize memory usage by cleaning up data structures and forcing garbage collection."""
        try:
            if not self.memory_optimization_enabled:
                return
                
            # Truncate large data structures
            if len(self.api_latencies) > 50:
                self.api_latencies = self.api_latencies[-50:]
                
            if len(self.data_quality_scores) > 20:
                self.data_quality_scores = self.data_quality_scores[-20:]
                
            if len(self.processing_times) > 20:
                self.processing_times = self.processing_times[-20:]
            
            # Clear old cache entries if memory is tight
            if hasattr(self, 'cache') and len(self.cache) > 250:
                # Keep only most recent half of cache entries
                cache_items = list(self.cache.items())
                self.cache.clear()
                for key, value in cache_items[-125:]:  # Keep last 125 entries
                    self.cache[key] = value
            
            # Force garbage collection periodically
            self.operation_count += 1
            if self.operation_count % self.gc_collection_interval == 0:
                collected = gc.collect()
                if collected > 0:
                    self.logger.debug(f"Garbage collection freed {collected} objects")
                    
            # Log memory usage occasionally
            if self.operation_count % 50 == 0:
                memory_info = self._get_memory_usage()
                self.logger.info(f"Memory optimization active - Current usage: {memory_info}")
                
        except Exception as e:
            self.logger.warning(f"Memory optimization error: {e}")

    def _check_memory_pressure(self) -> bool:
        """Check if system is under memory pressure."""
        try:
            # Simple check based on available memory
            memory_info = self._get_memory_usage()
            if isinstance(memory_info, dict) and 'available_mb' in memory_info:
                available_mb = memory_info['available_mb']
                return available_mb < 500  # Less than 500MB available
            return False
        except Exception:
            return False

    def _safe_float_from_percentage(self, value):
        """Safely convert a percentage value to float, handling both string and numeric inputs.
        
        Args:
            value: Can be a string like '5.2%', a float like 0.052, or a numeric value
            
        Returns:
            float: The percentage value as a float (e.g., 5.2 for '5.2%')
        """
        try:
            if value is None:
                return 0.0
            
            # If it's already a number, check if it's in decimal form (< 1) or percentage form (> 1)
            if isinstance(value, (int, float)):
                # If value is between 0 and 1, assume it's in decimal form (e.g., 0.052 = 5.2%)
                if 0 <= value <= 1:
                    return value * 100
                else:
                    # If value is > 1, assume it's already in percentage form
                    return float(value)
            
            # If it's a string, handle percentage signs
            if isinstance(value, str):
                # Remove % sign and whitespace
                cleaned = value.strip().replace('%', '')
                return float(cleaned)
            
            # Fallback for other types - try to convert to string first, then process
            try:
                str_value = str(value)
                if '%' in str_value:
                    cleaned = str_value.strip().replace('%', '')
                    return float(cleaned)
                else:
                    return float(value)
            except (ValueError, TypeError):
                return float(value)
            
        except (ValueError, TypeError, AttributeError) as e:
            self.logger.debug(f"Could not convert percentage value: {value} (type: {type(value)}) - Error: {e}")
            return 0.0
    async def _fetch_bybit_open_interest_direct(self, symbol: str) -> float:
        """
        Fetch Open Interest directly from Bybit API bypassing CCXT
        
        Args:
            symbol: CCXT format symbol (e.g., 'BTC/USDT:USDT')
            
        Returns:
            float: Open Interest value
        """
        try:
            import aiohttp
            import talib
            
            # Convert CCXT symbol to Bybit format
            if ':USDT' in symbol:  # Perpetual futures
                bybit_symbol = symbol.replace('/USDT:USDT', 'USDT')  # BTC/USDT:USDT -> BTCUSDT
                category = 'linear'
            elif '/USDT' in symbol:  # Spot
                bybit_symbol = symbol.replace('/', '')  # BTC/USDT -> BTCUSDT  
                category = 'spot'
            elif '/USD' in symbol:  # Inverse contracts
                bybit_symbol = symbol.replace('/', '')  # BTC/USD -> BTCUSD
                category = 'inverse'
            else:
                # Assume it's already in Bybit format
                bybit_symbol = symbol
                category = 'linear'  # Default to linear
            
            # Bybit API endpoint
            url = "https://api.bybit.com/v5/market/tickers"
            params = {
                'category': category,
                'symbol': bybit_symbol
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    if response.status != 200:
                        self.logger.warning(f"Bybit API error for {symbol}: HTTP {response.status}")
                        return 0
                    
                    data = await response.json()
                    
                    if data.get('retCode') != 0:
                        self.logger.warning(f"Bybit API returned error for {symbol}: {data}")
                        return 0
                    
                    result = data.get('result', {})
                    ticker_list = result.get('list', [])
                    
                    if not ticker_list:
                        self.logger.warning(f"No Bybit ticker data for {symbol} ({bybit_symbol})")
                        return 0
                    
                    ticker = ticker_list[0]
                    
                    # Extract Open Interest
                    oi_raw = ticker.get('openInterest') or ticker.get('openInterestValue')
                    if oi_raw:
                        try:
                            oi = float(oi_raw)
                            self.logger.info(f"🎯 Bybit OI for {symbol}: {oi:,.2f} ({category}/{bybit_symbol})")
                            return oi
                        except (ValueError, TypeError) as e:
                            self.logger.warning(f"Error parsing Bybit OI for {symbol}: {oi_raw} - {e}")
                            return 0
                    else:
                        self.logger.debug(f"No OI fields in Bybit response for {symbol}")
                        return 0
                        
        except Exception as e:
            self.logger.warning(f"Error fetching Bybit OI for {symbol}: {e}")
            return 0

    def _transform_futures_premium_for_template(self, futures_premium_data: Dict[str, Any]) -> Dict[str, Any]:
        """Transform futures premium data to match template expectations."""
        try:
            if not futures_premium_data or 'premiums' not in futures_premium_data:
                return futures_premium_data
            
            # Transform each symbol's premium data
            transformed_premiums = {}
            for symbol, data in futures_premium_data['premiums'].items():
                transformed_data = {
                    'mark_price': data.get('perpetual_price', data.get('index_price', 0)),
                    'index_price': data.get('index_price', 0),
                    'premium': f"{data.get('perpetual_basis', 0):.3f}%",
                    'premium_value': data.get('perpetual_basis', 0),  # Numeric value for template logic
                    'premium_type': 'Contango' if data.get('perpetual_basis', 0) >= 0 else 'Backwardation',
                    'quarterly_price': data.get('quarterly_price', 0),
                    'quarterly_basis': f"{data.get('quarterly_basis', 0):.3f}%"
                }
                transformed_premiums[symbol] = transformed_data
            
            # Update the futures premium data with transformed structure
            transformed_data = futures_premium_data.copy()
            transformed_data['premiums'] = transformed_premiums
            
            # Add premium_data at the top level for template compatibility
            transformed_data['premium_data'] = transformed_premiums
            
            return transformed_data
            
        except Exception as e:
            self.logger.error(f"Error transforming futures premium data for template: {str(e)}")
            return futures_premium_data

    async def fetch_market_data(self, symbol: str, timeframe: str = '1d', limit: int = 100) -> Optional[Dict[str, Any]]:
        """Fetch market data for a specific symbol and timeframe.
        
        Args:
            symbol: Trading symbol (e.g., 'BTCUSDT', 'ETHUSDT')
            timeframe: Chart timeframe ('1m', '5m', '1h', '1d', etc.)
            limit: Number of candles to fetch
            
        Returns:
            Dictionary containing OHLCV data and market statistics, or None if failed
        """
        try:
            start_time = time.time()
            self.logger.debug(f"Fetching market data for {symbol} [{timeframe}] limit={limit}")
            
            # Convert symbol to proper format if needed
            formatted_symbol = self._convert_symbol_format(symbol)
            
            # Fetch OHLCV data with retry mechanism
            ohlcv_data = await self._fetch_with_retry('fetch_ohlcv', formatted_symbol, timeframe, limit, timeout=15)
            
            if not ohlcv_data:
                self.logger.warning(f"No OHLCV data received for {symbol}")
                return None
            
            # Fetch current ticker for additional market data
            ticker_data = await self._fetch_with_retry('fetch_ticker', formatted_symbol, timeout=10)
            
            # Fetch order book for spread analysis
            try:
                orderbook_data = await self._fetch_with_retry('fetch_order_book', formatted_symbol, timeout=8)
            except Exception as e:
                self.logger.debug(f"Could not fetch orderbook for {symbol}: {e}")
                orderbook_data = None
            
            # Process OHLCV data
            df = pd.DataFrame(ohlcv_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            
            # Calculate basic statistics
            current_price = float(df['close'].iloc[-1]) if not df.empty else 0
            price_change_24h = 0
            volume_24h = float(df['volume'].sum()) if not df.empty else 0
            
            if len(df) >= 2:
                price_change_24h = ((current_price - float(df['close'].iloc[0])) / float(df['close'].iloc[0])) * 100
            
            # Calculate volatility (standard deviation of returns)
            if len(df) >= 2:
                returns = df['close'].pct_change().dropna()
                volatility = float(returns.std() * 100) if not returns.empty else 0
            else:
                volatility = 0
            
            # Calculate average spread from orderbook
            spread_pct = 0
            if orderbook_data and 'bids' in orderbook_data and 'asks' in orderbook_data:
                if orderbook_data['bids'] and orderbook_data['asks']:
                    best_bid = float(orderbook_data['bids'][0][0])
                    best_ask = float(orderbook_data['asks'][0][0])
                    spread_pct = ((best_ask - best_bid) / best_bid) * 100
            
            # Prepare market data response
            market_data = {
                'symbol': symbol,
                'timeframe': timeframe,
                'timestamp': int(time.time() * 1000),
                'price': {
                    'current': current_price,
                    'change_24h': price_change_24h,
                    'high_24h': float(df['high'].max()) if not df.empty else 0,
                    'low_24h': float(df['low'].min()) if not df.empty else 0
                },
                'volume': {
                    'total_24h': volume_24h,
                    'average': float(df['volume'].mean()) if not df.empty else 0
                },
                'statistics': {
                    'volatility': volatility,
                    'spread_pct': spread_pct,
                    'data_points': len(df)
                },
                'ohlcv': ohlcv_data,
                'ticker': ticker_data,
                'orderbook_summary': {
                    'bid_depth': len(orderbook_data['bids']) if orderbook_data and 'bids' in orderbook_data else 0,
                    'ask_depth': len(orderbook_data['asks']) if orderbook_data and 'asks' in orderbook_data else 0,
                    'spread_pct': spread_pct
                } if orderbook_data else None
            }
            
            # Log performance
            fetch_duration = time.time() - start_time
            await self._log_api_latency(start_time, f"fetch_market_data:{symbol}")
            self.logger.debug(f"Market data fetch completed for {symbol} in {fetch_duration:.3f}s")
            
            return market_data
            
        except Exception as e:
            self.logger.error(f"Error fetching market data for {symbol}: {str(e)}")
            self._log_error(f"fetch_market_data:{symbol}", str(e))
            return None

    async def create_analysis_report(self, market_data: Dict[str, Any], analysis_type: str = 'comprehensive') -> Optional[Dict[str, Any]]:
        """Create an analysis report from market data.
        
        Args:
            market_data: Market data dictionary from fetch_market_data or generate_market_summary
            analysis_type: Type of analysis ('comprehensive', 'technical', 'fundamental', 'sentiment')
            
        Returns:
            Dictionary containing analysis results and insights, or None if failed
        """
        try:
            start_time = time.time()
            self.logger.debug(f"Creating {analysis_type} analysis report")
            
            if not market_data:
                self.logger.warning("No market data provided for analysis")
                return None
            
            analysis_report = {
                'analysis_type': analysis_type,
                'timestamp': int(time.time() * 1000),
                'data_quality_score': self._calculate_data_quality(market_data),
                'summary': {},
                'insights': [],
                'recommendations': [],
                'risk_factors': []
            }
            
            # Comprehensive analysis includes all components
            if analysis_type in ['comprehensive', 'all']:
                analysis_report['technical_analysis'] = await self._create_technical_analysis(market_data)
                analysis_report['market_structure'] = await self._create_market_structure_analysis(market_data)
                analysis_report['sentiment_analysis'] = await self._create_sentiment_analysis(market_data)
                analysis_report['risk_analysis'] = await self._create_risk_analysis(market_data)
                
            # Technical analysis focus
            elif analysis_type == 'technical':
                analysis_report['technical_analysis'] = await self._create_technical_analysis(market_data)
                
            # Fundamental analysis focus  
            elif analysis_type == 'fundamental':
                analysis_report['fundamental_analysis'] = await self._create_fundamental_analysis(market_data)
                
            # Sentiment analysis focus
            elif analysis_type == 'sentiment':
                analysis_report['sentiment_analysis'] = await self._create_sentiment_analysis(market_data)
            
            # Generate executive summary
            analysis_report['summary'] = await self._generate_analysis_summary(analysis_report, market_data)
            
            # Generate actionable insights
            analysis_report['insights'] = await self._generate_analysis_insights(analysis_report, market_data)
            
            # Generate recommendations
            analysis_report['recommendations'] = await self._generate_analysis_recommendations(analysis_report, market_data)
            
            # Calculate processing time
            processing_duration = time.time() - start_time
            analysis_report['metadata'] = {
                'processing_time_seconds': processing_duration,
                'data_sources': list(market_data.keys()) if isinstance(market_data, dict) else ['unknown'],
                'analysis_depth': analysis_type,
                'reliability_score': self._calculate_analysis_reliability(analysis_report)
            }
            
            self.logger.info(f"Analysis report created in {processing_duration:.3f}s with {len(analysis_report.get('insights', []))} insights")
            return analysis_report
            
        except Exception as e:
            self.logger.error(f"Error creating analysis report: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return None

    async def format_report_data(self, raw_data: Dict[str, Any], format_type: str = 'standard', target: str = 'general') -> Dict[str, Any]:
        """Format raw report data for specific outputs and audiences.
        
        Args:
            raw_data: Raw report data from generate_market_summary or create_analysis_report
            format_type: Output format ('standard', 'compact', 'detailed', 'executive')
            target: Target audience ('general', 'technical', 'executive', 'api')
            
        Returns:
            Formatted data dictionary optimized for the specified format and target
        """
        try:
            start_time = time.time()
            self.logger.debug(f"Formatting report data: {format_type} for {target} audience")
            
            if not raw_data:
                self.logger.warning("No raw data provided for formatting")
                return {}
            
            # Create base formatted structure
            formatted_data = {
                'format_info': {
                    'format_type': format_type,
                    'target_audience': target,
                    'formatted_at': datetime.now().isoformat(),
                    'data_timestamp': raw_data.get('timestamp', int(time.time() * 1000))
                },
                'metadata': self._extract_metadata(raw_data),
                'content': {}
            }
            
            # Apply format-specific transformations
            if format_type == 'executive':
                formatted_data['content'] = await self._format_for_executive(raw_data)
                
            elif format_type == 'technical':
                formatted_data['content'] = await self._format_for_technical(raw_data)
                
            elif format_type == 'compact':
                formatted_data['content'] = await self._format_for_compact(raw_data)
                
            elif format_type == 'detailed':
                formatted_data['content'] = await self._format_for_detailed(raw_data)
                
            elif format_type == 'api':
                formatted_data = await self._format_for_api(raw_data)  # API format replaces entire structure
                
            else:  # standard format
                formatted_data['content'] = await self._format_for_standard(raw_data)
            
            # Apply target audience adjustments
            if target == 'executive':
                formatted_data = await self._adjust_for_executive_audience(formatted_data)
            elif target == 'technical':
                formatted_data = await self._adjust_for_technical_audience(formatted_data)
            elif target == 'api':
                formatted_data = await self._adjust_for_api_audience(formatted_data)
                
            # Add formatting statistics
            processing_duration = time.time() - start_time
            formatted_data['format_info']['processing_time_seconds'] = processing_duration
            formatted_data['format_info']['original_data_size'] = len(str(raw_data))
            formatted_data['format_info']['formatted_data_size'] = len(str(formatted_data))
            
            self.logger.debug(f"Report formatting completed in {processing_duration:.3f}s")
            return formatted_data
            
        except Exception as e:
            self.logger.error(f"Error formatting report data: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return raw_data  # Return original data as fallback

    # Helper methods for the new functionality
    async def _create_technical_analysis(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create technical analysis from market data."""
        try:
            technical_data = {
                'trend_analysis': 'neutral',
                'support_resistance': {'support': [], 'resistance': []},
                'momentum_indicators': {},
                'volume_analysis': {},
                'pattern_recognition': []
            }
            
            # Extract price data if available
            if 'price' in market_data:
                price_info = market_data['price']
                change_24h = price_info.get('change_24h', 0)
                
                # Simple trend determination
                if change_24h > 2:
                    technical_data['trend_analysis'] = 'bullish'
                elif change_24h < -2:
                    technical_data['trend_analysis'] = 'bearish'
                
                technical_data['momentum_indicators'] = {
                    'price_momentum': 'positive' if change_24h > 0 else 'negative',
                    'strength': min(abs(change_24h) / 10, 1.0)  # Normalize to 0-1
                }
            
            return technical_data
            
        except Exception as e:
            self.logger.error(f"Error in technical analysis: {e}")
            return {}

    async def _create_market_structure_analysis(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create market structure analysis."""
        try:
            structure_data = {
                'market_regime': market_data.get('market_overview', {}).get('regime', 'unknown'),
                'liquidity_analysis': {},
                'market_depth': {},
                'order_flow': {}
            }
            
            # Add liquidity metrics if available
            if 'volume' in market_data:
                volume_info = market_data['volume']
                structure_data['liquidity_analysis'] = {
                    'volume_24h': volume_info.get('total_24h', 0),
                    'liquidity_rating': self._get_volume_description(volume_info.get('total_24h', 0))
                }
            
            return structure_data
            
        except Exception as e:
            self.logger.error(f"Error in market structure analysis: {e}")
            return {}

    async def _create_sentiment_analysis(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create sentiment analysis from market data."""
        try:
            sentiment_data = {
                'overall_sentiment': 'neutral',
                'sentiment_score': 50.0,
                'sentiment_drivers': [],
                'social_metrics': {}
            }
            
            # Derive sentiment from various market indicators
            sentiment_score = 50.0  # Start neutral
            
            # Adjust based on market regime
            if 'market_overview' in market_data:
                regime = market_data['market_overview'].get('regime', '')
                if 'bullish' in regime.lower():
                    sentiment_score += 20
                elif 'bearish' in regime.lower():
                    sentiment_score -= 20
            
            # Adjust based on smart money index
            if 'smart_money_index' in market_data:
                smi_value = market_data['smart_money_index'].get('index', 50)
                sentiment_score += (smi_value - 50) * 0.5
            
            # Determine overall sentiment
            if sentiment_score > 60:
                sentiment_data['overall_sentiment'] = 'bullish'
            elif sentiment_score < 40:
                sentiment_data['overall_sentiment'] = 'bearish'
            
            sentiment_data['sentiment_score'] = max(0, min(100, sentiment_score))
            
            return sentiment_data
            
        except Exception as e:
            self.logger.error(f"Error in sentiment analysis: {e}")
            return {}

    async def _create_risk_analysis(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create risk analysis from market data."""
        try:
            risk_data = {
                'risk_level': 'medium',
                'risk_factors': [],
                'volatility_analysis': {},
                'correlation_risks': []
            }
            
            # Analyze volatility if statistics available
            if 'statistics' in market_data:
                volatility = market_data['statistics'].get('volatility', 0)
                
                if volatility > 5:
                    risk_data['risk_level'] = 'high'
                    risk_data['risk_factors'].append('High volatility detected')
                elif volatility < 1:
                    risk_data['risk_level'] = 'low'
                
                risk_data['volatility_analysis'] = {
                    'current_volatility': volatility,
                    'volatility_rating': 'high' if volatility > 5 else 'medium' if volatility > 2 else 'low'
                }
            
            return risk_data
            
        except Exception as e:
            self.logger.error(f"Error in risk analysis: {e}")
            return {}

    async def _create_fundamental_analysis(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create fundamental analysis from market data."""
        try:
            fundamental_data = {
                'valuation_metrics': {},
                'market_health': {},
                'adoption_indicators': {}
            }
            
            # Add market health indicators
            if 'market_overview' in market_data:
                overview = market_data['market_overview']
                fundamental_data['market_health'] = {
                    'total_volume': overview.get('total_volume', 0),
                    'open_interest': overview.get('total_open_interest', 0),
                    'market_participation': 'active' if overview.get('total_volume', 0) > 1000000 else 'moderate'
                }
            
            return fundamental_data
            
        except Exception as e:
            self.logger.error(f"Error in fundamental analysis: {e}")
            return {}

    async def _generate_analysis_summary(self, analysis_report: Dict[str, Any], market_data: Dict[str, Any]) -> Dict[str, Any]:
        """Generate executive summary of analysis."""
        try:
            summary = {
                'key_findings': [],
                'market_outlook': 'neutral',
                'confidence_level': 'medium'
            }
            
            # Extract key findings from different analysis components
            if 'technical_analysis' in analysis_report:
                trend = analysis_report['technical_analysis'].get('trend_analysis', 'neutral')
                summary['key_findings'].append(f"Technical trend: {trend}")
                
            if 'sentiment_analysis' in analysis_report:
                sentiment = analysis_report['sentiment_analysis'].get('overall_sentiment', 'neutral')
                summary['key_findings'].append(f"Market sentiment: {sentiment}")
            
            # Determine overall outlook
            bullish_signals = sum(1 for finding in summary['key_findings'] if 'bullish' in finding.lower())
            bearish_signals = sum(1 for finding in summary['key_findings'] if 'bearish' in finding.lower())
            
            if bullish_signals > bearish_signals:
                summary['market_outlook'] = 'bullish'
            elif bearish_signals > bullish_signals:
                summary['market_outlook'] = 'bearish'
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Error generating analysis summary: {e}")
            return {}

    async def _generate_analysis_insights(self, analysis_report: Dict[str, Any], market_data: Dict[str, Any]) -> List[str]:
        """Generate actionable insights from analysis."""
        try:
            insights = []
            
            # Technical insights
            if 'technical_analysis' in analysis_report:
                trend = analysis_report['technical_analysis'].get('trend_analysis', 'neutral')
                if trend != 'neutral':
                    insights.append(f"Technical analysis indicates a {trend} trend in progress")
            
            # Risk insights
            if 'risk_analysis' in analysis_report:
                risk_level = analysis_report['risk_analysis'].get('risk_level', 'medium')
                if risk_level == 'high':
                    insights.append("Current market conditions present elevated risk levels")
            
            # Default insight if none generated
            if not insights:
                insights.append("Market conditions appear stable with no significant anomalies detected")
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Error generating analysis insights: {e}")
            return []

    async def _generate_analysis_recommendations(self, analysis_report: Dict[str, Any], market_data: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations from analysis."""
        try:
            recommendations = []
            
            # Risk-based recommendations
            if 'risk_analysis' in analysis_report:
                risk_level = analysis_report['risk_analysis'].get('risk_level', 'medium')
                if risk_level == 'high':
                    recommendations.append("Consider reducing position sizes due to elevated volatility")
                elif risk_level == 'low':
                    recommendations.append("Market conditions are stable for normal position sizing")
            
            # Trend-based recommendations
            if 'technical_analysis' in analysis_report:
                trend = analysis_report['technical_analysis'].get('trend_analysis', 'neutral')
                if trend == 'bullish':
                    recommendations.append("Consider long positions aligned with the upward trend")
                elif trend == 'bearish':
                    recommendations.append("Exercise caution with long positions in current downtrend")
            
            # Default recommendation
            if not recommendations:
                recommendations.append("Monitor market conditions closely and maintain appropriate risk management")
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Error generating recommendations: {e}")
            return []

    def _calculate_analysis_reliability(self, analysis_report: Dict[str, Any]) -> float:
        """Calculate reliability score for analysis report."""
        try:
            score = 50.0  # Base score
            
            # Increase score based on available analysis components
            if 'technical_analysis' in analysis_report:
                score += 15
            if 'sentiment_analysis' in analysis_report:
                score += 15
            if 'risk_analysis' in analysis_report:
                score += 10
            if 'fundamental_analysis' in analysis_report:
                score += 10
            
            return min(100.0, score)
            
        except Exception as e:
            self.logger.error(f"Error calculating reliability score: {e}")
            return 50.0

    def _extract_metadata(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extract metadata from raw report data."""
        try:
            metadata = {
                'data_sources': [],
                'timestamp': raw_data.get('timestamp', int(time.time() * 1000)),
                'data_quality': raw_data.get('data_quality_score', 100),
                'sections': list(raw_data.keys()) if isinstance(raw_data, dict) else []
            }
            
            # Identify data sources
            if 'market_overview' in raw_data:
                metadata['data_sources'].append('market_data')
            if 'whale_activity' in raw_data:
                metadata['data_sources'].append('whale_tracking')
            if 'smart_money_index' in raw_data:
                metadata['data_sources'].append('institutional_analysis')
                
            return metadata
            
        except Exception as e:
            self.logger.error(f"Error extracting metadata: {e}")
            return {}

    async def _format_for_executive(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for executive audience - high-level insights only."""
        try:
            executive_format = {
                'executive_summary': {},
                'key_metrics': {},
                'recommendations': [],
                'risk_assessment': {}
            }
            
            # Extract high-level market regime
            if 'market_overview' in raw_data:
                regime = raw_data['market_overview'].get('regime', 'Unknown')
                executive_format['executive_summary']['market_condition'] = regime
                
            # Extract key performance metrics
            if 'performance_metrics' in raw_data:
                metrics = raw_data['performance_metrics']
                executive_format['key_metrics'] = {
                    'system_health': 'Operational',
                    'data_quality': f"{metrics.get('data_quality', {}).get('avg_score', 100):.0f}%"
                }
            
            return executive_format
            
        except Exception as e:
            self.logger.error(f"Error formatting for executive: {e}")
            return {}

    async def _format_for_technical(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for technical audience - detailed technical data."""
        try:
            # Return most of the raw data with technical focus
            technical_format = raw_data.copy()
            
            # Add technical metadata
            technical_format['technical_metadata'] = {
                'data_freshness': self._calculate_data_freshness(raw_data),
                'api_performance': raw_data.get('performance_metrics', {}),
                'calculation_times': self._extract_calculation_times(raw_data)
            }
            
            return technical_format
            
        except Exception as e:
            self.logger.error(f"Error formatting for technical: {e}")
            return raw_data

    async def _format_for_compact(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data in compact form - essential information only."""
        try:
            compact_format = {}
            
            # Extract only essential fields
            if 'market_overview' in raw_data:
                compact_format['market'] = {
                    'regime': raw_data['market_overview'].get('regime', 'Unknown'),
                    'volume': self._format_number(raw_data['market_overview'].get('total_volume', 0))
                }
            
            if 'smart_money_index' in raw_data:
                compact_format['institutional'] = {
                    'index': raw_data['smart_money_index'].get('index', 50),
                    'signal': raw_data['smart_money_index'].get('signal', 'NEUTRAL')
                }
            
            return compact_format
            
        except Exception as e:
            self.logger.error(f"Error formatting for compact: {e}")
            return {}

    async def _format_for_detailed(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data in detailed form - all available information."""
        try:
            # Return all raw data plus additional computed metrics
            detailed_format = raw_data.copy()
            
            # Add detailed computations
            detailed_format['detailed_metrics'] = {
                'data_completeness': self._calculate_data_completeness(raw_data),
                'cross_correlations': self._calculate_cross_correlations(raw_data),
                'trend_analysis': self._calculate_trend_analysis(raw_data)
            }
            
            return detailed_format
            
        except Exception as e:
            self.logger.error(f"Error formatting for detailed: {e}")
            return raw_data

    async def _format_for_standard(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data in standard form - balanced information."""
        try:
            # Return normalized raw data
            return self._normalize_data_for_template(raw_data)
            
        except Exception as e:
            self.logger.error(f"Error formatting for standard: {e}")
            return raw_data

    async def _format_for_api(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Format data for API consumption - structured and consistent."""
        try:
            api_format = {
                'status': 'success',
                'timestamp': int(time.time() * 1000),
                'data': {},
                'metadata': {
                    'version': '1.0',
                    'format': 'api',
                    'data_sources': self._extract_metadata(raw_data).get('data_sources', [])
                }
            }
            
            # Structure data for API consumption
            api_format['data'] = {
                'market_summary': raw_data.get('market_overview', {}),
                'institutional_flows': raw_data.get('smart_money_index', {}),
                'whale_activity': raw_data.get('whale_activity', {}),
                'futures_data': raw_data.get('futures_premium', {}),
                'performance_stats': raw_data.get('performance_metrics', {})
            }
            
            return api_format
            
        except Exception as e:
            self.logger.error(f"Error formatting for API: {e}")
            return {'status': 'error', 'message': str(e), 'data': None}

    async def _adjust_for_executive_audience(self, formatted_data: Dict[str, Any]) -> Dict[str, Any]:
        """Adjust formatting for executive audience."""
        # Remove technical details, focus on business impact
        if 'content' in formatted_data and isinstance(formatted_data['content'], dict):
            # Keep only high-level insights
            executive_content = {}
            for key, value in formatted_data['content'].items():
                if key in ['executive_summary', 'key_metrics', 'recommendations', 'risk_assessment']:
                    executive_content[key] = value
            formatted_data['content'] = executive_content
        
        return formatted_data

    async def _adjust_for_technical_audience(self, formatted_data: Dict[str, Any]) -> Dict[str, Any]:
        """Adjust formatting for technical audience."""
        # Keep all technical details
        return formatted_data

    async def _adjust_for_api_audience(self, formatted_data: Dict[str, Any]) -> Dict[str, Any]:
        """Adjust formatting for API consumption."""
        # Ensure consistent structure for API clients
        if 'status' not in formatted_data:
            formatted_data['status'] = 'success'
        
        # For API format, we don't need the format_info wrapper since _format_for_api already provides the structure
        if formatted_data.get('status') == 'success' and 'data' in formatted_data:
            return formatted_data
        
        return formatted_data

    def _calculate_data_freshness(self, data: Dict[str, Any]) -> str:
        """Calculate how fresh the data is."""
        try:
            timestamp = data.get('timestamp', int(time.time() * 1000))
            current_time = int(time.time() * 1000)
            age_seconds = (current_time - timestamp) / 1000
            
            if age_seconds < 60:
                return f"{age_seconds:.0f} seconds ago"
            elif age_seconds < 3600:
                return f"{age_seconds/60:.0f} minutes ago"
            else:
                return f"{age_seconds/3600:.1f} hours ago"
                
        except Exception:
            return "unknown"

    def _extract_calculation_times(self, data: Dict[str, Any]) -> Dict[str, float]:
        """Extract calculation timing information."""
        try:
            times = {}
            if 'performance_metrics' in data:
                perf = data['performance_metrics']
                if 'processing_time' in perf:
                    times['avg_processing'] = perf['processing_time'].get('avg', 0)
                    times['max_processing'] = perf['processing_time'].get('max', 0)
            return times
        except Exception:
            return {}

    def _calculate_data_completeness(self, data: Dict[str, Any]) -> float:
        """Calculate what percentage of expected data is present."""
        try:
            expected_sections = ['market_overview', 'smart_money_index', 'whale_activity', 'futures_premium', 'performance_metrics']
            present_sections = [section for section in expected_sections if section in data and data[section]]
            return (len(present_sections) / len(expected_sections)) * 100
        except Exception:
            return 0.0

    def _calculate_cross_correlations(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate correlations between different data sections."""
        try:
            correlations = {}
            
            # Simple correlation analysis between market regime and smart money
            if 'market_overview' in data and 'smart_money_index' in data:
                regime = data['market_overview'].get('regime', '')
                smi_value = data['smart_money_index'].get('index', 50)
                
                regime_numeric = 1 if 'bullish' in regime.lower() else -1 if 'bearish' in regime.lower() else 0
                smi_normalized = (smi_value - 50) / 50  # Normalize to -1 to 1
                
                correlations['market_smart_money'] = {
                    'correlation': regime_numeric * smi_normalized,
                    'alignment': 'aligned' if regime_numeric * smi_normalized > 0 else 'divergent'
                }
            
            return correlations
        except Exception:
            return {}

    def _calculate_trend_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate trend analysis from available data."""
        try:
            trend_data = {}
            
            if 'market_overview' in data:
                regime = data['market_overview'].get('regime', '')
                trend_strength = data['market_overview'].get('trend_strength', 0)
                
                trend_data['primary_trend'] = regime
                trend_data['strength'] = trend_strength
                trend_data['confidence'] = 'high' if isinstance(trend_strength, (int, float)) and abs(float(str(trend_strength).replace('%', ''))) > 70 else 'medium'
            
            return trend_data
        except Exception:
            return {}
