#!/bin/bash

#############################################################################
# Script: test_dashboard_performance.sh
# Purpose: Test and validate test dashboard performance
# Author: Virtuoso CCXT Development Team
# Version: 1.0.0
# Created: 2025-08-28
# Modified: 2025-08-28
#############################################################################
#
# Description:
   Automates automated testing, validation, and quality assurance for the Virtuoso trading
   system. This script provides comprehensive functionality for managing
   the trading infrastructure with proper error handling and validation.
#
# Dependencies:
#   - Bash 4.0+
#   - python3
#   - curl
#   - grep
#   - Access to project directory structure
#
# Usage:
#   ./test_dashboard_performance.sh [options]
#   
#   Examples:
#     ./test_dashboard_performance.sh
#     ./test_dashboard_performance.sh --verbose
#     ./test_dashboard_performance.sh --dry-run
#
# Options:
#   -h, --help       Show help message
#   -v, --verbose    Enable verbose output
#   -d, --dry-run    Show what would be done
#
# Environment Variables:
#   PROJECT_ROOT     Trading system root directory
#   VPS_HOST         VPS hostname (default: VPS_HOST_REDACTED)
#   VPS_USER         VPS username (default: linuxuser)
#
# Output:
#   - Console output with operation status
#   - Log messages with timestamps
#   - Success/failure indicators
#
# Exit Codes:
#   0 - All tests passed
#   1 - Test failures detected
#   2 - Test configuration error
#   3 - Dependencies missing
#   4 - Environment setup failed
#
# Notes:
#   - Run from project root directory
#   - Requires proper SSH key configuration for VPS operations
#   - Creates backups before destructive operations
#
#############################################################################

"""
Dashboard Performance Testing Script
Tests all dashboard endpoints and measures performance improvements

This script tests:
- Response times for all dashboard endpoints
- Cache hit rates and performance
- Memory usage and CPU impact
- Error rates and reliability
"""

set -e

echo "üß™ Dashboard Performance Testing Suite"
echo "======================================"

# Configuration
VPS_HOST="${VPS_HOST:-VPS_HOST_REDACTED}"
BASE_URL="http://${VPS_HOST}:8003"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

# Test results storage
RESULTS_FILE="/tmp/dashboard_performance_results_$(date +%s).json"

echo -e "${BLUE}üìä Test Configuration:${NC}"
echo "  Base URL: $BASE_URL"
echo "  Results: $RESULTS_FILE"
echo "  Test time: $(date)"
echo

# Function to test endpoint performance
test_endpoint() {
    local endpoint="$1"
    local description="$2"
    local max_time="${3:-5.0}"
    
    echo -e "${BLUE}Testing: ${description}${NC}"
    echo "  Endpoint: ${endpoint}"
    
    # Test multiple times for average
    local total_time=0
    local success_count=0
    local error_count=0
    local total_size=0
    
    for i in {1..5}; do
        result=$(curl -s -o /tmp/response_$i.json -w "%{http_code}|%{time_total}|%{size_download}" "${BASE_URL}${endpoint}" || echo "000|999|0")
        
        IFS='|' read -r status_code time_total size_download <<< "$result"
        
        if [[ "$status_code" == "200" ]]; then
            success_count=$((success_count + 1))
            total_time=$(echo "$total_time + $time_total" | bc -l)
            total_size=$((total_size + size_download))
            echo "    Try $i: HTTP $status_code - ${time_total}s - ${size_download} bytes"
        else
            error_count=$((error_count + 1))
            echo -e "    Try $i: ${RED}HTTP $status_code - ${time_total}s${NC}"
        fi
    done
    
    if [[ $success_count -gt 0 ]]; then
        local avg_time=$(echo "scale=3; $total_time / $success_count" | bc -l)
        local avg_size=$((total_size / success_count))
        
        # Determine status
        local status="‚úÖ"
        if (( $(echo "$avg_time > $max_time" | bc -l) )); then
            status="‚ùå SLOW"
        elif [[ $error_count -gt 0 ]]; then
            status="‚ö†Ô∏è ERRORS"
        fi
        
        echo -e "  ${status} Average: ${avg_time}s (${success_count}/${5} success, ${avg_size} bytes)"
        
        # Store results
        echo "{\"endpoint\":\"$endpoint\",\"description\":\"$description\",\"avg_time\":$avg_time,\"success_rate\":$((success_count*100/5)),\"avg_size\":$avg_size,\"timestamp\":$(date +%s)}" >> "$RESULTS_FILE.tmp"
    else
        echo -e "  ‚ùå FAILED: All requests failed"
        echo "{\"endpoint\":\"$endpoint\",\"description\":\"$description\",\"avg_time\":999,\"success_rate\":0,\"avg_size\":0,\"timestamp\":$(date +%s)}" >> "$RESULTS_FILE.tmp"
    fi
    
    echo
}

# Function to test concurrent load
test_concurrent_load() {
    echo -e "${PURPLE}üîÑ Concurrent Load Testing${NC}"
    
    local endpoint="$1"
    local concurrent="${2:-10}"
    
    echo "  Testing $concurrent concurrent requests to $endpoint"
    
    # Create temporary script for concurrent testing
    cat > /tmp/concurrent_test.sh << EOF
#!/bin/bash
curl -s -o /dev/null -w "%{http_code}|%{time_total}" "${BASE_URL}${endpoint}"
EOF
    chmod +x /tmp/concurrent_test.sh
    
    # Run concurrent tests
    start_time=$(date +%s.%3N)
    
    for i in $(seq 1 $concurrent); do
        /tmp/concurrent_test.sh &
    done
    wait
    
    end_time=$(date +%s.%3N)
    total_time=$(echo "$end_time - $start_time" | bc -l)
    
    echo "  Concurrent test completed in ${total_time}s"
    echo
}

echo -e "${YELLOW}üöÄ Starting Dashboard Performance Tests${NC}"
echo

# Test 1: Critical Dashboard Endpoints
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 1: Critical Dashboard Endpoints ‚ïê‚ïê‚ïê${NC}"

test_endpoint "/api/dashboard-cached/mobile-data" "Mobile Dashboard Data" 3.0
test_endpoint "/api/dashboard-cached/overview" "Dashboard Overview" 3.0  
test_endpoint "/api/dashboard-cached/alerts" "Dashboard Alerts" 2.0
test_endpoint "/api/dashboard-cached/opportunities" "Alpha Opportunities" 3.0
test_endpoint "/api/dashboard-cached/signals" "Trading Signals" 2.0

# Test 2: Health and Status Endpoints
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 2: Health and Status Endpoints ‚ïê‚ïê‚ïê${NC}"

test_endpoint "/api/dashboard/health" "Dashboard Health Check" 1.0
test_endpoint "/health" "System Health Check" 1.0

# Test 3: Market Data Endpoints
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 3: Market Data Endpoints ‚ïê‚ïê‚ïê${NC}"

test_endpoint "/api/dashboard-cached/market-overview" "Market Overview" 2.0
test_endpoint "/api/dashboard-cached/market-movers" "Market Movers" 2.0
test_endpoint "/api/dashboard-cached/symbols" "Symbol Data" 3.0

# Test 4: Streaming Endpoints (if available)
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 4: Streaming Endpoints (Priority 1) ‚ïê‚ïê‚ïê${NC}"

if curl -s "${BASE_URL}/api/dashboard-stream/mobile-data-stream" > /dev/null 2>&1; then
    test_endpoint "/api/dashboard-stream/mobile-data-stream" "Streaming Mobile Data" 2.0
    test_endpoint "/api/dashboard-stream/overview-stream" "Streaming Overview" 2.0
    test_endpoint "/api/dashboard-stream/cache-performance" "Cache Performance Metrics" 1.0
else
    echo "  ‚ö†Ô∏è Streaming endpoints not available (Priority 1 fixes not deployed)"
fi

# Test 5: Concurrent Load Testing
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 5: Concurrent Load Testing ‚ïê‚ïê‚ïê${NC}"

test_concurrent_load "/api/dashboard-cached/mobile-data" 5
test_concurrent_load "/api/dashboard-cached/overview" 3
test_concurrent_load "/api/dashboard/health" 10

# Test 6: Cache Performance Analysis
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 6: Cache Performance Analysis ‚ïê‚ïê‚ïê${NC}"

echo "Testing cache performance..."

# Check if we can get cache stats
if curl -s "${BASE_URL}/api/cache/stats" > /tmp/cache_stats.json 2>/dev/null; then
    echo "  ‚úÖ Cache stats available"
    cat /tmp/cache_stats.json | jq '.hit_rate // "N/A"' 2>/dev/null | while read -r hit_rate; do
        echo "  üìä Cache hit rate: $hit_rate"
    done
else
    echo "  ‚ö†Ô∏è Cache stats not available"
fi

# Test multiple requests to same endpoint to check caching
echo "Testing cache effectiveness (5 rapid requests):"
for i in {1..5}; do
    start_time=$(date +%s.%3N)
    status_code=$(curl -s -o /dev/null -w "%{http_code}" "${BASE_URL}/api/dashboard-cached/overview")
    end_time=$(date +%s.%3N)
    response_time=$(echo "$end_time - $start_time" | bc -l)
    echo "    Request $i: HTTP $status_code - ${response_time}s"
done
echo

# Test 7: Error Rate Testing
echo -e "${BLUE}‚ïê‚ïê‚ïê Test 7: Error Rate Testing ‚ïê‚ïê‚ïê${NC}"

echo "Testing error handling with invalid endpoints..."
test_endpoint "/api/dashboard-cached/nonexistent" "Non-existent Endpoint (should 404)" 1.0
test_endpoint "/api/dashboard-cached/" "Empty path" 1.0

# Generate final report
echo -e "${GREEN}‚ïê‚ïê‚ïê Performance Test Results Summary ‚ïê‚ïê‚ïê${NC}"

if [[ -f "$RESULTS_FILE.tmp" ]]; then
    # Convert to proper JSON array
    echo "[" > "$RESULTS_FILE"
    sed '$!s/$/,/' "$RESULTS_FILE.tmp" >> "$RESULTS_FILE"
    echo "]" >> "$RESULTS_FILE"
    rm "$RESULTS_FILE.tmp"
    
    echo "üìä Detailed results saved to: $RESULTS_FILE"
    
    # Generate summary
    echo
    echo "üéØ Performance Summary:"
    
    # Count endpoints by performance
    fast_count=$(jq '[.[] | select(.avg_time < 1.0)] | length' "$RESULTS_FILE")
    good_count=$(jq '[.[] | select(.avg_time >= 1.0 and .avg_time < 3.0)] | length' "$RESULTS_FILE")
    slow_count=$(jq '[.[] | select(.avg_time >= 3.0)] | length' "$RESULTS_FILE")
    
    echo "  ‚ö° Fast (< 1s): $fast_count endpoints"
    echo "  ‚úÖ Good (1-3s): $good_count endpoints"  
    echo "  ‚ùå Slow (> 3s): $slow_count endpoints"
    
    # Show slowest endpoints
    echo
    echo "üêå Slowest endpoints:"
    jq -r '.[] | select(.avg_time > 0) | "\(.avg_time)s - \(.description)"' "$RESULTS_FILE" | sort -rn | head -5
    
    echo
    echo "üèÜ Recommendations:"
    
    if [[ $slow_count -gt 0 ]]; then
        echo "  1. Deploy quick fixes for slow endpoints"
        echo "  2. Implement Priority 1 optimizations"
        echo "  3. Enable streaming responses for large data"
    fi
    
    if [[ $good_count -gt $fast_count ]]; then
        echo "  4. Consider cache warming for frequently accessed endpoints"
        echo "  5. Implement response compression"
    fi
    
    if [[ $fast_count -gt 0 ]]; then
        echo "  6. Fast endpoints performing well - maintain current optimizations"
    fi
    
else
    echo "‚ùå No test results generated"
fi

echo
echo "üèÅ Performance testing complete!"
echo "Deploy fixes with:"
echo "  ./scripts/deploy_dashboard_fixes.sh           (Quick wins)"
echo "  ./scripts/deploy_priority1_fixes.sh          (Advanced optimizations)"
echo "  ./scripts/deploy_priority2_fixes.sh          (Long-term improvements)"