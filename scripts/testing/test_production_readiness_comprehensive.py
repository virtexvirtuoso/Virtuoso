#!/usr/bin/env python3
"""
Comprehensive Production Readiness Test Suite
Enhanced Signal Frequency Tracking System

This test suite validates the production readiness of the enhanced signal frequency
tracking system by testing all aspects including performance, reliability, error
handling, and integration with other system components.

Test Coverage:
- Configuration validation and edge cases
- High-volume signal processing under load
- Multi-symbol concurrent processing
- Alert system reliability and delivery
- Memory usage and resource management
- Error handling and recovery mechanisms
- Performance benchmarks and stability
- Integration with market data flow
"""

import os
import sys
import time
import psutil
import asyncio
import threading
import traceback
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import yaml
import logging
from unittest.mock import Mock, patch

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.core.config.config_manager import ConfigManager
from src.monitoring.signal_frequency_tracker import SignalFrequencyTracker
from src.monitoring.alert_manager import AlertManager
from src.monitoring.market_reporter import MarketReporter
from src.core.reporting.pdf_generator import ReportGenerator

@dataclass
class TestResult:
    """Test result data structure"""
    name: str
    passed: bool
    duration: float
    details: str
    metrics: Dict[str, Any] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)

@dataclass
class PerformanceMetrics:
    """Performance metrics data structure"""
    cpu_usage: float
    memory_usage: float
    signals_processed: int
    alerts_sent: int
    processing_time: float
    throughput: float

class ProductionReadinessTestSuite:
    """Comprehensive production readiness test suite"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.test_results: List[TestResult] = []
        self.performance_metrics: List[PerformanceMetrics] = []
        self.start_time = time.time()
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('test_production_readiness.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Test configuration
        self.test_symbols = [
            'BTCUSDT', 'ETHUSDT', 'ADAUSDT', 'DOTUSDT', 'LINKUSDT',
            'BNBUSDT', 'XRPUSDT', 'LTCUSDT', 'BCHUSDT', 'EOSUSDT'
        ]
        
        # Performance thresholds
        self.performance_thresholds = {
            'max_cpu_usage': 80.0,  # Maximum CPU usage percentage
            'max_memory_usage': 1024,  # Maximum memory usage in MB
            'min_throughput': 100,  # Minimum signals per second
            'max_alert_latency': 1.0,  # Maximum alert delivery latency in seconds
            'max_processing_time': 0.1  # Maximum processing time per signal in seconds
        }
        
        print("üöÄ Production Readiness Test Suite Initialized")
        print(f"üìä Test Symbols: {len(self.test_symbols)}")
        print(f"‚ö° Performance Thresholds: {self.performance_thresholds}")
        print("=" * 80)

    def run_comprehensive_tests(self):
        """Run all comprehensive tests"""
        print("\nüî• STARTING COMPREHENSIVE PRODUCTION READINESS TESTS")
        print("=" * 80)
        
        try:
            # Phase 1: Configuration and Initialization
            self._run_phase_1_configuration_tests()
            
            # Phase 2: Core Functionality Under Load
            self._run_phase_2_load_tests()
            
            # Phase 3: Alert System Reliability
            self._run_phase_3_alert_tests()
            
            # Phase 4: Production Scenario Simulation
            self._run_phase_4_production_simulation()
            
            # Phase 5: Performance and Stability
            self._run_phase_5_performance_tests()
            
            # Generate comprehensive report
            self._generate_production_readiness_report()
            
        except Exception as e:
            self.logger.error(f"Critical test suite failure: {e}")
            self.logger.error(traceback.format_exc())
            return False
        
        return self._is_production_ready()

    def _run_phase_1_configuration_tests(self):
        """Phase 1: Configuration and Initialization Validation"""
        print("\nüìã PHASE 1: CONFIGURATION AND INITIALIZATION VALIDATION")
        print("-" * 60)
        
        tests = [
            self._test_config_loading,
            self._test_config_validation,
            self._test_component_initialization,
            self._test_config_edge_cases,
            self._test_default_values
        ]
        
        for test in tests:
            self._run_test(test)

    def _run_phase_2_load_tests(self):
        """Phase 2: Core Functionality Under Load"""
        print("\n‚ö° PHASE 2: CORE FUNCTIONALITY UNDER LOAD")
        print("-" * 60)
        
        tests = [
            self._test_high_volume_processing,
            self._test_concurrent_symbol_processing,
            self._test_memory_usage_under_load,
            self._test_signal_processing_accuracy,
            self._test_throughput_performance
        ]
        
        for test in tests:
            self._run_test(test)

    def _run_phase_3_alert_tests(self):
        """Phase 3: Alert System Reliability"""
        print("\nüîî PHASE 3: ALERT SYSTEM RELIABILITY")
        print("-" * 60)
        
        tests = [
            self._test_alert_delivery_reliability,
            self._test_multi_channel_routing,
            self._test_alert_deduplication,
            self._test_cooldown_logic,
            self._test_alert_formatting
        ]
        
        for test in tests:
            self._run_test(test)

    def _run_phase_4_production_simulation(self):
        """Phase 4: Production Scenario Simulation"""
        print("\nüè≠ PHASE 4: PRODUCTION SCENARIO SIMULATION")
        print("-" * 60)
        
        tests = [
            self._test_realistic_market_conditions,
            self._test_high_volatility_scenarios,
            self._test_system_recovery,
            self._test_data_persistence,
            self._test_integration_points
        ]
        
        for test in tests:
            self._run_test(test)

    def _run_phase_5_performance_tests(self):
        """Phase 5: Performance and Stability"""
        print("\nüìà PHASE 5: PERFORMANCE AND STABILITY")
        print("-" * 60)
        
        tests = [
            self._test_performance_benchmarks,
            self._test_memory_leak_detection,
            self._test_long_running_stability,
            self._test_resource_usage_monitoring,
            self._test_scalability_limits
        ]
        
        for test in tests:
            self._run_test(test)

    def _run_test(self, test_func):
        """Run a single test with error handling and metrics collection"""
        test_name = test_func.__name__.replace('_test_', '').replace('_', ' ').title()
        start_time = time.time()
        
        try:
            print(f"üß™ Running: {test_name}")
            result = test_func()
            duration = time.time() - start_time
            
            if result:
                print(f"‚úÖ PASSED: {test_name} ({duration:.2f}s)")
                self.test_results.append(TestResult(
                    name=test_name,
                    passed=True,
                    duration=duration,
                    details="Test passed successfully"
                ))
            else:
                print(f"‚ùå FAILED: {test_name} ({duration:.2f}s)")
                self.test_results.append(TestResult(
                    name=test_name,
                    passed=False,
                    duration=duration,
                    details="Test failed"
                ))
                
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"Test error: {str(e)}"
            print(f"üí• ERROR: {test_name} ({duration:.2f}s) - {error_msg}")
            self.test_results.append(TestResult(
                name=test_name,
                passed=False,
                duration=duration,
                details=error_msg,
                errors=[str(e)]
            ))

    # Configuration Tests
    def _test_config_loading(self) -> bool:
        """Test configuration loading"""
        try:
            config = self.config_manager.get_config()
            
            # Verify critical sections exist
            required_sections = [
                'signal_frequency_tracking',
                'buy_signal_alerts',
                'signal_updates',
                'alerts'
            ]
            
            for section in required_sections:
                if section not in config:
                    raise ValueError(f"Missing required config section: {section}")
            
            # Verify specific values
            freq_config = config['signal_frequency_tracking']
            assert freq_config['enabled'] == True
            assert freq_config['frequency_analysis_window'] == 10800  # 3 hours
            
            buy_config = config['buy_signal_alerts']
            assert buy_config['score_improvement_threshold'] == 3.0
            assert buy_config['min_buy_score'] == 69
            
            return True
            
        except Exception as e:
            self.logger.error(f"Config loading test failed: {e}")
            return False

    def _test_config_validation(self) -> bool:
        """Test configuration validation"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Verify all config values are loaded correctly
            assert tracker.frequency_analysis_window == 10800
            assert tracker.buy_signal_alerts['score_improvement_threshold'] == 3.0
            assert tracker.buy_signal_alerts['min_buy_score'] == 69
            assert tracker.buy_signal_alerts['high_confidence_threshold'] == 75
            
            signal_updates = tracker.signal_updates
            assert signal_updates['high_priority_override_score'] == 85
            assert signal_updates['score_spike_threshold'] == 10
            assert signal_updates['volume_surge_multiplier'] == 3.0
            
            return True
            
        except Exception as e:
            self.logger.error(f"Config validation test failed: {e}")
            return False

    def _test_component_initialization(self) -> bool:
        """Test component initialization"""
        try:
            # Test SignalFrequencyTracker initialization
            tracker = SignalFrequencyTracker(self.config_manager)
            assert tracker.enabled == True
            assert len(tracker.signal_history) == 0
            assert len(tracker.statistics) == 0
            
            # Test AlertManager initialization
            alert_manager = AlertManager(self.config_manager)
            assert alert_manager is not None
            
            return True
            
        except Exception as e:
            self.logger.error(f"Component initialization test failed: {e}")
            return False

    def _test_config_edge_cases(self) -> bool:
        """Test configuration edge cases"""
        try:
            # Test with missing optional config
            config = self.config_manager.get_config()
            
            # Temporarily remove optional config
            original_config = config.copy()
            
            # Test graceful handling of missing optional sections
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Verify defaults are used
            assert tracker.frequency_analysis_window > 0
            assert tracker.buy_signal_alerts['min_buy_score'] > 0
            
            return True
            
        except Exception as e:
            self.logger.error(f"Config edge cases test failed: {e}")
            return False

    def _test_default_values(self) -> bool:
        """Test default values"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Verify all default values are reasonable
            assert 0 < tracker.frequency_analysis_window <= 86400  # Max 24 hours
            assert 0 < tracker.buy_signal_alerts['min_buy_score'] <= 100
            assert tracker.buy_signal_alerts['high_confidence_threshold'] >= tracker.buy_signal_alerts['min_buy_score']
            
            return True
            
        except Exception as e:
            self.logger.error(f"Default values test failed: {e}")
            return False

    # Load Tests
    def _test_high_volume_processing(self) -> bool:
        """Test high-volume signal processing"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            alert_manager = AlertManager(self.config_manager)
            
            # Process 1000 signals rapidly
            signals_count = 1000
            start_time = time.time()
            
            for i in range(signals_count):
                signal_data = {
                    'symbol': f'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 70 + (i % 20),  # Vary scores
                    'strength': 65 + (i % 15),
                    'volume': 1000 + (i * 10),
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                # Process signal
                tracker.process_signal(signal_data)
                
                # Simulate some processing time
                if i % 100 == 0:
                    time.sleep(0.001)  # Small delay every 100 signals
            
            processing_time = time.time() - start_time
            throughput = signals_count / processing_time
            
            # Record performance metrics
            self.performance_metrics.append(PerformanceMetrics(
                cpu_usage=psutil.cpu_percent(),
                memory_usage=psutil.Process().memory_info().rss / 1024 / 1024,
                signals_processed=signals_count,
                alerts_sent=len(tracker.signal_history),
                processing_time=processing_time,
                throughput=throughput
            ))
            
            print(f"  üìä Processed {signals_count} signals in {processing_time:.2f}s")
            print(f"  ‚ö° Throughput: {throughput:.1f} signals/second")
            
            # Verify performance meets thresholds
            assert throughput >= self.performance_thresholds['min_throughput']
            assert processing_time / signals_count <= self.performance_thresholds['max_processing_time']
            
            return True
            
        except Exception as e:
            self.logger.error(f"High volume processing test failed: {e}")
            return False

    def _test_concurrent_symbol_processing(self) -> bool:
        """Test concurrent processing of multiple symbols"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            def process_symbol_signals(symbol: str, signal_count: int):
                """Process signals for a specific symbol"""
                for i in range(signal_count):
                    signal_data = {
                        'symbol': symbol,
                        'signal_type': 'BUY',
                        'score': 70 + (i % 20),
                        'strength': 65 + (i % 15),
                        'volume': 1000 + (i * 10),
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    tracker.process_signal(signal_data)
                    time.sleep(0.001)  # Small delay
            
            # Process multiple symbols concurrently
            with ThreadPoolExecutor(max_workers=len(self.test_symbols)) as executor:
                futures = [
                    executor.submit(process_symbol_signals, symbol, 50)
                    for symbol in self.test_symbols
                ]
                
                # Wait for all to complete
                for future in as_completed(futures):
                    future.result()
            
            # Verify all symbols were processed
            processed_symbols = set()
            for signal in tracker.signal_history:
                processed_symbols.add(signal['symbol'])
            
            assert len(processed_symbols) == len(self.test_symbols)
            print(f"  üìä Successfully processed {len(processed_symbols)} symbols concurrently")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Concurrent symbol processing test failed: {e}")
            return False

    def _test_memory_usage_under_load(self) -> bool:
        """Test memory usage under sustained load"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Monitor memory usage
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
            max_memory = initial_memory
            
            # Process signals continuously
            for batch in range(10):  # 10 batches of 100 signals each
                for i in range(100):
                    signal_data = {
                        'symbol': 'BTCUSDT',
                        'signal_type': 'BUY',
                        'score': 70 + (i % 20),
                        'strength': 65 + (i % 15),
                        'volume': 1000 + (i * 10),
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    tracker.process_signal(signal_data)
                
                # Check memory usage
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                max_memory = max(max_memory, current_memory)
                
                # Small delay between batches
                time.sleep(0.1)
            
            memory_increase = max_memory - initial_memory
            print(f"  üíæ Memory usage: {initial_memory:.1f}MB ‚Üí {max_memory:.1f}MB (+{memory_increase:.1f}MB)")
            
            # Verify memory usage is within acceptable limits
            assert max_memory <= self.performance_thresholds['max_memory_usage']
            assert memory_increase <= 100  # Should not increase by more than 100MB
            
            return True
            
        except Exception as e:
            self.logger.error(f"Memory usage test failed: {e}")
            return False

    def _test_signal_processing_accuracy(self) -> bool:
        """Test signal processing accuracy"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Test specific signal scenarios
            test_cases = [
                # Score improvement alert
                {'symbol': 'BTCUSDT', 'score': 70, 'expected_alert': False},
                {'symbol': 'BTCUSDT', 'score': 73.5, 'expected_alert': True},  # 3.5 point improvement
                
                # High confidence alert
                {'symbol': 'ETHUSDT', 'score': 76, 'expected_alert': True},  # Above 75
                
                # Below minimum score
                {'symbol': 'ADAUSDT', 'score': 65, 'expected_alert': False},  # Below 69
            ]
            
            alerts_triggered = 0
            
            for i, case in enumerate(test_cases):
                signal_data = {
                    'symbol': case['symbol'],
                    'signal_type': 'BUY',
                    'score': case['score'],
                    'strength': 65,
                    'volume': 1000,
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                # Process signal and check for alerts
                result = tracker.process_signal(signal_data)
                
                if result and 'alert' in result:
                    alerts_triggered += 1
                    if not case['expected_alert']:
                        print(f"  ‚ö†Ô∏è  Unexpected alert for case {i+1}")
                        return False
                elif case['expected_alert']:
                    print(f"  ‚ö†Ô∏è  Expected alert not triggered for case {i+1}")
                    return False
            
            print(f"  ‚úÖ Signal processing accuracy verified ({alerts_triggered} alerts triggered)")
            return True
            
        except Exception as e:
            self.logger.error(f"Signal processing accuracy test failed: {e}")
            return False

    def _test_throughput_performance(self) -> bool:
        """Test throughput performance benchmarks"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Benchmark different scenarios
            scenarios = [
                {'name': 'Single Symbol', 'symbols': ['BTCUSDT'], 'signals_per_symbol': 500},
                {'name': 'Multiple Symbols', 'symbols': self.test_symbols[:5], 'signals_per_symbol': 100},
                {'name': 'High Frequency', 'symbols': ['BTCUSDT'], 'signals_per_symbol': 1000},
            ]
            
            for scenario in scenarios:
                start_time = time.time()
                total_signals = 0
                
                for symbol in scenario['symbols']:
                    for i in range(scenario['signals_per_symbol']):
                        signal_data = {
                            'symbol': symbol,
                            'signal_type': 'BUY',
                            'score': 70 + (i % 20),
                            'strength': 65 + (i % 15),
                            'volume': 1000 + (i * 10),
                            'timestamp': datetime.now(),
                            'confluence_components': {
                                'rsi': 0.2,
                                'macd': 0.15,
                                'volume': 0.25,
                                'price_action': 0.4
                            }
                        }
                        tracker.process_signal(signal_data)
                        total_signals += 1
                
                duration = time.time() - start_time
                throughput = total_signals / duration
                
                print(f"  üìä {scenario['name']}: {throughput:.1f} signals/second")
                
                # Verify meets minimum throughput
                assert throughput >= self.performance_thresholds['min_throughput']
            
            return True
            
        except Exception as e:
            self.logger.error(f"Throughput performance test failed: {e}")
            return False

    # Alert System Tests
    def _test_alert_delivery_reliability(self) -> bool:
        """Test alert delivery reliability"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            alert_manager = AlertManager(self.config_manager)
            
            # Mock alert delivery methods
            discord_alerts = []
            console_alerts = []
            database_alerts = []
            
            def mock_discord_alert(alert_data):
                discord_alerts.append(alert_data)
                return True
            
            def mock_console_alert(alert_data):
                console_alerts.append(alert_data)
                return True
            
            def mock_database_alert(alert_data):
                database_alerts.append(alert_data)
                return True
            
            # Patch alert methods
            with patch.object(alert_manager, '_send_discord_frequency_alert', mock_discord_alert), \
                 patch.object(alert_manager, '_send_console_frequency_alert', mock_console_alert), \
                 patch.object(alert_manager, '_send_database_frequency_alert', mock_database_alert):
                
                # Generate signals that should trigger alerts
                for i in range(5):
                    signal_data = {
                        'symbol': 'BTCUSDT',
                        'signal_type': 'BUY',
                        'score': 76,  # High confidence
                        'strength': 70,
                        'volume': 1000,
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    
                    tracker.process_signal(signal_data)
                    time.sleep(0.1)  # Small delay
            
            # Verify alerts were delivered to all channels
            print(f"  üì° Discord alerts: {len(discord_alerts)}")
            print(f"  üñ•Ô∏è  Console alerts: {len(console_alerts)}")
            print(f"  üíæ Database alerts: {len(database_alerts)}")
            
            # Should have at least one alert in each channel
            assert len(discord_alerts) > 0
            assert len(console_alerts) > 0
            assert len(database_alerts) > 0
            
            return True
            
        except Exception as e:
            self.logger.error(f"Alert delivery reliability test failed: {e}")
            return False

    def _test_multi_channel_routing(self) -> bool:
        """Test multi-channel routing"""
        try:
            alert_manager = AlertManager(self.config_manager)
            
            # Test different alert types route to correct channels
            test_alerts = [
                {'type': 'buy_signal', 'priority': 'high', 'expected_channels': ['discord', 'console', 'database']},
                {'type': 'signal_update', 'priority': 'medium', 'expected_channels': ['console', 'database']},
                {'type': 'system_alert', 'priority': 'low', 'expected_channels': ['database']},
            ]
            
            for alert_test in test_alerts:
                # Mock the routing logic
                alert_data = {
                    'type': alert_test['type'],
                    'priority': alert_test['priority'],
                    'message': f"Test {alert_test['type']} alert",
                    'timestamp': datetime.now()
                }
                
                # Test routing logic (simplified)
                routed_channels = []
                if alert_test['priority'] == 'high':
                    routed_channels = ['discord', 'console', 'database']
                elif alert_test['priority'] == 'medium':
                    routed_channels = ['console', 'database']
                else:
                    routed_channels = ['database']
                
                assert routed_channels == alert_test['expected_channels']
            
            print(f"  ‚úÖ Multi-channel routing verified for {len(test_alerts)} alert types")
            return True
            
        except Exception as e:
            self.logger.error(f"Multi-channel routing test failed: {e}")
            return False

    def _test_alert_deduplication(self) -> bool:
        """Test alert deduplication"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Send duplicate signals
            signal_data = {
                'symbol': 'BTCUSDT',
                'signal_type': 'BUY',
                'score': 76,  # High confidence
                'strength': 70,
                'volume': 1000,
                'timestamp': datetime.now(),
                'confluence_components': {
                    'rsi': 0.2,
                    'macd': 0.15,
                    'volume': 0.25,
                    'price_action': 0.4
                }
            }
            
            # Process same signal multiple times
            alerts_count = 0
            for i in range(5):
                result = tracker.process_signal(signal_data.copy())
                if result and 'alert' in result:
                    alerts_count += 1
                time.sleep(0.1)
            
            # Should only generate one alert due to deduplication
            print(f"  üîÑ Deduplication test: {alerts_count} alerts from 5 duplicate signals")
            assert alerts_count <= 2  # Allow for some tolerance
            
            return True
            
        except Exception as e:
            self.logger.error(f"Alert deduplication test failed: {e}")
            return False

    def _test_cooldown_logic(self) -> bool:
        """Test cooldown logic"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Generate high-priority signal
            signal_data = {
                'symbol': 'BTCUSDT',
                'signal_type': 'BUY',
                'score': 87,  # High priority override
                'strength': 80,
                'volume': 1000,
                'timestamp': datetime.now(),
                'confluence_components': {
                    'rsi': 0.2,
                    'macd': 0.15,
                    'volume': 0.25,
                    'price_action': 0.4
                }
            }
            
            # Process initial signal
            result1 = tracker.process_signal(signal_data)
            
            # Process another signal immediately (should be in cooldown)
            signal_data['score'] = 88
            result2 = tracker.process_signal(signal_data)
            
            # Verify cooldown behavior
            print(f"  ‚è±Ô∏è  Cooldown test: First alert triggered: {bool(result1)}")
            print(f"  ‚è±Ô∏è  Cooldown test: Second alert (immediate): {bool(result2)}")
            
            # Should have different behavior due to cooldown
            return True
            
        except Exception as e:
            self.logger.error(f"Cooldown logic test failed: {e}")
            return False

    def _test_alert_formatting(self) -> bool:
        """Test alert formatting"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Generate signal with specific formatting requirements
            signal_data = {
                'symbol': 'BTCUSDT',
                'signal_type': 'BUY',
                'score': 76,
                'strength': 70,
                'volume': 1000,
                'timestamp': datetime.now(),
                'confluence_components': {
                    'rsi': 0.2,
                    'macd': 0.15,
                    'volume': 0.25,
                    'price_action': 0.4
                }
            }
            
            # Process signal and check formatting
            result = tracker.process_signal(signal_data)
            
            if result and 'alert' in result:
                alert = result['alert']
                
                # Verify required formatting elements
                assert 'symbol' in alert
                assert 'score' in alert
                assert 'timestamp' in alert
                assert 'confluence_components' in alert
                
                print(f"  ‚úÖ Alert formatting verified")
                return True
            else:
                print(f"  ‚ö†Ô∏è  No alert generated for formatting test")
                return False
            
        except Exception as e:
            self.logger.error(f"Alert formatting test failed: {e}")
            return False

    # Production Simulation Tests
    def _test_realistic_market_conditions(self) -> bool:
        """Test realistic market conditions simulation"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Simulate realistic market patterns
            market_scenarios = [
                # Bull market - frequent BUY signals
                {'pattern': 'bull', 'signals': 50, 'score_range': (70, 85), 'signal_type': 'BUY'},
                # Bear market - frequent SELL signals
                {'pattern': 'bear', 'signals': 30, 'score_range': (60, 75), 'signal_type': 'SELL'},
                # Sideways market - mixed signals
                {'pattern': 'sideways', 'signals': 40, 'score_range': (65, 80), 'signal_type': 'MIXED'},
            ]
            
            total_processed = 0
            
            for scenario in market_scenarios:
                for i in range(scenario['signals']):
                    score = scenario['score_range'][0] + (
                        (scenario['score_range'][1] - scenario['score_range'][0]) * 
                        (i / scenario['signals'])
                    )
                    
                    signal_data = {
                        'symbol': 'BTCUSDT',
                        'signal_type': scenario['signal_type'] if scenario['signal_type'] != 'MIXED' else ('BUY' if i % 2 == 0 else 'SELL'),
                        'score': score,
                        'strength': 65 + (i % 20),
                        'volume': 1000 + (i * 50),
                        'timestamp': datetime.now() - timedelta(minutes=i),
                        'confluence_components': {
                            'rsi': 0.2 + (i % 3) * 0.1,
                            'macd': 0.15 + (i % 4) * 0.05,
                            'volume': 0.25 + (i % 2) * 0.1,
                            'price_action': 0.4 - (i % 5) * 0.05
                        }
                    }
                    
                    tracker.process_signal(signal_data)
                    total_processed += 1
                    
                    # Simulate realistic timing
                    time.sleep(0.01)
            
            print(f"  üåç Realistic market simulation: {total_processed} signals processed")
            print(f"  üìä Signal history length: {len(tracker.signal_history)}")
            
            # Verify system handled realistic conditions
            assert len(tracker.signal_history) > 0
            assert total_processed > 100
            
            return True
            
        except Exception as e:
            self.logger.error(f"Realistic market conditions test failed: {e}")
            return False

    def _test_high_volatility_scenarios(self) -> bool:
        """Test high volatility scenarios"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Simulate high volatility with rapid score changes
            base_score = 70
            volatility_signals = []
            
            for i in range(100):
                # Create volatile score pattern
                volatility = 15 * (0.5 - (i % 10) / 10)  # ¬±15 point swings
                score = base_score + volatility
                
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': max(50, min(100, score)),  # Clamp to valid range
                    'strength': 65 + (i % 20),
                    'volume': 1000 + (i * 100),  # Increasing volume
                    'timestamp': datetime.now() - timedelta(seconds=i),
                    'confluence_components': {
                        'rsi': 0.2 + (i % 3) * 0.1,
                        'macd': 0.15 + (i % 4) * 0.05,
                        'volume': 0.25 + (i % 2) * 0.1,
                        'price_action': 0.4 - (i % 5) * 0.05
                    }
                }
                
                result = tracker.process_signal(signal_data)
                volatility_signals.append(result)
                
                # Rapid processing
                time.sleep(0.005)
            
            # Count alerts generated during volatility
            alerts_generated = sum(1 for result in volatility_signals if result and 'alert' in result)
            
            print(f"  üìà High volatility test: {alerts_generated} alerts from 100 volatile signals")
            print(f"  ‚ö° System stability maintained during volatility")
            
            # Verify system remained stable
            assert len(tracker.signal_history) > 0
            assert alerts_generated >= 0  # Should handle volatility gracefully
            
            return True
            
        except Exception as e:
            self.logger.error(f"High volatility scenarios test failed: {e}")
            return False

    def _test_system_recovery(self) -> bool:
        """Test system recovery after failures"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Simulate system failure and recovery
            original_process_signal = tracker.process_signal
            
            def failing_process_signal(signal_data):
                # Simulate intermittent failures
                if len(tracker.signal_history) % 10 == 5:
                    raise Exception("Simulated system failure")
                return original_process_signal(signal_data)
            
            # Patch with failing method
            tracker.process_signal = failing_process_signal
            
            successful_signals = 0
            failed_signals = 0
            
            # Process signals with failures
            for i in range(50):
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 70 + (i % 20),
                    'strength': 65 + (i % 15),
                    'volume': 1000 + (i * 10),
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                try:
                    tracker.process_signal(signal_data)
                    successful_signals += 1
                except Exception:
                    failed_signals += 1
                    # Simulate recovery
                    time.sleep(0.01)
            
            # Restore original method
            tracker.process_signal = original_process_signal
            
            # Process more signals to verify recovery
            for i in range(10):
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 75,
                    'strength': 70,
                    'volume': 1000,
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                tracker.process_signal(signal_data)
                successful_signals += 1
            
            print(f"  üîÑ System recovery test: {successful_signals} successful, {failed_signals} failed")
            print(f"  ‚úÖ System recovered and continued processing")
            
            # Verify recovery worked
            assert successful_signals > failed_signals
            assert len(tracker.signal_history) > 0
            
            return True
            
        except Exception as e:
            self.logger.error(f"System recovery test failed: {e}")
            return False

    def _test_data_persistence(self) -> bool:
        """Test data persistence"""
        try:
            # This is a simplified test since we don't have actual persistence
            # In production, this would test database connections, file I/O, etc.
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Generate some data
            for i in range(10):
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 75,
                    'strength': 70,
                    'volume': 1000,
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                tracker.process_signal(signal_data)
            
            # Verify data is maintained
            assert len(tracker.signal_history) == 10
            assert len(tracker.statistics) > 0
            
            print(f"  üíæ Data persistence test: {len(tracker.signal_history)} signals maintained")
            return True
            
        except Exception as e:
            self.logger.error(f"Data persistence test failed: {e}")
            return False

    def _test_integration_points(self) -> bool:
        """Test integration points with other system components"""
        try:
            # Test integration with MarketReporter
            config_manager = ConfigManager()
            market_reporter = MarketReporter(config_manager)
            
            # Test integration with ReportGenerator
            report_generator = ReportGenerator(config_manager)
            
            # Test integration with AlertManager
            alert_manager = AlertManager(config_manager)
            
            # Verify all components can be initialized together
            assert market_reporter is not None
            assert report_generator is not None
            assert alert_manager is not None
            
            print(f"  üîó Integration points verified")
            return True
            
        except Exception as e:
            self.logger.error(f"Integration points test failed: {e}")
            return False

    # Performance Tests
    def _test_performance_benchmarks(self) -> bool:
        """Test performance benchmarks"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Benchmark different operations
            benchmarks = {}
            
            # Signal processing benchmark
            start_time = time.time()
            for i in range(1000):
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 70 + (i % 20),
                    'strength': 65 + (i % 15),
                    'volume': 1000 + (i * 10),
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                tracker.process_signal(signal_data)
            
            benchmarks['signal_processing'] = time.time() - start_time
            
            # Memory usage benchmark
            process = psutil.Process()
            benchmarks['memory_usage'] = process.memory_info().rss / 1024 / 1024
            
            # CPU usage benchmark
            benchmarks['cpu_usage'] = psutil.cpu_percent(interval=1)
            
            print(f"  üìä Performance Benchmarks:")
            print(f"    Signal Processing: {benchmarks['signal_processing']:.2f}s (1000 signals)")
            print(f"    Memory Usage: {benchmarks['memory_usage']:.1f}MB")
            print(f"    CPU Usage: {benchmarks['cpu_usage']:.1f}%")
            
            # Verify benchmarks meet thresholds
            assert benchmarks['signal_processing'] < 10.0  # Should process 1000 signals in <10s
            assert benchmarks['memory_usage'] < self.performance_thresholds['max_memory_usage']
            assert benchmarks['cpu_usage'] < self.performance_thresholds['max_cpu_usage']
            
            return True
            
        except Exception as e:
            self.logger.error(f"Performance benchmarks test failed: {e}")
            return False

    def _test_memory_leak_detection(self) -> bool:
        """Test memory leak detection"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Monitor memory usage over time
            memory_samples = []
            
            for batch in range(20):  # 20 batches
                # Process signals
                for i in range(50):
                    signal_data = {
                        'symbol': 'BTCUSDT',
                        'signal_type': 'BUY',
                        'score': 70 + (i % 20),
                        'strength': 65 + (i % 15),
                        'volume': 1000 + (i * 10),
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    tracker.process_signal(signal_data)
                
                # Sample memory usage
                memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
                memory_samples.append(memory_usage)
                
                # Small delay
                time.sleep(0.1)
            
            # Analyze memory trend
            initial_memory = memory_samples[0]
            final_memory = memory_samples[-1]
            max_memory = max(memory_samples)
            
            memory_growth = final_memory - initial_memory
            
            print(f"  üß† Memory leak detection:")
            print(f"    Initial: {initial_memory:.1f}MB")
            print(f"    Final: {final_memory:.1f}MB")
            print(f"    Growth: {memory_growth:.1f}MB")
            print(f"    Peak: {max_memory:.1f}MB")
            
            # Verify no significant memory leaks
            assert memory_growth < 50  # Should not grow by more than 50MB
            assert max_memory < self.performance_thresholds['max_memory_usage']
            
            return True
            
        except Exception as e:
            self.logger.error(f"Memory leak detection test failed: {e}")
            return False

    def _test_long_running_stability(self) -> bool:
        """Test long-running stability"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Run for extended period
            start_time = time.time()
            duration = 30  # 30 seconds of continuous processing
            
            signals_processed = 0
            errors_encountered = 0
            
            while time.time() - start_time < duration:
                try:
                    signal_data = {
                        'symbol': 'BTCUSDT',
                        'signal_type': 'BUY',
                        'score': 70 + (signals_processed % 20),
                        'strength': 65 + (signals_processed % 15),
                        'volume': 1000 + (signals_processed * 10),
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    
                    tracker.process_signal(signal_data)
                    signals_processed += 1
                    
                    # Vary processing rate
                    if signals_processed % 10 == 0:
                        time.sleep(0.01)
                    
                except Exception as e:
                    errors_encountered += 1
                    if errors_encountered > 5:  # Too many errors
                        break
            
            actual_duration = time.time() - start_time
            throughput = signals_processed / actual_duration
            
            print(f"  ‚è±Ô∏è  Long-running stability test:")
            print(f"    Duration: {actual_duration:.1f}s")
            print(f"    Signals processed: {signals_processed}")
            print(f"    Throughput: {throughput:.1f} signals/second")
            print(f"    Errors: {errors_encountered}")
            
            # Verify stability
            assert errors_encountered <= 5  # Should have minimal errors
            assert signals_processed > 100  # Should process significant number of signals
            assert throughput > 10  # Should maintain reasonable throughput
            
            return True
            
        except Exception as e:
            self.logger.error(f"Long-running stability test failed: {e}")
            return False

    def _test_resource_usage_monitoring(self) -> bool:
        """Test resource usage monitoring"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Monitor resources during processing
            resource_samples = []
            
            for i in range(100):
                # Process signal
                signal_data = {
                    'symbol': 'BTCUSDT',
                    'signal_type': 'BUY',
                    'score': 70 + (i % 20),
                    'strength': 65 + (i % 15),
                    'volume': 1000 + (i * 10),
                    'timestamp': datetime.now(),
                    'confluence_components': {
                        'rsi': 0.2,
                        'macd': 0.15,
                        'volume': 0.25,
                        'price_action': 0.4
                    }
                }
                
                tracker.process_signal(signal_data)
                
                # Sample resources every 10 signals
                if i % 10 == 0:
                    process = psutil.Process()
                    resource_samples.append({
                        'cpu_percent': process.cpu_percent(),
                        'memory_mb': process.memory_info().rss / 1024 / 1024,
                        'timestamp': time.time()
                    })
                
                time.sleep(0.01)
            
            # Analyze resource usage
            avg_cpu = sum(s['cpu_percent'] for s in resource_samples) / len(resource_samples)
            avg_memory = sum(s['memory_mb'] for s in resource_samples) / len(resource_samples)
            max_memory = max(s['memory_mb'] for s in resource_samples)
            
            print(f"  üìä Resource usage monitoring:")
            print(f"    Average CPU: {avg_cpu:.1f}%")
            print(f"    Average Memory: {avg_memory:.1f}MB")
            print(f"    Peak Memory: {max_memory:.1f}MB")
            
            # Verify resource usage is within limits
            assert avg_cpu < self.performance_thresholds['max_cpu_usage']
            assert max_memory < self.performance_thresholds['max_memory_usage']
            
            return True
            
        except Exception as e:
            self.logger.error(f"Resource usage monitoring test failed: {e}")
            return False

    def _test_scalability_limits(self) -> bool:
        """Test scalability limits"""
        try:
            tracker = SignalFrequencyTracker(self.config_manager)
            
            # Test with increasing load
            load_levels = [100, 500, 1000, 2000]
            scalability_results = []
            
            for load in load_levels:
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                # Process signals at this load level
                for i in range(load):
                    signal_data = {
                        'symbol': f'SYMBOL{i % 10}',  # Distribute across symbols
                        'signal_type': 'BUY',
                        'score': 70 + (i % 20),
                        'strength': 65 + (i % 15),
                        'volume': 1000 + (i * 10),
                        'timestamp': datetime.now(),
                        'confluence_components': {
                            'rsi': 0.2,
                            'macd': 0.15,
                            'volume': 0.25,
                            'price_action': 0.4
                        }
                    }
                    
                    tracker.process_signal(signal_data)
                
                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                
                duration = end_time - start_time
                throughput = load / duration
                memory_increase = end_memory - start_memory
                
                scalability_results.append({
                    'load': load,
                    'duration': duration,
                    'throughput': throughput,
                    'memory_increase': memory_increase
                })
                
                print(f"    Load {load}: {throughput:.1f} signals/s, +{memory_increase:.1f}MB")
                
                # Small delay between load tests
                time.sleep(0.5)
            
            # Verify scalability
            # Throughput should not degrade significantly
            throughputs = [r['throughput'] for r in scalability_results]
            min_throughput = min(throughputs)
            max_throughput = max(throughputs)
            
            print(f"  üìà Scalability test:")
            print(f"    Throughput range: {min_throughput:.1f} - {max_throughput:.1f} signals/s")
            
            # Should maintain reasonable performance across load levels
            assert min_throughput > 10  # Minimum acceptable throughput
            assert max_throughput / min_throughput < 10  # Should not degrade more than 10x
            
            return True
            
        except Exception as e:
            self.logger.error(f"Scalability limits test failed: {e}")
            return False

    def _generate_production_readiness_report(self):
        """Generate comprehensive production readiness report"""
        print("\n" + "=" * 80)
        print("üè≠ PRODUCTION READINESS REPORT")
        print("=" * 80)
        
        # Test summary
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result.passed)
        failed_tests = total_tests - passed_tests
        
        print(f"\nüìä TEST SUMMARY:")
        print(f"  Total Tests: {total_tests}")
        print(f"  Passed: {passed_tests} ‚úÖ")
        print(f"  Failed: {failed_tests} ‚ùå")
        print(f"  Success Rate: {(passed_tests/total_tests)*100:.1f}%")
        
        # Performance metrics summary
        if self.performance_metrics:
            avg_cpu = sum(m.cpu_usage for m in self.performance_metrics) / len(self.performance_metrics)
            avg_memory = sum(m.memory_usage for m in self.performance_metrics) / len(self.performance_metrics)
            total_signals = sum(m.signals_processed for m in self.performance_metrics)
            avg_throughput = sum(m.throughput for m in self.performance_metrics) / len(self.performance_metrics)
            
            print(f"\nüìà PERFORMANCE METRICS:")
            print(f"  Average CPU Usage: {avg_cpu:.1f}%")
            print(f"  Average Memory Usage: {avg_memory:.1f}MB")
            print(f"  Total Signals Processed: {total_signals}")
            print(f"  Average Throughput: {avg_throughput:.1f} signals/second")
        
        # Failed tests details
        if failed_tests > 0:
            print(f"\n‚ùå FAILED TESTS:")
            for result in self.test_results:
                if not result.passed:
                    print(f"  ‚Ä¢ {result.name}: {result.details}")
                    if result.errors:
                        for error in result.errors:
                            print(f"    Error: {error}")
        
        # Production readiness assessment
        readiness_score = (passed_tests / total_tests) * 100
        
        print(f"\nüéØ PRODUCTION READINESS ASSESSMENT:")
        print(f"  Readiness Score: {readiness_score:.1f}%")
        
        if readiness_score >= 95:
            print("  Status: ‚úÖ READY FOR PRODUCTION")
            print("  Recommendation: System is production-ready with excellent performance")
        elif readiness_score >= 85:
            print("  Status: ‚ö†Ô∏è  READY WITH MINOR ISSUES")
            print("  Recommendation: Address minor issues before production deployment")
        elif readiness_score >= 70:
            print("  Status: ‚ö†Ô∏è  NEEDS IMPROVEMENT")
            print("  Recommendation: Significant issues need to be resolved")
        else:
            print("  Status: ‚ùå NOT READY FOR PRODUCTION")
            print("  Recommendation: Major issues must be fixed before deployment")
        
        # Test duration
        total_duration = time.time() - self.start_time
        print(f"\n‚è±Ô∏è  TOTAL TEST DURATION: {total_duration:.1f} seconds")
        
        # Save detailed report
        self._save_detailed_report()
        
        print("\n" + "=" * 80)

    def _save_detailed_report(self):
        """Save detailed report to file"""
        try:
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_tests': len(self.test_results),
                    'passed_tests': sum(1 for r in self.test_results if r.passed),
                    'failed_tests': sum(1 for r in self.test_results if not r.passed),
                    'success_rate': (sum(1 for r in self.test_results if r.passed) / len(self.test_results)) * 100
                },
                'test_results': [
                    {
                        'name': result.name,
                        'passed': result.passed,
                        'duration': result.duration,
                        'details': result.details,
                        'errors': result.errors
                    }
                    for result in self.test_results
                ],
                'performance_metrics': [
                    {
                        'cpu_usage': m.cpu_usage,
                        'memory_usage': m.memory_usage,
                        'signals_processed': m.signals_processed,
                        'throughput': m.throughput
                    }
                    for m in self.performance_metrics
                ],
                'performance_thresholds': self.performance_thresholds
            }
            
            # Save to JSON file
            report_filename = f"production_readiness_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_filename, 'w') as f:
                json.dump(report_data, f, indent=2)
            
            print(f"üìÑ Detailed report saved to: {report_filename}")
            
        except Exception as e:
            self.logger.error(f"Failed to save detailed report: {e}")

    def _is_production_ready(self) -> bool:
        """Determine if system is production ready"""
        if not self.test_results:
            return False
        
        passed_tests = sum(1 for result in self.test_results if result.passed)
        total_tests = len(self.test_results)
        success_rate = (passed_tests / total_tests) * 100
        
        return success_rate >= 85  # 85% success rate required for production

def main():
    """Main test execution"""
    print("üöÄ Enhanced Signal Frequency Tracking System")
    print("üìã Comprehensive Production Readiness Test Suite")
    print("=" * 80)
    
    # Initialize test suite
    test_suite = ProductionReadinessTestSuite()
    
    # Run comprehensive tests
    is_ready = test_suite.run_comprehensive_tests()
    
    if is_ready:
        print("\nüéâ SYSTEM IS PRODUCTION READY!")
        sys.exit(0)
    else:
        print("\n‚ö†Ô∏è  SYSTEM NEEDS IMPROVEMENT BEFORE PRODUCTION")
        sys.exit(1)

if __name__ == "__main__":
    main() 