#!/usr/bin/env python3
"""
Test Performance Optimizations

This script tests the performance improvements from our scipy.signal optimizations
and vectorized operations in the indicators.
"""

import sys
import time
import numpy as np
import pandas as pd
from pathlib import Path

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

def generate_test_data(num_points=10000):
    """Generate realistic OHLCV test data for performance testing."""
    print(f"üìä Generating {num_points:,} test data points...")
    
    # Generate realistic price data with trends and volatility
    np.random.seed(42)  # For reproducible results
    
    # Base price with trend
    base_price = 50000
    trend = np.linspace(0, 1000, num_points)  # Upward trend
    noise = np.random.normal(0, 500, num_points)  # Price noise
    
    close_prices = base_price + trend + noise
    
    # Generate OHLC from close prices
    high_offset = np.abs(np.random.normal(0, 100, num_points))
    low_offset = np.abs(np.random.normal(0, 100, num_points))
    open_offset = np.random.normal(0, 50, num_points)
    
    data = pd.DataFrame({
        'open': close_prices + open_offset,
        'high': close_prices + high_offset,
        'low': close_prices - low_offset,
        'close': close_prices,
        'volume': np.random.lognormal(10, 1, num_points),  # Realistic volume distribution
        'timestamp': pd.date_range('2024-01-01', periods=num_points, freq='1min')
    })
    
    return data

def test_swing_detection_performance():
    """Test the optimized swing detection vs old nested loop approach."""
    print("\nüîç Testing Swing Detection Performance (O(n¬≤) vs O(n log n))")
    print("=" * 60)
    
    # Generate test data
    test_data = generate_test_data(5000)  # Smaller dataset for comparison
    highs = test_data['high'].values
    lows = test_data['low'].values
    
    # Test optimized version (scipy.signal.find_peaks)
    print("Testing optimized version (scipy.signal.find_peaks)...")
    
    start_time = time.time()
    from scipy.signal import find_peaks
    
    window = 10
    threshold = 0.003
    
    # Optimized swing detection
    peak_indices, _ = find_peaks(
        highs,
        distance=window,
        prominence=np.std(highs) * threshold,
        width=window//2
    )
    
    trough_indices, _ = find_peaks(
        -lows,
        distance=window,
        prominence=np.std(lows) * threshold,
        width=window//2
    )
    
    optimized_time = time.time() - start_time
    optimized_results = len(peak_indices) + len(trough_indices)
    
    # Test old nested loop approach (simulation)
    print("Testing old nested loop approach (simulated)...")
    
    start_time = time.time()
    
    # Simulate the old O(n¬≤) approach
    swing_points = 0
    for i in range(window, len(highs) - window):
        # Simulate the nested loop checks (without full implementation)
        local_max = max(highs[i-window:i+window+1])
        local_min = min(lows[i-window:i+window+1])
        
        if highs[i] == local_max:
            # Simulate the threshold checks
            significant = any(highs[i] > highs[j] * (1 + threshold) for j in range(max(0, i-window), i))
            if significant:
                swing_points += 1
                
        if lows[i] == local_min:
            # Simulate the threshold checks
            significant = any(lows[i] < lows[j] * (1 - threshold) for j in range(max(0, i-window), i))
            if significant:
                swing_points += 1
    
    old_time = time.time() - start_time
    
    # Results
    speedup = old_time / optimized_time if optimized_time > 0 else float('inf')
    
    print(f"\nüìà Swing Detection Results:")
    print(f"  Old method (O(n¬≤)):     {old_time:.4f}s ({swing_points} swing points)")
    print(f"  Optimized (O(n log n)): {optimized_time:.4f}s ({optimized_results} swing points)")
    print(f"  üöÄ Speedup:             {speedup:.1f}x faster")
    
    return speedup

def test_pandas_vectorization():
    """Test vectorized operations vs .apply(lambda) and .iterrows()."""
    print("\nüìä Testing Pandas Vectorization Performance")
    print("=" * 60)
    
    # Generate test data
    test_data = generate_test_data(50000)
    
    # Test 1: pd.to_numeric() vs apply(lambda)
    print("Test 1: Data type conversion")
    
    # Create string data to convert
    string_data = test_data['volume'].astype(str)
    
    # Old way: apply(lambda)
    start_time = time.time()
    result_old = string_data.apply(lambda x: float(x) if isinstance(x, str) else x)
    old_time = time.time() - start_time
    
    # New way: pd.to_numeric()
    start_time = time.time()
    result_new = pd.to_numeric(string_data, errors='coerce')
    new_time = time.time() - start_time
    
    conversion_speedup = old_time / new_time if new_time > 0 else float('inf')
    
    print(f"  apply(lambda):  {old_time:.4f}s")
    print(f"  pd.to_numeric(): {new_time:.4f}s")
    print(f"  üöÄ Speedup:     {conversion_speedup:.1f}x faster")
    
    # Test 2: Vectorized calculations vs iterrows()
    print("\nTest 2: Complex calculations")
    
    # Old way: iterrows()
    start_time = time.time()
    results_iterrows = []
    for idx, row in test_data.head(1000).iterrows():  # Smaller sample for iterrows
        calc = (row['high'] + row['low'] + row['close']) / 3 * row['volume']
        results_iterrows.append(calc)
    old_time_iterrows = time.time() - start_time
    
    # Scale up the time estimate for full dataset
    estimated_full_time = old_time_iterrows * (len(test_data) / 1000)
    
    # New way: Vectorized
    start_time = time.time()
    typical_price = (test_data['high'] + test_data['low'] + test_data['close']) / 3
    results_vectorized = typical_price * test_data['volume']
    vectorized_time = time.time() - start_time
    
    vectorization_speedup = estimated_full_time / vectorized_time if vectorized_time > 0 else float('inf')
    
    print(f"  iterrows() (estimated): {estimated_full_time:.4f}s")
    print(f"  Vectorized:            {vectorized_time:.4f}s")
    print(f"  üöÄ Speedup:            {vectorization_speedup:.1f}x faster")
    
    return conversion_speedup, vectorization_speedup

def test_memory_usage():
    """Test memory efficiency improvements."""
    print("\nüíæ Testing Memory Usage")
    print("=" * 60)
    
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    
    # Baseline memory
    baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # Generate large dataset
    large_data = generate_test_data(100000)
    
    # Memory after data creation
    after_data_memory = process.memory_info().rss / 1024 / 1024
    
    # Test vectorized operations (memory efficient)
    start_memory = process.memory_info().rss / 1024 / 1024
    
    # Efficient operations
    typical_price = (large_data['high'] + large_data['low'] + large_data['close']) / 3
    volume_weighted = typical_price * large_data['volume']
    rolling_mean = volume_weighted.rolling(20).mean()
    
    end_memory = process.memory_info().rss / 1024 / 1024
    
    print(f"  Baseline memory:        {baseline_memory:.1f} MB")
    print(f"  After data creation:    {after_data_memory:.1f} MB")
    print(f"  After calculations:     {end_memory:.1f} MB")
    print(f"  üìä Memory increase:     {end_memory - start_memory:.1f} MB")
    
    return end_memory - start_memory

def run_comprehensive_performance_test():
    """Run all performance tests and generate a summary report."""
    print("üöÄ COMPREHENSIVE PERFORMANCE TEST")
    print("=" * 80)
    print("Testing the scipy.signal and vectorization optimizations...")
    
    results = {}
    
    # Test 1: Swing Detection
    try:
        swing_speedup = test_swing_detection_performance()
        results['swing_detection'] = swing_speedup
    except Exception as e:
        print(f"‚ùå Swing detection test failed: {e}")
        results['swing_detection'] = 0
    
    # Test 2: Pandas Vectorization
    try:
        conv_speedup, vec_speedup = test_pandas_vectorization()
        results['data_conversion'] = conv_speedup
        results['vectorization'] = vec_speedup
    except Exception as e:
        print(f"‚ùå Vectorization test failed: {e}")
        results['data_conversion'] = 0
        results['vectorization'] = 0
    
    # Test 3: Memory Usage
    try:
        memory_usage = test_memory_usage()
        results['memory_usage_mb'] = memory_usage
    except Exception as e:
        print(f"‚ùå Memory test failed: {e}")
        results['memory_usage_mb'] = 0
    
    # Generate Summary Report
    print("\n" + "=" * 80)
    print("üìä PERFORMANCE TEST SUMMARY")
    print("=" * 80)
    
    print(f"\nüéØ Optimization Results:")
    print(f"  1. Swing Detection (O(n¬≤) ‚Üí O(n log n)):  {results['swing_detection']:.1f}x faster")
    print(f"  2. Data Conversion (.apply ‚Üí pd.to_numeric): {results['data_conversion']:.1f}x faster")
    print(f"  3. Calculations (.iterrows ‚Üí vectorized):   {results['vectorization']:.1f}x faster")
    print(f"  4. Memory usage during operations:          {results['memory_usage_mb']:.1f} MB")
    
    overall_improvement = np.mean([
        results['swing_detection'],
        results['data_conversion'], 
        results['vectorization']
    ])
    
    print(f"\n‚úÖ Overall Performance Improvement: {overall_improvement:.1f}x faster")
    
    # Validation
    if overall_improvement >= 10:
        print("üéâ EXCELLENT: Achieved 10x+ performance improvement!")
    elif overall_improvement >= 5:
        print("‚úÖ GOOD: Achieved 5x+ performance improvement!")
    elif overall_improvement >= 2:
        print("üëç MODERATE: Achieved 2x+ performance improvement!")
    else:
        print("‚ö†Ô∏è MINIMAL: Less than 2x improvement, may need further optimization")
    
    print(f"\nüìã Test completed successfully!")
    return results

if __name__ == "__main__":
    results = run_comprehensive_performance_test()